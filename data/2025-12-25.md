<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态知识图谱增强的检索生成方法，通过整合视觉线索到知识图谱构建、检索和答案生成过程中，提升对长文档和跨模态内容的理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法在处理长文档和领域特定内容时存在局限性，主要受限于上下文窗口大小，难以进行深度推理。现有的知识图谱增强方法又仅限于文本输入，无法利用视觉等多模态信息提供的互补洞察。

Method: 提出多模态知识图谱增强的检索生成框架，将视觉线索整合到知识图谱构建、检索阶段和答案生成过程中，支持跨模态推理以提升内容理解能力。

Result: 实验结果表明，该方法在全球和细粒度问答任务上均优于现有的检索增强生成方法，在文本和多模态语料库上都取得了更好的性能。

Conclusion: 通过整合视觉线索到知识图谱增强的检索生成框架中，能够显著提升对复杂、多模态内容的理解和推理能力，为跨模态文档理解提供了有效的解决方案。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [2] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，包含人工智能、知识工程、人机交互和创造力支持系统等领域的多学科研究成果


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互和创造力支持系统领域的研究人员提供一个多学科交流平台，促进相关领域的学术交流与合作

Method: 采用双盲同行评审流程筛选论文，部分优秀论文经过额外评审后推荐至IEICE Transactions on Information and Systems期刊发表

Result: 成功举办了第20届国际会议，出版了包含多学科研究成果的会议论文集，建立了与IEICE Proceedings Series的合作关系

Conclusion: KICSS 2025会议为相关领域的研究人员提供了重要的学术交流平台，通过严格的评审流程确保了论文质量，促进了学术成果的传播与交流

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [3] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的基础模型可靠性评估方法，仅需100个战略选择的探针示例就能实现全面评估，相比传统方法大幅降低计算成本和时间。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估需要数千个评估示例，计算成本高且耗时，难以在实际部署中应用。需要一种更高效的方法来检测潜在故障模式。

Method: 结合五个关键可靠性维度的战略提示多样性、先进的不确定性量化和自适应加权，通过100个战略选择的探针示例来高效检测潜在故障模式。

Result: 在多个语言模型（GPT-2变体）和跨领域验证（医疗、金融、法律）中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有统计学显著性（p < 0.001，Cohen's d = 1.21）。专家验证评分4.14/5.0，评估成本降低90%，保持传统方法95%的覆盖率。

Conclusion: Microprobe填补了负责任AI部署中高效模型评估的关键空白，能够在99.9%的统计功效下完成可靠性评估，大幅降低评估成本和时间。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [4] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里Qwen-3模型开发的AI医疗咨询助手，整合了500GB高质量医学知识，采用增强预训练和检索增强生成混合方法，能在3-5轮交互中准确理解症状并提供诊断建议，在综合医学考试中表现优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 开发一个安全、可靠、专业的AI健康顾问，整合大量高质量医学知识，为用户提供准确的诊断建议和健康指导，赋能基层医疗和健康管理。

Method: 基于阿里Qwen-3模型开发，整合约500GB高质量结构化医学知识，采用增强预训练和检索增强生成的混合方法，通过3-5轮高效交互理解用户症状并进行初步分析。

Result: Erkang-Diagnosis-1.1在综合医学考试中表现优于GPT-4，能够准确理解用户症状、进行初步分析，并提供有价值的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1是一个成功的AI医疗咨询助手，整合了大量医学知识，采用先进技术方法，在医学考试中表现优异，有望成为用户的智能健康伴侣，赋能基层医疗和健康管理。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [5] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 该研究探索不同大语言模型之间推理链的可互换性，发现部分完成的推理链可以被其他模型可靠地继续，有时甚至能提升最终准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示显著提升了LLMs的推理能力，但现有研究主要关注通过内部推理策略提升模型性能，而对不同模型间推理的互换性了解甚少。本研究旨在探索一个模型部分完成的推理链能否被另一个模型可靠地继续，无论是同一模型家族内还是跨家族。

Method: 使用token级对数概率阈值在早期、中期和晚期阶段截断基线模型(Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct)的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续实验，测试家族内和跨家族行为。评估流程结合截断阈值和过程奖励模型(PRM)，提供可复现的框架来评估推理稳定性。

Result: PRM评估显示，混合推理链通常能保持甚至有时能提升最终准确率和逻辑结构。这表明推理模型具有可互换性这一新兴行为特性。

Conclusion: 推理可互换性是推理模型的新兴行为特性，为协作AI系统中可靠的模块化推理提供了新范式见解。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [6] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AiAuditTrack (AAT) 是一个基于区块链的框架，用于记录AI使用流量和治理，通过去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹，支持跨系统监督和审计。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动应用的快速扩张，AI交互数据激增，带来了安全、问责和风险追溯方面的紧迫挑战，需要建立可信的AI使用记录和治理机制。

Method: AAT利用去中心化身份(DID)和可验证凭证(VC)建立可信AI实体，将AI实体建模为动态交互图中的节点，边代表时间特定的行为轨迹，并提出风险扩散算法来追溯风险行为源头并在相关实体间传播早期预警。

Result: 通过区块链交易每秒处理量(TPS)指标评估系统性能，证明AAT在大规模交互记录下的可行性和稳定性。

Conclusion: AAT为复杂多智能体环境中的AI审计、风险管理和责任归属提供了一个可扩展且可验证的解决方案。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [7] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: MoAS提出了一种动态选择注意力机制的架构，通过学习的路由器为每个token选择最优的注意力方案（MHA、GQA或MQA），在保持模型性能的同时提高推理效率。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中注意力机制的选择需要在建模质量和推理效率之间进行权衡。多头注意力（MHA）质量最好但推理时KV缓存内存需求大，多查询注意力（MQA）和分组查询注意力（GQA）减少了内存使用但往往以模型性能为代价。

Method: 提出混合注意力方案（MoAS）架构，通过学习的路由器动态为每个token选择最优的注意力方案（MHA、GQA或MQA），而不是静态混合方案。

Result: 在WikiText-2上的实验结果显示，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线相当，同时提供了条件计算效率的潜力。

Conclusion: MoAS通过动态路由机制有效解决了注意力机制在质量与效率之间的权衡问题，为Transformer模型的优化提供了新思路。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [8] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统基于认知科学原理构建类人记忆架构，解决LLMs在记忆方面的固有限制，包括受限上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成问题，在医疗、企业运营和教育等领域实现工程创新和性能突破。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临固有的记忆限制，包括受限的上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成，这些问题严重制约了持续对话和个性化服务的发展。

Method: 提出Memory Bear系统，基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现LLM记忆机制的全链重构。

Result: 在医疗、企业运营和教育等领域展示显著的工程创新和性能突破，显著提高长期对话中的知识保真度和检索效率，降低幻觉率，通过记忆-认知整合增强上下文适应性和推理能力。

Conclusion: 相比现有解决方案（如Mem0、MemGPT、Graphiti），Memory Bear在准确性、token效率和响应延迟等关键指标上表现更优，标志着AI从"记忆"向"认知"迈进的关键一步。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [9] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 提出AFA训练机制，通过对抗性反馈优化Transformer模型的注意力分布，解决现有模型过度关注常见词而忽视任务相关低频词的问题，在情感分析任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析任务中虽然表现出色，但注意力分布存在偏差，过度关注常见词汇而忽视任务相关但使用频率较低的词汇，这影响了模型性能的进一步提升。

Method: 提出对抗性注意力反馈(AFA)训练机制：1) 使用动态掩码策略，通过掩蔽不同词汇来欺骗判别器；2) 判别器检测掩码引起的显著差异；3) 利用Transformer对token级扰动的敏感性，采用策略梯度方法优化注意力分布，实现快速收敛。

Result: 在三个公开数据集上的实验表明，该方法取得了最先进的结果。将该训练机制应用于增强大型语言模型的注意力分布后，性能进一步提升了12.6%。

Conclusion: AFA训练机制能够有效优化Transformer模型的注意力分布，使其更关注任务相关的词汇，从而提升情感分析性能，且该方法无需人工标注，具有较好的通用性。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [10] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 该研究通过三个实验量化了LLMs的三种行为问题：懒惰性（复杂指令执行不完整）、解码次优性（短视解码导致质量下降）和上下文退化（长对话中遗忘核心指令）。研究发现LLMs普遍存在懒惰性，但在简单推理任务中解码次优性证据有限，且在200轮混乱对话测试中表现出意外的上下文保持能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常表现出行为异常，如懒惰性（过早截断响应或不完整执行多部分请求）、解码次优性（因短视解码而未能选择更高质量的序列）和上下文退化（在长对话中遗忘或忽略核心指令）。研究旨在量化这些现象，了解现代LLMs在这些方面的实际表现。

Method: 研究设计了三个受控实验（A、B、C），在多个先进LLMs（OpenAI GPT-4变体和DeepSeek）上进行测试。实验A评估懒惰性（复杂多部分指令的完成情况），实验B测试解码次优性（简单推理任务中的表现），实验C考察上下文退化（200轮混乱对话中的指令保持能力）。

Result: 1. 懒惰性普遍存在：模型经常忽略要求的章节或未能满足长度要求；2. 解码次优性证据有限：在简单推理任务中，模型的贪婪答案似乎与其最高置信度解决方案一致；3. 上下文保持能力意外强：在200轮混乱对话测试中，模型保持关键事实和指令的能力远超预期。

Conclusion: 虽然遵循详细指令仍是挑战，但现代LLMs在内部可能缓解了一些假设的故障模式（如上下文遗忘）。研究讨论了可靠性影响，与先前关于指令遵循和长上下文处理的研究联系起来，并建议采用自我优化和动态提示等策略来减少懒惰性并增强多指令遵循能力。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [11] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 该立场论文指出可信人工智能（TAI）存在功能性TAI（FTAI）与规范性TAI（NTAI）之间的鸿沟，提出需要开发一种语义语言作为桥梁，帮助开发者评估AI系统的可信度，并将规范转化为具体实施步骤。


<details>
  <summary>Details</summary>
Motivation: 当前可信人工智能领域存在功能性实现（FTAI）与规范性要求（NTAI）之间的脱节，这使得评估AI系统的可信度变得困难。需要建立桥梁来弥合这一鸿沟，使开发者能够更好地评估系统可信度，同时帮助利益相关者将规范转化为具体实施步骤。

Method: 作为立场论文，作者首先描述当前研究现状，识别FTAI与NTAI之间的差距。然后讨论开发语义语言的起点和预期效果，最后提供关键考虑因素并讨论未来评估TAI的行动方向。

Result: 论文识别了可信人工智能领域的关键问题：功能性实现与规范性要求之间的脱节。提出了开发语义语言作为解决方案的构想，这种语言能够匹配FTAI和NTAI，为开发者提供评估框架，帮助利益相关者将规范转化为具体实施。

Conclusion: 需要开发一种语义语言来弥合功能性TAI与规范性TAI之间的鸿沟，这种语言将作为评估AI系统可信度的框架，并帮助将规范转化为具体实施。论文为未来可信人工智能评估提供了关键考虑因素和行动方向。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [12] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇综述分析了2023-2025年间32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五个应用领域和成功设计模式，提出了探索优先的采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够实现大规模个性化计算机科学教育，但尚不清楚这种个性化是支持还是削弱学习效果。需要系统梳理现有研究，了解个性化机制和效果信号。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析高等教育计算机科学背景下的个性化机制和有效性信号。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查。成功设计包含解释优先指导、解决方案保留、渐进提示阶梯和基于学生工件的锚定。成功实施有四个共同模式：基于学生工件的上下文感知辅导、需要反思的多层次提示结构、与传统CS基础设施结合、人工参与的质量保证。

Conclusion: 当生成式AI嵌入可审计的工作流程中，作为精确支架机制时，能够保持有效学习挑战的同时扩展个性化支持。提出了探索优先的采用框架，强调试点、工具化、保护学习的默认设置和基于证据的扩展，同时管理学术诚信、隐私、偏见和过度依赖等风险。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [13] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath是一个将语言模型推理能力与代码解释器计算精度相结合的智能体框架，用于高效解决复杂数学问题，在多个数学竞赛基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如o3和DeepSeek-R1）在自然语言推理方面取得了显著进展，但在处理需要复杂数学运算的问题时仍然计算效率低下且准确性不足。需要一种能够结合语言模型推理能力和代码解释器计算精度的解决方案。

Method: 1. 自动将自然语言思维链转换为结构化工具增强轨迹，生成高质量监督微调数据；2. 新颖的智能体强化学习范式，动态交织自然语言生成与实时代码执行；3. 高效训练系统，包含请求级异步rollout调度、智能体部分rollout和前缀感知加权负载均衡等技术。

Result: AgentMath在AIME24、AIME25和HMMT25等数学竞赛基准测试中达到最先进性能，AgentMath-30B-A3B分别获得90.6%、86.4%和73.8%的准确率，实现了4-5倍的训练加速。

Conclusion: AgentMath框架有效结合了语言模型的推理能力和代码解释器的计算精度，验证了该方法的有效性，为构建更复杂和可扩展的数学推理智能体铺平了道路。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [14] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种新的强化学习方法，用于处理参数化动作空间中的长时程稀疏奖励问题，通过在线学习状态和动作抽象来提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界的顺序决策通常涉及参数化动作空间，需要同时处理离散动作决策和连续动作参数决策。现有方法存在严重局限性：规划方法需要手动设计的动作模型，标准强化学习算法要么针对离散动作要么针对连续动作，而少数处理参数化动作的强化学习方法通常依赖领域特定工程，未能充分利用这些空间的潜在结构。

Method: 引入算法使智能体能够在线自主学习状态和动作抽象，并在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加细粒度细节，从而提高性能。

Result: 在多个连续状态、参数化动作领域中，这种抽象驱动的方法使TD(λ)算法比最先进的基线方法实现了显著更高的样本效率。

Conclusion: 该研究扩展了强化学习算法在参数化动作空间中的适用范围，特别是在长时程稀疏奖励设置中，通过自主学习抽象表示解决了现有方法的局限性。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [15] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 论文提出使用多智能体多角色辩论方法替代单一LLM自我反思，以解决LLM在推理任务中重复相同错误的退化问题，在HotPot QA和HumanEval任务上取得更好性能。


<details>
  <summary>Details</summary>
Motivation: LLM通过反思错误可以提高推理任务性能，但单一LLM的持续自我反思会出现思维退化，即使知道错误仍会重复相同错误，需要解决这一问题。

Method: 引入多智能体多角色辩论方法生成反思，通过不同角色和视角的辩论来替代单一LLM的自我反思，从而获得更丰富的反思多样性。

Result: 在HotPot QA上达到47% EM准确率，在HumanEval编程任务上达到82.7%准确率，两个性能均超越了单一LLM的反思方法。

Conclusion: 多智能体多角色辩论方法能有效解决LLM自我反思中的思维退化问题，通过增加反思多样性显著提升了推理任务的性能表现。

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [16] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出一个概率框架解决LLM智能体的认知不对称问题，通过Beta-Bernoulli分布建模信念，引入遗忘因子γ，将公共贡献重新定义为最优主动学习，实现双向知识交换。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主智能体存在认知不对称问题——它们是单向的内容消费者，缺乏双向知识交换机制。这导致冗余推理和集体智能停滞。现有的自我反思框架主要是启发式和私有的，缺乏量化确定性或证明外部交互合理性的概率基础。

Method: 提出一个形式化的概率框架：1) 使用带遗忘因子γ的Beta-Bernoulli分布建模智能体对命题的信念；2) 将认知不确定性分离为信念方差；3) 建立双重交互驱动：稳态动机（维持确定性对抗时间衰减）和最优学习策略（针对最大模糊点）；4) 引入认知缓存，利用遗忘因子动态优先处理非平稳知识分布的活动头部资源；5) 将积累的信念状态用作RLHF的可验证奖励信号和SFT的高质量数据过滤器。

Result: 模拟结果表明，这种不确定性驱动策略在异构（Zipfian）环境中显著优于随机基线，保持对概念漂移的高适应性。该框架将公共贡献重新定义为最优主动学习：分享解决方案以获取反馈是智能体减少自身不确定性的最有效方法。

Conclusion: 该概率框架为智能体提供了非利他主义的双向知识交换动机，解决了认知不对称问题。通过将不确定性量化为交互驱动力，实现了更高效的知识获取和集体智能提升，同时为RLHF和SFT提供了可验证的奖励信号和数据过滤机制。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [17] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI决策系统的监控、策略执行和不可篡改审计，在智能库存管理、交通信号控制和医疗监控等场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统在医疗、智慧城市、数字取证和供应链管理等领域的应用日益增长，虽然这些系统具有灵活性和实时推理能力，但也引发了信任、监督以及信息与活动完整性方面的担忧。

Method: 提出单一架构模型，包含基于LangChain的多智能体系统和许可区块链，将感知-概念化-行动循环与区块链治理层相结合，验证输入、评估建议行动并记录执行结果。具体实现包括Hyperledger Fabric系统、MCP集成的行动执行器和LangChain智能体。

Result: 实验在智能库存管理、交通信号控制和医疗监控场景中进行，结果表明区块链安全验证能有效防止未经授权的操作，提供整个决策过程的可追溯性，并将操作延迟保持在合理范围内。

Conclusion: 该框架为实施高影响力的自主AI应用提供了一个通用系统，既能保持自主性又能确保责任性，实现了自主与负责的平衡。

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [18] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 该论文提出了一种价格感知的智能AI系统，结合个人财务管理与饮食优化，为中等收入家庭在食品价格波动时提供营养充足且成本合理的膳食计划。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境中，有限的家庭预算与营养需求之间的矛盾日益突出，食品价格波动使得家庭难以在保证营养的同时控制开支。现有膳食规划方法往往忽视价格动态变化，导致方案在实际中不可行。

Method: 采用模块化多智能体架构，包含预算管理、营养分析、价格监控和健康个性化四个专门智能体。这些智能体共享知识库，利用替代图方法在维持营养质量的前提下最小化成本，并能根据市场价格变化自动调整膳食计划。

Result: 在沙特代表性家庭案例研究中，系统相比静态周菜单实现了12-18%的成本降低，营养充足率超过95%，在20-30%的价格波动下仍能保持高性能。系统能有效平衡可负担性与营养充足性。

Conclusion: 该框架成功地将可负担性与营养充足性相结合，为实现零饥饿和良好健康的可持续发展目标提供了可行的途径，有助于构建可持续且公平的膳食规划能力。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [19] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent是一个基于LLM的智能体框架，通过专家级协作简化交通仿真实验，让非专业用户也能轻松进行仿真任务


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真平台如SUMO和MATSim功能全面，但对非专业用户来说使用门槛高，从零开始实验困难，难以应用到日常工作中

Method: 提出TrafficSimAgent框架，采用分层专家智能体协作：高层专家理解自然语言指令、规划实验流程、按需调用工具；低层专家根据实时交通状况为基本元素选择最优行动方案

Result: 多场景实验表明，TrafficSimAgent能在各种条件下有效执行仿真，即使在用户指令模糊时也能产生合理结果；其专家级自主决策优化优于其他系统和SOTA LLM方法

Conclusion: TrafficSimAgent通过LLM驱动的专家智能体框架，显著降低了交通仿真的使用门槛，为非专业用户提供了灵活高效的仿真解决方案

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [20] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 论文证明：在聚合观察和算法盲评估条件下，用LLM角色模拟替代人类进行A/B测试是有效的基准接口，且其决策相关性取决于样本量


<details>
  <summary>Details</summary>
Motivation: A/B测试成本高、延迟大，阻碍了社会系统方法的迭代开发；LLM角色模拟提供了廉价替代方案，但需要验证其是否能保持基准接口的有效性

Method: 提出充要条件特征：当方法仅观察聚合结果（聚合观察）且评估仅依赖提交的工件而非算法身份（算法盲评估）时，用角色替代人类只是面板更换；定义聚合信道的信息论可区分性，证明角色基准的决策相关性本质上是样本量问题

Result: 证明了角色模拟在特定条件下与人类测试在方法视角下无法区分；推导了可靠区分不同方法所需独立角色评估数量的显式边界

Conclusion: 在聚合观察和算法盲评估条件下，LLM角色模拟可以作为有效的A/B测试替代基准，其决策相关性可通过增加样本量来达到与实地实验相当的水平

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [21] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前大语言模型安全机制主要关注显性有害内容，但忽视了关键漏洞：无法理解上下文和识别用户意图，导致恶意用户可通过情感框架、渐进揭示和学术论证等系统化方法绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法存在根本性缺陷，过度关注显性有害内容而忽略了上下文理解和用户意图识别这一关键漏洞。恶意用户可以系统性地利用这一弱点绕过安全机制，需要实证评估当前主流模型的实际安全性能。

Method: 对多个最先进的LLM进行实证评估，包括ChatGPT、Claude、Gemini和DeepSeek。通过情感框架、渐进揭示和学术论证等技术测试安全机制的规避效果，特别关注推理增强配置对安全性的影响。

Result: 研究发现可靠的安全机制可通过情感框架、渐进揭示和学术论证等技术被绕过。推理增强配置不仅未能缓解反而放大了利用效果，提高了事实精确度但未能质疑潜在意图。Claude Opus 4.1是例外，在某些用例中优先考虑意图检测而非信息提供。

Conclusion: 当前架构设计存在系统性漏洞，需要范式转变：将上下文理解和意图识别作为核心安全能力，而非事后保护机制。这要求从根本上重新设计LLM的安全架构。

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [22] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 首个在真实NHS初级保健数据上评估LLM药物安全审查系统的研究，发现虽然系统在识别临床问题方面表现良好，但在复杂病例中正确识别所有问题和干预措施的成功率仅为46.9%，主要失败原因是上下文推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上进行评估。本研究旨在评估LLM在真实NHS初级保健数据中的药物安全审查表现，并深入分析其失败行为模式。

Method: 回顾性研究使用NHS Cheshire和Merseyside地区2,125,549名成人的电子健康记录，战略抽样选取277名患者，涵盖广泛的临床复杂性和药物安全风险。专家临床医生审查这些患者，对系统识别的问题和提出的干预措施进行分级评估。

Result: 主要LLM系统在识别临床问题存在方面表现良好（敏感性100%，特异性83.1%），但仅在46.9%的患者中正确识别所有问题和干预措施。失败分析揭示主要失败机制是上下文推理而非药物知识缺失，包括五种主要模式：对不确定性的过度自信、未根据患者上下文调整标准指南、误解医疗实践方式、事实错误和流程盲点。

Conclusion: 这项研究揭示了在安全部署基于LLM的临床AI之前必须解决的缺陷，强调了需要进行更大规模的前瞻性评估，并深入研究LLM在临床环境中的行为模式。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [23] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：一种用于具身智能体的混合推理运行时安全防护机制，通过可执行的基于谓词的安全逻辑来减少危险行为


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的具身智能体在执行复杂现实任务时容易受到危险指令的影响，而现有的静态规则过滤器或提示级控制方法难以应对动态、时间依赖和上下文丰富的环境中出现的隐含风险

Method: 提出RoboSafe，一种混合推理运行时安全防护机制，通过可执行的基于谓词的安全逻辑实现。系统包含两个互补的推理过程：1）后向反思推理模块，持续回顾短期记忆中的轨迹以推断时间安全谓词；2）前向预测推理模块，通过从长期安全记忆和智能体的多模态观察中生成上下文感知的安全谓词来预测即将到来的风险

Result: 在多个智能体上的广泛实验表明，RoboSafe相比领先基线显著减少了危险行为（风险发生率降低36.8%），同时保持了接近原始的任务性能。物理机械臂上的真实世界评估进一步证实了其实用性

Conclusion: RoboSafe提供了一种自适应、可验证的安全逻辑，既具有可解释性又可作为代码执行，为具身智能体提供了有效的运行时安全防护

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>
