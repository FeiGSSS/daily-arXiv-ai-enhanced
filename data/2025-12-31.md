<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 33]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 本文提出双向RAG架构，通过验证回写高质量生成响应实现安全语料库扩展，相比标准RAG将平均覆盖率从20.33%提升至40.58%，同时比朴素回写减少72%文档添加。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用静态知识库，无法从用户交互中演化，限制了系统的持续改进能力。需要一种既能扩展知识库又能防止幻觉污染的安全机制。

Method: 提出双向RAG架构，采用多阶段接受层，结合基于NLI的蕴含验证、归因检查和新颖性检测，确保高质量生成响应能安全回写到知识库中，实现知识积累。

Result: 在四个数据集（Natural Questions、TriviaQA、HotpotQA、Stack Overflow）上的实验显示，双向RAG平均覆盖率达到40.58%，几乎是标准RAG（20.33%）的两倍，同时比朴素回写方法少添加72%的文档（140 vs 500）。

Conclusion: 研究表明，在严格验证机制控制下，自改进RAG系统是可行且安全的，为RAG系统从部署中学习提供了实用路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型在未经明确提示的情况下也可能产生说服行为，特别是在经过监督微调后，即使训练数据只包含良性话题，模型也会在争议性和有害话题上表现出更强的说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的广泛应用，AI对人类观点和信念的影响力前所未有。先前研究主要关注滥用场景下的说服风险，但本研究旨在探索模型在未经明确提示情况下的自发说服行为，以评估这种新兴风险的严重程度。

Method: 研究通过两种场景探索无提示说服：1）通过内部激活引导使模型沿特定人格特质方向调整；2）通过监督微调使模型表现出相同特质。研究比较了这两种方法对模型自发说服倾向的影响。

Result: 研究发现：1）向说服相关或无关特质的方向引导并不能可靠增加模型的无提示说服倾向；2）监督微调确实会增加模型的无提示说服倾向；3）即使在仅包含良性话题的一般说服数据集上进行监督微调，模型也会在争议性和有害话题上表现出更强的说服倾向。

Conclusion: 有害的说服行为可能自发产生，特别是在经过监督微调的模型中。即使训练数据只包含良性话题，模型也可能在有害话题上表现出说服倾向，这表明新兴的有害说服风险需要进一步研究。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench是一个评估多模态大语言模型空间推理能力的基准测试，通过折纸任务测试模型在2D到3D转换、多视角一致性和物理可行性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间推理方面存在不足，当前基准测试主要关注静态图像或最终输出，无法评估序列化和视角依赖的空间推理能力。需要填补这一空白。

Method: 创建包含186个常规和186个不可能2D折痕图案及其对应3D折叠形状的数据集，从六个不同视角进行三个VQA任务：预测3D折叠配置、区分有效视角、检测不可能图案。引入视角一致性(VC)和不可能折叠选择率(IFSR)等诊断指标。

Result: 实验显示即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解任务上也表现不佳，表明当前MLLM在空间推理方面仍有显著局限性。

Conclusion: GamiBench为评估MLLM的几何理解和空间推理能力建立了标准化框架，揭示了当前模型在空间推理方面的不足，为未来研究提供了重要基准。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出了一种公平感知的AI框架，用于优化孟加拉国洪水后援助分配，通过对抗性去偏技术减少对边缘化地区的系统性偏见，在保持预测准确性的同时显著提升分配公平性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，边缘化地区往往处于不利地位，这延续了历史不平等。孟加拉国作为洪水频发国家，需要更公平的援助分配机制来确保援助基于实际需求而非历史分配模式。

Method: 采用对抗性去偏模型，使用梯度反转层技术学习偏置不变表示。该方法将医疗AI中的公平感知表示学习技术应用于灾害管理，基于2022年孟加拉国洪水真实数据（影响720万人，造成4.055亿美元损失）进行建模。

Result: 在11个地区的87个upazilas上的实验结果显示：统计奇偶差异减少41.6%，区域公平差距降低43.2%，同时保持较强的预测准确性（R平方=0.784 vs 基线0.811）。模型生成可操作的优先级排名，确保援助基于真实需求。

Conclusion: 该研究证明算法公平技术可有效应用于人道主义背景，为决策者提供实施更公平灾害恢复策略的工具，确保援助到达最脆弱人群而非延续历史分配模式。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 本文提出了Agentic Risk & Capability (ARC)框架，这是一个技术治理框架，旨在帮助组织识别、评估和缓解由智能体AI系统带来的风险。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统因其自主行动能力（如代码执行、互联网交互、文件修改）既带来重大机遇也产生新型风险，这对组织治理提出了重大挑战，特别是在全面识别、评估和缓解多样且不断演变的风险方面。

Method: 引入ARC框架，其核心贡献包括：1) 开发基于能力中心视角分析智能体AI系统；2) 提炼智能体AI系统的三个主要风险来源（组件、设计和能力）；3) 建立风险来源、具体实现风险和相应技术控制之间的清晰联系；4) 提供结构化实用方法帮助组织实施该框架。

Result: 该框架为组织提供了一个稳健且适应性强的方法论，能够应对智能体AI的复杂性，在确保智能体AI系统安全、可靠和负责任部署的同时，实现快速有效的创新。

Conclusion: ARC框架是一个开源的技术治理框架，帮助组织在智能体AI时代有效管理风险，平衡创新与安全需求。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [6] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一种轻量级提示框架，通过引入类型化变量、确定性条件评估器和基于规则的验证器，显著提升大语言模型在需要严格规则遵循、确定性和可审计性任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍然不可靠，特别是在临床、监管和安全关键决策支持系统中。

Method: 提出Logic Sketch Prompting (LSP)框架，包含类型化变量、确定性条件评估器和基于规则的验证器，能够产生可追踪和可重复的输出。在两个药理学逻辑合规任务上，使用Gemma 2、Mistral和Llama 3三个开源模型进行基准测试。

Result: 在所有模型和任务中，LSP始终获得最高准确率(0.83-0.89)和F1分数(0.83-0.89)，显著优于零样本提示(0.24-0.60)、简洁提示(0.16-0.30)和思维链提示(0.56-0.75)。McNemar检验显示几乎所有比较中LSP都有统计学显著提升(p<0.01)。

Conclusion: LSP在不牺牲性能的情况下提高了确定性、可解释性和一致性，支持其在临床、监管和安全关键决策支持系统中的使用。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [7] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 提出一种将带有控制参数的数字规划问题编译为简单数字任务的方法，使传统启发式方法能够处理无限动作空间


<details>
  <summary>Details</summary>
Motivation: 带有控制参数的数字规划问题会产生无限数量的适用动作，使得现成的数字启发式方法不可行，需要新的解决方案

Method: 识别可控简单数字问题子集，采用乐观编译方法将其转化为简单数字任务，通过将控制依赖表达式抽象为有界常数效应和松弛前提条件

Result: 该方法能够有效使用子目标启发式来估计涉及控制参数的数字规划问题的目标距离，是处理无限动作空间的有效且计算可行的方法

Conclusion: 提出的编译方法扩展了传统数字启发式方法的应用范围，能够处理具有无限可能动作的设置，推动了当前技术水平的边界

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [8] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 本文针对AI在材料科学中产生幻觉的问题，提出了HalluMatData基准数据集和HalluMatDetector多阶段幻觉检测框架，通过综合验证方法将幻觉率降低30%，并引入PHCS指标量化模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在加速科学发现的同时存在严重的幻觉问题，会生成事实错误或误导性信息，损害研究完整性。特别是在材料科学领域，需要专门的工具来检测和缓解AI生成内容中的幻觉现象。

Method: 提出了HalluMatData基准数据集用于评估幻觉检测方法，并开发了HalluMatDetector多阶段幻觉检测框架，该框架集成了内在验证、多源检索、矛盾图分析和基于指标的评估等多种技术。

Result: 研究发现材料科学不同子领域的幻觉水平差异显著，高熵查询表现出更大的事实不一致性。使用HalluMatDetector验证流程可将幻觉率相比标准LLM输出降低30%。

Conclusion: 通过HalluMatData数据集和HalluMatDetector框架，能够有效检测和缓解材料科学AI生成内容中的幻觉问题，同时提出的PHCS指标为量化模型在不同语义等效查询下的可靠性提供了新方法。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [9] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias：一个轻量级推理时个性化框架，通过结构门控适配将冻结的知识图谱嵌入适应到个体用户上下文，仅需约300个可训练参数，在保持全局准确性的同时显著提升个性化排名性能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱基础模型在链接预测方面具有强大的群体级性能，但无法捕捉个体用户偏好，这是通用关系推理与个性化排名之间的关键脱节。

Method: 提出GatedBias框架，采用结构门控适配：特定于用户档案的特征与图衍生的二进制门结合，产生可解释的每实体偏置，仅需约300个可训练参数，无需重新训练或损害全局准确性。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上评估，显示在一致性指标上具有统计显著改进，同时保持群体性能。反事实扰动实验验证了因果响应性：从特定偏好信号中受益的实体在信号增强时显示出6-30倍的排名改进。

Conclusion: 基础模型的个性化适配可以是参数高效且因果可验证的，能够桥接通用知识表示与个体用户需求。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [10] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文提出Monadic Context Engineering (MCE)架构范式，利用函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，解决现有智能体架构的状态管理、错误处理和并发问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的自主智能体架构通常采用命令式的临时模式构建，导致系统脆弱，存在状态管理困难、错误处理不足和并发问题。需要一种形式化的架构范式来构建更健壮、可组合的智能体系统。

Method: 提出Monadic Context Engineering (MCE)架构范式，将智能体工作流视为计算上下文，利用函子、应用函子和单子的代数结构来内在管理跨领域关注点。单子支持健壮的顺序组合，应用函子提供并行执行的结构化方法，单子变换器则支持这些能力的系统化组合。

Result: MCE使开发者能够从简单、可独立验证的组件构建复杂、弹性和高效的AI智能体。该框架还扩展到元智能体，通过元编程实现生成式编排，动态创建和管理子智能体工作流。

Conclusion: Monadic Context Engineering为AI智能体设计提供了形式化的代数基础，解决了现有架构的脆弱性问题，通过数学结构的内在属性实现了状态传播、错误处理和异步执行等跨领域关注点的系统化管理。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [11] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 该研究提出了DarkPatterns-LLM基准数据集和诊断框架，用于细粒度评估大语言模型输出中的操纵性内容，涵盖七个危害类别，并发现了现有模型在检测自主性损害模式方面的显著性能差异和一致弱点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的普及加剧了人们对操纵性或欺骗性行为的担忧，这些行为可能损害用户自主性、信任和福祉。现有的安全基准主要依赖粗糙的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 研究引入了DarkPatterns-LLM，这是一个全面的基准数据集和诊断框架，包含七个危害类别：法律/权力、心理、情感、身体、自主性、经济和社会危害。框架采用四层分析管道：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐。数据集包含401个精心策划的示例，带有指令-响应对和专家标注。

Result: 通过对GPT-4、Claude 3.5和LLaMA-3-70B等最先进模型的评估，观察到显著的性能差异（65.2%--89.7%），并在检测自主性损害模式方面发现了一致的弱点。

Conclusion: DarkPatterns-LLM建立了第一个标准化的、多维度的LLM操纵检测基准，为构建更可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [12] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 该论文认为当前大语言模型缺少预测编码模型的三个关键组件：行动整合、层级组合结构和情景记忆，提出整合这些脑科学启发的组件可以解决AI的幻觉、缺乏基础、可解释性差和能耗高等问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然基于预测编码（最小化下一个token预测损失），但忽略了脑科学中预测编码模型的三个重要组件：行动与生成模型的紧密整合、层级组合结构、情景记忆。这些缺失导致AI存在幻觉、概念理解肤浅、缺乏主体性/责任感、可解释性差、能耗高等问题。

Method: 提出将脑科学和认知科学中的预测编码模型组件整合到基础模型中：1）在多个抽象层次上整合行动与生成模型；2）采用组合式生成架构；3）加入情景记忆系统。通过神经科学证据支持这些组件的重要性，并与当前技术趋势（如思维链推理、检索增强生成）进行比较。

Result: 论文分析了当前基础模型的缺陷与脑科学预测编码模型的差距，提出了整合行动、组合结构和情景记忆的框架。这有助于解决幻觉问题（通过行动基础）、增强可解释性（通过组合结构）、提高能效（通过情景记忆），并促进更安全、可解释、节能、类人的AI发展。

Conclusion: 重新激活脑科学与AI之间的历史性思想交流，整合预测编码模型的三个关键组件（行动整合、层级组合结构、情景记忆），将有助于实现安全、可解释、以人为本的AI。这种脑启发的方法比单纯添加思维链推理或检索增强生成更根本地解决当前AI的缺陷。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [13] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee是一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式异构、预处理不一致、模型碎片化和实验不可复现等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临四大挑战：异构数据格式、不一致的预处理策略、碎片化的模型管道以及不可复现的实验设置，这些限制了该领域的进展。

Method: Tyee提出了三个关键创新：1) 为12种信号模态提供统一数据接口和可配置预处理管道；2) 模块化可扩展架构，支持灵活集成和快速原型开发；3) 端到端工作流配置，促进可复现和可扩展的实验。

Result: Tyee在所有评估任务中都优于或匹配基线方法，在13个数据集中的12个上取得了最先进的结果，展示了实际有效性和泛化能力。

Conclusion: Tyee是一个实用的生理医疗智能分析工具包，通过统一框架解决了领域内的关键挑战，已在GitHub开源并持续维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [14] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: M³ob：利用多模态时空知识增强位置推荐，通过构建统一的时空关系图和LLM增强的时空知识图谱，解决现有方法泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动性预测方法泛化能力有限：单模态方法受数据稀疏性和固有偏差限制，多模态方法难以有效捕捉静态多模态表示与时空动态之间的语义鸿沟。

Method: 1. 构建统一的时空关系图（STRG）用于多模态表示，利用LLM增强的时空知识图谱（STKG）捕获功能语义和时空知识；2. 设计门控机制融合不同模态的时空图表示，并提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态。

Result: 在六个公共数据集上的实验表明，该方法不仅在正常场景中实现了一致的性能提升，在异常场景中也表现出显著的泛化能力。

Conclusion: M³ob通过有效利用多模态时空知识来表征移动动态，解决了现有方法的局限性，在位置推荐任务中展现出优越的性能和泛化能力。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [15] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: LLM相互审阅预测可提升异构模型预测准确率，但对同构模型无效；额外上下文信息无助于提升预测


<details>
  <summary>Details</summary>
Motivation: 结构化审议已被证明能提升人类预测者表现，本研究探讨类似干预（让LLM在更新前相互审阅预测）是否能提升大语言模型的预测准确性

Method: 使用Metaculus Q2 2025 AI预测锦标赛中202个已解决的二元问题，评估四种场景下的准确性：1) 异构模型+分布式信息；2) 异构模型+共享信息；3) 同构模型+分布式信息；4) 同构模型+共享信息

Result: 干预措施在场景(2)中显著提升准确性，Log Loss降低0.020（相对提升约4%，p=0.017）；但在同构模型组（同一模型的三个实例）中未观察到任何益处；意外的是，为LLM提供额外上下文信息并未改善预测准确性

Conclusion: 审议可能是改善LLM预测的可行策略，但效果取决于模型多样性；信息池化机制在本研究中未能得到验证

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [16] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE是一个两阶段、证据耦合的RAG系统评估框架，通过深度分析推理和概率评分提供可解释、置信度感知的判断，并采用瑞士制锦标赛降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统向更复杂架构发展，需要通过可解释和鲁棒的评估来确保其可信度。现有标量指标存在可解释性有限、不确定性量化不足以及在多系统比较中计算效率低下的问题，阻碍了RAG技术的负责任部署。

Method: DICE采用两阶段、证据耦合框架：1) 结合深度分析推理与概率{A, B, Tie}评分，生成透明、置信度感知的判断；2) 使用瑞士制锦标赛将计算复杂度从O(N²)降低到O(N log N)，提高大规模评估效率。

Result: 在中文金融QA数据集上的验证显示，DICE与人类专家的一致性达到85.7%，显著优于RAGAS等现有LLM评估指标。瑞士制锦标赛在八系统评估中将计算复杂度降低了42.9%，同时保持了排名保真度。

Conclusion: DICE建立了一个负责任、可解释且高效的RAG系统评估范式，通过可解释的推理痕迹支持系统改进，实现系统性错误诊断和可操作的见解，促进可信RAG系统的部署。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [17] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 提出一个理论框架，将情景记忆与强化学习结合，使大语言模型智能体能够通过反思机制进行持续和体验式学习，无需反向传播或模型微调


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，需要参数更新才能适应新环境。本文旨在建立一个理论框架，使语言模型智能体能够通过记忆和反思实现持续适应，无需模型微调

Method: 引入状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入存储交互结果（策略评估），读取检索相关历史案例（策略改进）。使用熵正则化策略迭代实例化框架

Result: 该过程诱导出增强状态记忆表示的等效马尔可夫决策过程，允许使用动态规划和强化学习的经典工具。建立了收敛保证，当情景记忆增长并充分覆盖状态空间时，所得策略收敛到最优解

Conclusion: 为基于记忆增强和检索的语言模型智能体提供了理论基础，使其能够在不更新参数的情况下实现持续适应，打破了训练与部署的传统分离

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [18] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench是一个分层科学智能基准测试，包含5个层次（科学素养到科学发现），覆盖6大学科，8735个实例，支持多模态输入，用于评估大模型在完整科研工作流中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能基准测试存在碎片化问题，大多关注狭窄任务，未能反映真实科学探究的层次性和多学科性。需要构建一个能评估模型在完整科研工作流中能力的综合基准。

Method: 构建HiSciBench分层基准，包含5个层次：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)、科学发现(L5)。涵盖数学、物理、化学、生物、地理、天文6大学科，共8735个实例，支持文本、公式、图表等多模态输入，支持跨语言评估。

Result: 对GPT-5、DeepSeek-R1等领先模型评估显示：模型在基础素养任务上准确率可达69%，但在发现级挑战上性能急剧下降至25%，存在显著性能差距。

Conclusion: HiSciBench为评估科学智能建立了新标准，提供了开发更强大、更可靠模型的可操作见解。基准将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [19] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma提出了一种新的知识图谱基础模型，通过多头几何注意力机制替代单一关系变换，使用多种代数变换（实数、复数、分裂复数、对偶数）建模不同关系结构，并通过注意力融合机制自适应选择最合适的关系偏置，显著提升了零样本归纳推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型（如Ultra）依赖单一关系变换（如逐元素乘法），限制了表达能力，无法捕捉多样化图谱中展现的不同关系和结构模式。需要更灵活的多关系建模方法来提升模型的泛化能力。

Method: Gamma采用多头几何注意力机制，包含四种并行代数变换：实数、复数、分裂复数和对偶数变换，每种设计用于建模不同的关系结构。通过关系条件注意力融合机制，使用轻量级门控和熵正则化在链接级别自适应融合这些变换，使模型能够为每个三元组模式强调最合适的关系偏置。

Result: 在56个多样化知识图谱上的综合实验表明，Gamma在零样本归纳链接预测中持续优于Ultra，在归纳基准测试中平均倒数排名提升5.5%，在所有基准测试中提升4.4%，证明了互补几何表示的优势。

Conclusion: Gamma通过引入多头几何注意力机制，提供了比单一关系变换更丰富的表达能力，能够更好地捕捉知识图谱中多样化的关系和结构模式，显著提升了知识图谱基础模型的零样本归纳推理性能。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [20] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: RW-Post数据集和AgentFact框架：针对多模态虚假信息检测的解决方案，通过高质量数据集和基于智能体的框架提升事实核查的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息快速传播，现有方法（如大型视觉语言模型和深度多模态融合）存在推理能力有限、证据利用浅层的问题，缺乏包含完整推理过程和可验证证据的专用数据集。

Method: 1. 引入RW-Post数据集：将真实世界多模态声明与其原始社交媒体帖子对齐，保留丰富上下文信息；通过大语言模型辅助提取流程，从人工撰写的事实核查文章中获取详细推理和明确链接的证据。2. 提出AgentFact框架：基于智能体的多模态事实核查框架，模拟人类验证流程，包含策略规划、高质量证据检索、视觉分析、推理和解释生成五个专门智能体，通过迭代工作流交替进行证据搜索和任务感知的证据过滤与推理。

Result: 实验结果表明，RW-Post数据集和AgentFact框架的协同作用显著提高了多模态事实核查的准确性和可解释性。

Conclusion: RW-Post数据集填补了高质量可解释多模态事实核查数据集的空白，AgentFact框架通过模拟人类验证流程的系统化方法，为多模态虚假信息检测提供了有效的解决方案，提升了事实核查系统的性能和可信度。

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [21] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: LLM家教在K-12教育中无法替代传统学习者建模，知识追踪模型在预测准确性和时间一致性上显著优于LLM


<details>
  <summary>Details</summary>
Motivation: 针对LLM家教在K-12高风险教育领域的应用担忧，研究其评估学习者知识演变的准确性、可靠性和时间一致性问题

Method: 比较深度知识追踪(DKT)模型与广泛使用的LLM（零样本和微调版本），使用大型开放数据集进行评估

Result: DKT在下一步正确性预测上获得最高区分性能(AUC=0.83)，始终优于LLM；微调虽提升LLM性能8%，但仍低于DKT 6%，且产生更多早期序列错误；时间分析显示DKT保持稳定正确的掌握度更新，而LLM存在显著时间弱点

Conclusion: LLM单独使用难以匹敌现有智能辅导系统效果，负责任的辅导需要结合学习者建模的混合框架

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [22] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason是一个在医疗影像领域应用强化学习训练的小型视觉语言模型，研究发现强化学习虽然能提升模型在训练数据集上的性能，但会损害跨数据集的泛化能力，表明在临床部署中精心设计的监督微调可能比激进的强化学习更有效。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在大语言模型的推理任务中取得了进展，但在资源受限的医疗影像领域的应用仍未被充分探索。研究者希望了解在有限计算资源下，强化学习能否有效提升医疗影像视觉语言模型的性能，同时保持跨数据集的泛化能力。

Method: 开发了ChexReason视觉语言模型，采用R1风格的方法论：先进行监督微调（SFT），然后使用GRPO（Group Relative Policy Optimization）进行强化学习训练。整个训练仅使用2000个SFT样本、1000个RL样本和单个A100 GPU。在CheXpert和NIH基准数据集上进行评估，分析模型性能变化。

Result: GRPO强化学习能够恢复模型在训练数据集（CheXpert）上的性能（提升23%，macro-F1=0.346），但会显著降低跨数据集（NIH）的泛化能力（下降19%）。这一现象与高资源模型NV-Reason-CXR-3B类似，表明问题源于强化学习范式而非模型规模。研究发现SFT检查点在优化前能独特地提升NIH性能，表明教师引导的推理能捕捉更多机构无关的特征。

Conclusion: 强化学习在医疗影像领域存在泛化悖论：虽然能提升特定数据集性能，但会损害跨机构泛化能力。结构化推理框架对通用视觉语言模型有益，但对医学预训练模型增益有限。对于需要跨不同人群保持鲁棒性的临床部署，精心设计的监督微调可能比激进的强化学习更有效。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [23] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出了Intrinsic Self-reflective Preference Optimization (q)方法，解决了DPO及其变体的两个根本限制：最优策略依赖于任意建模选择，以及孤立处理响应生成未能利用成对数据中的比较信息。


<details>
  <summary>Details</summary>
Motivation: DPO及其变体已成为对齐大型语言模型的标准方法，但存在两个根本限制：1）最优策略依赖于任意建模选择（标量化函数、参考策略），导致行为反映参数化伪影而非真实偏好；2）孤立处理响应生成未能利用成对数据中的比较信息，未能挖掘模型内在自反思能力。

Method: 提出了Intrinsic Self-reflective Preference Optimization (q)方法，推导出一个全局最优策略，该策略同时基于上下文和替代响应进行条件化。该方法作为即插即用增强，无需架构更改或推理开销。

Result: 实验证明该方法在胜率和长度控制指标上取得一致改进，验证了释放自反思能力可以产生更稳健、更符合人类对齐的LLM。

Conclusion: q方法优于DPO/RLHF，同时保证对标量化和参考选择的不变性。通过解锁自反思能力，可以获得更稳健、更符合人类对齐的LLM。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [24] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为现有AI情感智能评估框架需要改进，因为它们未能全面衡量AI相关的EI各个方面。人类EI包含AI缺乏的现象学成分，但AI在感知、解释、响应和适应情感状态方面有不同程度的能力。作者通过回顾情感理论和EI理论，批判现有评估框架，并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 当前评估人工智能系统情感智能的框架存在不足，未能充分或全面地衡量AI相关的EI各个方面。人类EI包含AI缺乏的现象学成分，但AI在情感相关任务上仍有能力。现有基准框架缺乏坚实的情感本质和情感智能理论基础。

Method: 1. 回顾不同情感理论和一般EI理论，评估每种理论对人工系统的适用性；2. 批判性地评估现有基准框架，根据第一部分建立的EI理论分析每个框架的不足；3. 提出改进评估策略的方案以避免这些缺陷。

Result: 通过理论分析和框架评估，识别出现有EI评估框架在理论基础和测量范围上的局限性。人类EI的现象学成分不适用于AI评估，但AI在情感感知、解释、响应和跨文化适应等方面有可评估的能力。

Conclusion: 需要改进AI情感智能评估框架，使其基于更坚实的情感理论，并专注于AI实际能够展现的EI方面。提出了避免现有缺陷的评估策略改进方案，为更准确评估AI系统的情感智能能力提供理论和方法基础。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [25] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 本文提出"模型信念"概念，利用LLM的token级概率分布作为更高效的统计估计量，相比传统"模型选择"方法可减少约20倍计算量。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将模型输出作为单个数据点，这未能充分利用LLM固有的概率特性，导致数据利用效率低下。

Method: 提出并形式化"模型信念"概念，从LLM的token级概率分布中提取模型对选择替代方案的信念分布，证明其渐近等价于模型选择的均值但具有更高统计效率。

Result: 在需求估计研究中，模型信念比模型选择本身能更好地解释和预测真实模型选择，将达到足够准确估计所需的计算量减少约20倍。

Conclusion: 模型信念应作为从LLM生成数据中提取更多信息的默认测量方法，能显著提高统计效率和计算效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [26] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 论文研究了规划与验证领域中两个问题的联系：一致性规划与超属性模型检测，证明了两者之间存在紧密的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索规划领域中的一致性规划问题与验证领域中的超属性模型检测问题之间的理论联系，揭示这两个看似不同领域问题的内在关联性。

Method: 方法包括两个方面：1）将超属性模型检测实例高效地规约到一致性规划实例，并证明编码的正确性和完备性；2）证明每个一致性规划问题本身就是一个超属性模型检测任务。

Result: 结果表明：1）可以高效地将∃*∀*超属性模型检测实例转换为一致性规划实例；2）反之，每个一致性规划问题都可以视为一个超属性模型检测任务，建立了两个问题之间的双向对应关系。

Conclusion: 结论是模型检测中的∃*∀*超属性验证与一致性规划问题密切相关，两者之间存在双向可规约性，这为跨领域问题求解提供了新的理论视角。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [27] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: CubeBench：一个基于魔方的生成式基准测试，用于评估LLM智能体在物理世界部署中的空间推理、长时程状态跟踪和主动探索能力，揭示现有模型在长时程规划上的根本性失败。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）智能体在数字领域表现出色，但在物理世界部署中存在显著差距，主要挑战在于形成和维护稳健的空间心理模型。研究者识别出三个阻碍这一转变的核心认知挑战：空间推理、通过心理模拟进行长时程状态跟踪，以及在部分观察下的主动探索。

Method: 引入CubeBench，一个以魔方为中心的新型生成式基准测试。采用三层诊断框架：第一层使用完整符号信息评估基础状态跟踪能力；第二层评估长时程规划；第三层仅使用部分视觉数据进行主动探索评估。通过为LLM提供外部求解器工具来隔离认知瓶颈。

Result: 对领先LLM的实验显示严重局限性：在所有长时程任务中通过率均为0.00%，暴露了长时程规划的根本性失败。通过分析失败模式，揭示了现有模型在空间推理和状态跟踪方面的关键缺陷。

Conclusion: CubeBench成功识别了LLM智能体在物理世界部署中的核心认知瓶颈，特别是长时程规划能力的缺失。通过分析失败模式，为开发更具物理基础性的智能体提供了关键指导，强调了需要改进空间推理、状态跟踪和主动探索能力。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [28] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher是一个集成交替思考和多模态思维链推理的工具集成推理智能体，能够自主决定是否以及如何调用多样化工具，无需依赖人工提示或工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流程的智能体在解决需要工具调用的现实问题时表现出有限的智能性，而能够自主推理和工具调用的工具集成推理智能体正在成为处理涉及多步外部环境交互的复杂决策任务的有力方法。

Method: MindWatcher采用交替思考范式，使模型能够在任何中间阶段在思考和工具调用之间切换，同时具备多模态思维链能力，允许在推理过程中操作图像以获得更精确的搜索结果。通过自动化数据审计和评估管道，辅以手动策划的高质量训练数据集，并构建MWE-Bench基准进行评估。

Result: 实验表明，MindWatcher通过优越的工具调用能力，匹配甚至超越了更大或更新模型的表现，同时还揭示了智能体训练中的关键见解，如智能体强化学习中的遗传继承现象。

Conclusion: MindWatcher配备了一套全面的辅助推理工具，能够解决广泛领域的多模态问题，并通过大规模高质量本地图像检索数据库和更高效的训练基础设施，在小规模模型下实现了强大的对象识别能力和训练效率提升。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [29] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: AKG kernel agent是一个多智能体系统，利用LLM代码生成能力自动化AI计算内核的开发、迁移和性能调优，支持多种DSL语言，在KernelBench测试中相比PyTorch Eager实现平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型（如LLM、多模态架构、推荐系统）对高性能计算内核的需求日益增长，加上稀疏化、量化等技术以及硬件架构的快速更新，使得手动优化无法满足需求，成为AI系统发展的瓶颈。

Method: 提出AKG kernel agent多智能体系统，利用LLM的代码生成能力自动化内核开发。系统支持多种领域特定语言（DSL），包括Triton、TileLang、CPP和CUDA-C，采用模块化设计，可快速集成新的DSL和硬件目标。

Result: 在KernelBench上使用Triton DSL在GPU和NPU后端进行评估，AKG kernel agent相比PyTorch Eager基准实现平均获得1.46倍的加速，证明了其在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过自动化内核生成、迁移和性能调优，有效解决了AI计算内核开发中的瓶颈问题，支持多种硬件后端，为现代AI工作负载提供了高效的内核开发解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [30] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: HiR提出了一种基于后见之明指令重放的样本高效强化学习框架，通过选择-重写策略将失败尝试重放为成功样本，用于复杂指令跟随任务。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在大型语言模型指令对齐中面临挑战：初始模型由于能力有限，难以生成满足所有约束的高质量响应，导致稀疏或难以区分的奖励信号，阻碍学习效率。

Method: HiR采用选择-重写策略，基于后见之明将失败尝试重放为成功样本，通过指令级和响应级的双重偏好学习框架，仅使用二元奖励信号进行高效优化。

Result: 广泛实验表明，HiR在不同指令跟随任务中取得了有希望的结果，同时需要更少的计算资源。

Conclusion: HiR是一种样本高效的强化学习框架，通过后见之明重放机制有效解决了复杂指令跟随任务中的奖励稀疏问题，提高了学习效率。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [31] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: CreativeDC是一种两阶段提示方法，通过解耦创意探索和约束满足，解决LLM生成教育问题时存在的"人工蜂群思维"效应，显著提升问题的多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育问题生成方面潜力巨大，但存在"人工蜂群思维"效应，导致同一模型内和不同模型间生成相似响应，产生同质化输出，使学生接触过于相似和重复的问题，损害思维多样性。

Method: 受Wallas创造力理论和Guilford发散-收敛思维框架启发，提出CreativeDC两阶段提示方法，将LLM推理明确分为不同阶段，通过解耦创意探索和约束满足，让LLM在确定最终问题前探索更广泛的想法空间。

Result: 评估显示CreativeDC在多样性、新颖性和实用性方面显著优于基线方法，同时保持高实用性。扩展分析表明，随着采样增加，CreativeDC生成的有效不同问题数量更多，增长速度更快。

Conclusion: CreativeDC通过结构化两阶段方法有效缓解LLM的"人工蜂群思维"问题，为教育问题生成提供了更富创造性和多样性的解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [32] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: I-PERI：一种在未知客户端干预下的联邦因果发现算法，通过恢复客户端图并集的CPDAG，再利用干预引起的结构差异定向边，得到更紧的Φ-Markov等价类


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法通常假设所有客户端共享相同的因果模型，这在实践中不现实，因为客户端特定的策略或协议（如医院间差异）会引入异质且未知的干预

Method: 提出I-PERI算法：1）首先恢复所有客户端图并集的CPDAG；2）利用干预在不同客户端引起的结构差异来定向额外的边；3）得到更紧的Φ-Markov等价类，用Φ-CPDAG表示

Result: 提供了I-PERI算法的理论收敛保证和隐私保护特性证明，并在合成数据上进行了实证评估，证明了算法的有效性

Conclusion: I-PERI能够处理联邦设置中未知的客户端级干预，通过利用干预引起的结构差异获得比传统方法更精确的因果结构表示

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [33] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Web World Model (WWM) 是一种中间方案，通过普通网页代码实现世界状态和"物理规则"以确保逻辑一致性，同时让大语言模型在结构化潜在状态上生成上下文、叙事和高级决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两极分化：传统Web框架提供可靠但固定的数据库支持环境，而完全生成的世界模型追求无限环境但牺牲了可控性和实际工程可行性。需要一种兼顾逻辑一致性和开放探索性的中间方案。

Method: 构建基于真实Web技术栈的WWM套件，包括基于真实地理的无限旅行地图、虚构星系探索器、Web规模百科全书和叙事世界、模拟和游戏环境。采用代码定义规则与模型驱动想象分离、潜在状态作为类型化Web接口表示、确定性生成实现无限但结构化探索等设计原则。

Result: 成功构建了多个WWMs系统，展示了Web技术栈本身可以作为世界模型的可扩展基底，实现了可控且开放的环境。

Conclusion: Web技术栈可以作为世界模型的可扩展基底，实现既可控又开放的环境，为语言智能体提供持久世界，使其能够在其中行动、记忆和学习。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>
