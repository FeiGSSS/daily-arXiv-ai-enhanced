<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: 该研究提出DDFT协议评估语言模型的认知稳健性，发现模型规模与稳健性无关，错误检测能力是关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估主要测量理想条件下的知识掌握程度，但无法评估在现实压力下（如信息退化或对抗性攻击时）的知识稳健性。现有静态基准测试无法区分缺乏知识的模型和验证机制在压力下崩溃的模型。

Method: 引入Drill-Down和Fabricate Test（DDFT）协议，测量认知稳健性：模型在渐进语义压缩和对抗性伪造下保持事实准确性的能力。提出两系统认知模型：语义系统生成流畅文本，认知验证器验证事实准确性。评估9个前沿模型在8个知识领域和5个压缩级别（共1800次轮级评估）。

Result: 认知稳健性与传统设计范式正交。参数数量（r=0.083, p=0.832）和架构类型（r=0.153, p=0.695）均不显著预测稳健性，表明其源于训练方法和验证机制。错误检测能力强烈预测整体稳健性（rho=-0.817, p=0.007），是关键瓶颈。旗舰模型尽管规模大但表现脆弱，而较小模型可实现稳健性能。

Conclusion: DDFT框架为评估认知稳健性提供了理论基础和实用工具，有助于在关键应用部署前评估模型可靠性，挑战了模型规模与可靠性关系的传统假设。

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


### [2] [CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution](https://arxiv.org/abs/2512.23880)
*Xu Huang,Junwu Chen,Yuxing Fei,Zhuohan Li,Philippe Schwaller,Gerbrand Ceder*

Main category: cs.AI

TL;DR: CASCADE是一个自我进化的智能体框架，通过持续学习和自我反思两大元技能，使LLM智能体能够掌握复杂外部工具并编码知识，在科学任务中实现从"LLM+工具使用"到"LLM+技能获取"的转变。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体依赖预定义工具或脆弱的工具生成，限制了其在复杂科学任务中的能力和适应性。需要一种能够自我进化、掌握外部工具并积累可执行技能的系统。

Method: CASCADE框架包含两大元技能：1) 通过网页搜索和代码提取实现持续学习；2) 通过内省和知识图谱探索实现自我反思。系统还支持人机协作和记忆巩固，积累可共享的可执行技能。

Result: 在包含116个材料科学和化学研究任务的SciSkillBench基准测试中，使用GPT-5的CASCADE实现了93.3%的成功率，而没有进化机制的基线仅为35.4%。系统在计算分析、自主实验室实验和论文选择性复现等实际应用中表现出色。

Conclusion: CASCADE通过积累可跨智能体和科学家共享的可执行技能，实现了AI辅助科学研究的可扩展性，代表了从"LLM+工具使用"向"LLM+技能获取"的重要转变。

Abstract: Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from "LLM + tool use" to "LLM + skill acquisition". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.

</details>


### [3] [A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming](https://arxiv.org/abs/2512.23932)
*Ioanna Gemou,Evangelos Lamprou*

Main category: cs.AI

TL;DR: McCoy框架结合大语言模型和答案集编程，通过LLM将医学文献转化为ASP代码，结合患者数据进行疾病诊断，提供可解释的预测系统。


<details>
  <summary>Details</summary>
Motivation: 准确的疾病预测对于及时干预、有效治疗和减少医疗并发症至关重要。虽然符号AI已在医疗保健中应用，但由于构建高质量知识库需要大量努力，其采用仍然有限。

Method: McCoy框架结合大型语言模型（LLMs）与答案集编程（ASP）。它协调LLM将医学文献翻译成ASP代码，将其与患者数据结合，并使用ASP求解器处理以得出最终诊断。

Result: 初步结果显示，McCoy在小规模疾病诊断任务上表现出色，提供了一个强大且可解释的预测框架。

Conclusion: McCoy通过结合LLMs和ASP，克服了符号AI在医疗领域应用的知识库构建障碍，创建了一个利用两种范式优势的鲁棒、可解释的疾病预测系统。

Abstract: Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.

</details>


### [4] [SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing](https://arxiv.org/abs/2512.24008)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.AI

TL;DR: SPARK是一个基于多智能体LLM的个性化搜索框架，通过角色化智能体协作实现动态检索和个性化，模拟人类信息寻求行为的复杂性和流动性。


<details>
  <summary>Details</summary>
Motivation: 个性化搜索需要建模用户不断演化的多维信息需求，但现有系统受限于静态用户画像或单一检索流程，无法捕捉人类信息寻求行为的复杂性、流动性和上下文敏感性。

Method: SPARK框架定义角色空间（角色、专业知识、任务上下文、领域），引入角色协调器动态激活相关专业智能体。每个智能体执行独立的检索增强生成过程，配备长短期记忆存储和上下文感知推理模块。智能体间通过结构化通信协议（共享内存库、迭代辩论、接力式知识传递）进行协作。

Result: 该框架产生了关于协调效率、个性化质量和认知负载分布的可测试预测，同时包含自适应学习机制用于持续角色优化。通过整合细粒度智能体专业化和协作检索，SPARK为下一代搜索系统提供了理论框架。

Conclusion: SPARK展示了如何通过分布式智能体行为和最小协调规则产生涌现的个性化特性，为能够捕捉人类信息寻求行为复杂性的下一代搜索系统提供了重要见解。

Abstract: Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.

</details>


### [5] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化智能体框架，通过将LLM集成到"计划-执行-总结"认知范式中，解决了传统进化方法在代码空间中的早熟收敛和低效探索问题，显著提高了进化效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法缺乏结构化推理，导致从静态LLM向自改进智能体过渡困难，现有方法在高维代码空间中存在早熟收敛和探索效率低的问题。

Method: LoongFlow采用"计划-执行-总结"认知范式，将LLM集成到进化搜索中，使其变为推理密集型过程。结合混合进化记忆系统，通过多岛模型、MAP-Elites和自适应玻尔兹曼选择来平衡探索与利用，保持行为多样性。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比领先基线（如OpenEvolve、ShinkaEvolve）的进化效率高出60%，同时发现了更优的解决方案。

Conclusion: LoongFlow在自主科学发现领域迈出了重要一步，能够以更低的计算开销生成专家级解决方案，标志着从静态LLM到自进化智能体的重要进展。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


### [6] [Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks](https://arxiv.org/abs/2512.24156)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的基于图的方法，用于解决ARC-AGI-3基准测试中的交互推理任务。该方法结合视觉帧处理与系统化的状态空间探索，在基准测试中显著优于前沿LLM方法。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI-3基准测试包含类似游戏的任务，要求智能体通过有限交互推断任务机制并适应逐渐增加的复杂性。现有最先进的LLM无法可靠解决这些任务，因此需要探索新的方法。

Method: 方法结合视觉帧处理与基于图的系统化状态空间探索：1）将视觉帧分割为有意义的组件；2）基于视觉显著性对动作进行优先级排序；3）维护已探索状态和转换的有向图；4）通过跟踪已访问状态和已测试动作，优先选择到达未测试状态-动作对的最短路径动作。

Result: 在ARC-AGI-3预览挑战中，该方法在6个游戏的52个关卡中解决了中位数30个关卡，在私有排行榜上排名第3，显著优于前沿的基于LLM的智能体。

Conclusion: 即使没有学习，显式的基于图的探索也可以作为交互推理的强大基线，强调了在稀疏反馈环境中系统化状态跟踪和动作优先级排序的重要性，这些环境中当前LLM无法捕捉任务动态。

Abstract: We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.

</details>


### [7] [Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment](https://arxiv.org/abs/2512.24263)
*Lijun Zhang,Lin Li,Wei Wei,Yajie Qi,Huizhong Song,Jun Wang,Yaodong Yang,Jiye Liang*

Main category: cs.AI

TL;DR: 提出Risk-aware Stepwise Alignment (RSA)方法，通过嵌套风险度量将风险意识融入语言模型对齐过程，解决传统风险中性方法的不足，有效控制罕见但灾难性的有害行为。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法（如Safe RLHF和SACPO）通常采用风险中性范式，无法充分处理参考策略偏差带来的风险，对罕见但潜在灾难性有害行为的鲁棒性有限。需要一种能明确纳入风险意识的对齐方法。

Method: 提出风险感知逐步对齐(RSA)方法：1) 将安全对齐表述为令牌级风险感知约束策略优化问题；2) 通过逐步对齐程序解决，利用嵌套风险度量推导令牌级策略更新；3) 设计可缓解模型过度偏离参考策略的风险，并明确抑制低概率高影响有害行为。

Result: 实验结果表明，该方法在保持高水平帮助性的同时确保强安全性，显著抑制尾部风险（低概率但高影响的不安全响应）。理论分析在温和假设下证明了策略最优性。

Conclusion: RSA方法通过将风险意识明确纳入对齐过程，有效解决了传统风险中性方法的局限性，在控制模型偏离风险和抑制罕见灾难性行为方面表现出色，为语言模型安全对齐提供了新思路。

Abstract: When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.

</details>


### [8] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: 该研究系统分析了基于世界模型的表示空间规划方法（JEPA-WMs），通过实验确定了最优的模型架构、训练目标和规划算法组合，提出的模型在导航和操作任务上超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: AI领域长期面临开发能够解决广泛物理任务并在新任务和环境中泛化的智能体的挑战。现有方法通常在世界模型的状态-动作轨迹上进行训练，然后使用规划算法解决新任务。虽然传统规划在输入空间进行，但新兴方法在表示空间进行规划，有望通过抽象无关细节实现更高效的规划。

Method: 将这类方法统一归类为JEPA-WMs，并系统研究其中的关键技术选择：1）模型架构；2）训练目标；3）规划算法。通过模拟环境和真实机器人数据进行实验，分析这些组件如何影响规划成功率。

Result: 结合研究发现提出了一个优化模型，在导航和操作任务上超越了DINO-WM和V-JEPA-2-AC两个基准模型。代码、数据和检查点已开源。

Conclusion: 通过系统分析JEPA-WMs家族中的技术选择，确定了最优的模型架构、训练目标和规划算法组合，为表示空间规划方法的发展提供了重要指导。

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>


### [9] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 该研究分析了GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3三种领先大语言模型在密苏里大学数学竞赛问题上的表现，发现DeepSeek-V3在所有三个数学领域表现最佳，但所有模型在几何问题上都表现较弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用相同的数据集评估大语言模型的数学推理能力，这限制了研究结果的普适性，且可能无法完全捕捉数学任务中的多样化挑战。本研究旨在通过分析大语言模型在代表性不足的数学竞赛问题上的表现，更全面地评估其数学推理能力。

Method: 研究使用密苏里大学数学竞赛中的微积分、解析几何和离散数学问题，对GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3三种大语言模型进行测试。将模型的回答与已知正确答案进行比较以确定准确性，并分析模型的推理过程以探索不同问题类型和模型间的错误模式。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个类别中表现最佳，无论是推理过程还是最终答案正确率都最好。所有三种大语言模型在几何问题上都表现出明显的弱势。DeepSeek-V3的主要错误源于计算和逻辑错误，GPT-4o-mini经常出现逻辑和方法相关的错误，而Gemini则倾向于推理不完整和匆忙得出结论。

Conclusion: 在代表性不足的数学竞赛数据集上评估大语言模型，可以更深入地了解它们独特的错误模式，并突显在结构化推理方面持续存在的挑战，特别是在几何领域。这种评估方法有助于更全面地理解大语言模型的数学推理能力局限。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [10] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 论文提出了一种两阶段方法，将空间推理分解为原子构建块及其组合，通过监督微调学习基础空间物理，然后训练轻量级LoRA适配器进行多步规划，在ASCII艺术环境中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用语言能力上表现出色，但在结构化环境中的空间变换和多步规划方面仍然存在困难，这限制了它们在导航和规划等应用中的实际效果。

Method: 采用两阶段方法：1）对基本空间变换（如旋转、平移、缩放）进行监督微调，使模型具备基础空间物理知识；2）冻结该物理感知模型，在GRPO框架内训练轻量级LoRA适配器，以学习在基于谜题的环境中组合这些构建块进行多步规划的策略。为此合成了ASCII艺术数据集并构建了相应的强化学习环境。

Result: 该方法在动态环境（有显式状态更新）和静态环境（模型必须依赖其内部状态）中均一致优于基线方法，包括通用骨干模型、物理感知模型和端到端RL模型。此外，该方法收敛更快，训练更稳定，注意力模式分析显示微调确实带来了空间理解的实质性改进。

Conclusion: 将空间推理分解为原子构建块及其组合的两阶段方法能有效提升LLMs在结构化环境中的空间推理能力，为导航和规划等应用提供了更可靠的解决方案。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [11] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: RLMs是一种新的推理策略，让LLMs能够通过递归调用自身处理远超上下文窗口的长提示，性能显著优于基础模型和常见长上下文框架。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的上下文窗口有限，无法处理任意长度的提示，这限制了它们处理长文档、复杂任务的能力。

Method: 提出递归语言模型（RLMs），将长提示视为外部环境，让LLM能够以编程方式检查、分解提示，并递归调用自身处理提示片段。

Result: RLMs成功处理超出模型上下文窗口两个数量级的输入，在四个不同的长上下文任务中，即使对于较短的提示，也显著优于基础LLMs和常见长上下文框架，且每次查询成本相当或更低。

Conclusion: RLMs为LLMs处理任意长度提示提供了一种有效的推理时扩展方法，在保持成本效益的同时显著提升了长上下文处理能力。

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [12] [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)
*Dong Qiu,Duo Xu,Limengxi Yue*

Main category: cs.AI

TL;DR: 提出一个强化学习增强的LLM多智能体框架，通过Dec-POMDP建模协作，采用CTDE训练，引入GRPO优化策略，在协作写作和编码任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言任务中表现良好，但在多智能体环境中缺乏协作意识，难以优化全局性能。需要一种能够促进智能体间有效协作的框架。

Method: 1. 将协作建模为分散部分可观测马尔可夫决策过程（Dec-POMDP）；2. 采用集中训练分散执行（CTDE）架构；3. 引入组相对策略优化（GRPO）联合优化智能体策略；4. 设计简化的联合奖励函数平衡任务质量、速度和协调成本。

Result: 在协作写作和编码基准测试中：1. 任务处理速度比单智能体基线提高3倍；2. 写作任务中结构和风格一致性达到98.7%；3. 编码任务中测试通过率达到74.6%；4. 持续优于其他多智能体LLM基线。

Conclusion: 该框架为复杂工作流中的可靠协作提供了实用路径，通过强化学习增强LLM智能体在多智能体环境中的协作能力，显著提升了全局性能。

Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.

</details>


### [13] [Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions](https://arxiv.org/abs/2512.24679)
*Pengcheng Xia,Yixiang Huang,Chengjin Qin,Chengliang Liu*

Main category: cs.AI

TL;DR: 提出多模态跨域混合融合模型，通过双重解耦框架分离模态不变/特定特征和域不变/特定表示，结合跨域混合融合策略增强多样性，用于未见工况下的故障诊断。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法在未见工况下性能显著下降，域自适应方法依赖目标域样本，且大多依赖单模态信号，忽略了多模态信息的互补性。

Method: 提出多模态跨域混合融合模型，包含：1) 双重解耦框架分离模态不变/特定特征和域不变/特定表示；2) 跨域混合融合策略随机混合跨域模态信息；3) 三模态融合机制自适应集成多模态异构信息。

Result: 在感应电机故障诊断实验中，无论是恒定还是时变未见工况下，该方法均优于先进方法，消融研究验证了各组件和多模态融合的有效性。

Conclusion: 该方法通过双重解耦和跨域混合融合，实现了多模态信息的有效利用和域泛化能力的提升，为未见工况下的故障诊断提供了有效解决方案。

Abstract: Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.

</details>


### [14] [BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis](https://arxiv.org/abs/2512.24686)
*Songqi Zhou,Ruixue Liu,Boman Su,Jiazhou Wang,Yixing Wang,Benben Jiang*

Main category: cs.AI

TL;DR: 本文提出BatteryAgent框架，将物理知识特征与大型语言模型推理能力结合，实现锂电池故障的智能诊断，显著提升检测性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法的"黑箱"特性限制了可解释性，且受限于二元分类范式，难以提供根本原因分析和维护建议，需要更智能的诊断框架。

Method: 提出三层框架：物理感知层提取10个基于电化学原理的特征；检测与归因层使用梯度提升决策树和SHAP量化特征贡献；推理与诊断层利用LLM作为智能体核心，构建"数值-语义"桥梁生成综合诊断报告。

Result: 实验结果显示BatteryAgent有效纠正硬边界样本的误分类，AUROC达到0.986，显著优于当前最先进方法，并将传统二元检测扩展到多类型可解释诊断。

Conclusion: 该框架为电池安全管理提供了从"被动检测"到"智能诊断"的新范式转变，实现了故障类型识别、根本原因分析和维护建议的综合诊断能力。

Abstract: Fault diagnosis of lithium-ion batteries is critical for system safety. While existing deep learning methods exhibit superior detection accuracy, their "black-box" nature hinders interpretability. Furthermore, restricted by binary classification paradigms, they struggle to provide root cause analysis and maintenance recommendations. To address these limitations, this paper proposes BatteryAgent, a hierarchical framework that integrates physical knowledge features with the reasoning capabilities of Large Language Models (LLMs). The framework comprises three core modules: (1) A Physical Perception Layer that utilizes 10 mechanism-based features derived from electrochemical principles, balancing dimensionality reduction with physical fidelity; (2) A Detection and Attribution Layer that employs Gradient Boosting Decision Trees and SHAP to quantify feature contributions; and (3) A Reasoning and Diagnosis Layer that leverages an LLM as the agent core. This layer constructs a "numerical-semantic" bridge, combining SHAP attributions with a mechanism knowledge base to generate comprehensive reports containing fault types, root cause analysis, and maintenance suggestions. Experimental results demonstrate that BatteryAgent effectively corrects misclassifications on hard boundary samples, achieving an AUROC of 0.986, which significantly outperforms current state-of-the-art methods. Moreover, the framework extends traditional binary detection to multi-type interpretable diagnosis, offering a new paradigm shift from "passive detection" to "intelligent diagnosis" for battery safety management.

</details>


### [15] [Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences](https://arxiv.org/abs/2512.24829)
*Emmanuel Fashae,Michael Burke,Leimin Tian,Lingheng Meng,Pamela Carreno-Medrano*

Main category: cs.AI

TL;DR: 该研究提出了一个可解释的家庭物品摆放偏好模型，包含四个维度：空间实用性、习惯便利性、语义连贯性和常识适当性，并通过问卷验证和MCTS规划器应用展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统依赖从人类演示中推断的潜在偏好模型，虽然预测有效但缺乏可解释性，无法理解指导人类决策的可解释因素。

Method: 设计了包含四个可解释构念（空间实用性、习惯便利性、语义连贯性、常识适当性）的自报告问卷，通过63名参与者的在线研究验证，并将这些构念集成到蒙特卡洛树搜索（MCTS）规划器中。

Result: 问卷研究证实了这四个构念的心理区分度和解释力，在厨房和客厅两种场景中都有效。基于参与者偏好指导的MCTS规划器能够生成与参与者安排高度一致的合理物品摆放方案。

Conclusion: 该研究贡献了一个紧凑、可解释的物品摆放偏好模型，并展示了如何将其操作化用于机器人规划，为可解释的家庭机器人系统提供了理论基础。

Abstract: Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.

</details>


### [16] [GenZ: Foundational models as latent variable generators within traditional statistical models](https://arxiv.org/abs/2512.24834)
*Marko Jojic,Nebojsa Jojic*

Main category: cs.AI

TL;DR: GenZ提出了一种混合模型，通过可解释的语义特征连接基础模型和统计建模，解决了LLM无法捕捉数据集特定模式的问题，在房价预测和电影推荐任务上显著优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然拥有广泛的领域知识，但往往无法捕捉对预测任务至关重要的数据集特定模式。现有方法过度依赖基础模型的领域理解，而忽略了从数据中发现的统计模式。

Method: 通过迭代过程发现语义特征描述，该过程基于统计建模错误对比项目组，而不是单纯依赖基础模型的领域知识。采用广义EM算法联合优化语义特征描述符和统计模型参数，将冻结的基础模型用于基于发现特征的物品分类，将这些判断视为预测实值目标的潜在二元特征的噪声观测。

Result: 在房价预测任务中，使用多模态列表数据发现的语义特征实现了12%的中位数相对误差，显著优于依赖LLM通用领域知识的GPT-5基线（38%误差）。在Netflix电影嵌入预测中，仅从语义描述就能达到0.59的余弦相似度，相当于传统协同过滤需要约4000个用户评分才能达到的性能。

Conclusion: GenZ成功地将基础模型的领域知识与统计建模的数据驱动发现相结合，发现了数据集特定的模式（如预测本地房地产市场的建筑细节、预测用户偏好的系列电影成员资格），这些模式与模型单独的领域知识有所不同，证明了混合方法的有效性。

Abstract: We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.

</details>


### [17] [A study on constraint extraction and exception exclusion in care worker scheduling](https://arxiv.org/abs/2512.24853)
*Koki Suenaga,Tomohiro Furuta,Satoshi Ono*

Main category: cs.AI

TL;DR: 提出一种基于约束模板的方法，用于从养老院管理者访谈中提取设施特定的排班约束条件，并排除例外约束，以生成满足硬约束并减少软约束违反的护理人员排班表。


<details>
  <summary>Details</summary>
Motivation: 养老机构的排班条件因设施而异，需要通过与制定排班的管理者访谈来设计设施特定的约束条件。现有约束提取技术缺乏排除例外约束的机制。

Method: 使用约束模板提取各种组件的组合，如连续工作日的班次模式或员工组合。模板通过改变关注的天数和员工数量，以及将提取焦点改为模式或频率，来提取多种约束。同时包含排除例外约束的机制。

Result: 实验表明，该方法成功创建了满足所有硬约束的排班表，并通过避免提取例外约束，减少了软约束的违反次数。

Conclusion: 提出的约束模板方法能够有效提取养老机构特定的排班约束条件，并排除例外约束，从而生成更符合实际需求的护理人员排班表。

Abstract: Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.

</details>


### [18] [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/abs/2512.24873)
*Weixun Wang,XiaoXiao Xu,Wanhe An,Fangwen Dai,Wei Gao,Yancheng He,Ju Huang,Qiang Ji,Hanqi Jin,Xiaoyang Li,Yang Li,Zhongwen Li,Shirong Lin,Jiashun Liu,Zenan Liu,Tao Luo,Dilxat Muhtar,Yuanbin Qu,Jiaqiang Shi,Qinghui Sun,Yingshui Tan,Hao Tang,Runze Wang,Yi Wang,Zhaoguo Wang,Yanan Wu,Shaopan Xiong,Binchen Xu,Xander Xu,Yuchi Xu,Qipeng Zhang,Xixia Zhang,Haizhou Zhao,Jie Zhao,Shuaibing Zhao,Baihui Zheng,Jianhui Zheng,Suhang Zheng,Yanni Zhu,Mengze Cai,Kerui Cao,Xitong Chen,Yue Dai,Lifan Du,Tao Feng,Tao He,Jin Hu,Yijie Hu,Ziyu Jiang,Cheng Li,Xiang Li,Jing Liang,Chonghuan Liu,ZhenDong Liu,Haodong Mi,Yanhu Mo,Junjia Ni,Shixin Pei,Jingyu Shen,XiaoShuai Song,Cecilia Wang,Chaofan Wang,Kangyu Wang,Pei Wang,Tao Wang,Wei Wang,Ke Xiao,Mingyu Xu,Tiange Xu,Nan Ya,Siran Yang,Jianan Ye,Yaxing Zang,Duo Zhang,Junbo Zhang,Boren Zheng,Wanxi Deng,Ling Pan,Lin Qu,Wenbo Su,Jiamang Wang,Wei Wang,Hu Wei,Minggang Wu,Cheng Yu,Bing Zhao,Zhicheng Zheng,Bo Zheng*

Main category: cs.AI

TL;DR: ALE是一个端到端的智能体学习生态系统，包含ROLL权重优化框架、ROCK沙盒环境管理和iFlow CLI上下文工程工具，并发布了基于此训练的ROME开源智能体模型。


<details>
  <summary>Details</summary>
Motivation: 开源社区缺乏一个原则性的、端到端的生态系统来简化智能体开发，需要优化智能体LLM的生产流程。

Method: ALE包含三个组件：ROLL（权重优化后训练框架）、ROCK（轨迹生成的沙盒环境管理器）和iFlow CLI（高效上下文工程的智能体框架）。提出了数据组合协议和基于交互的策略对齐（IPA）算法，通过语义交互块而非单个token来分配信用。

Result: 发布了ROME开源智能体，在超过100万条轨迹上训练。在SWE-bench Verified和Terminal Bench等基准测试中表现优异，证明了ALE基础设施的有效性。

Conclusion: ALE提供了一个优化的智能体LLM生产管道，ROME智能体的成功验证了该生态系统的有效性，为开源智能体开发提供了完整的基础设施。

Abstract: Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.

</details>


### [19] [Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing](https://arxiv.org/abs/2512.24896)
*Andrii Gamalii,Daniel Górniak,Robert Nowak,Bartłomiej Olber,Krystian Radlak,Jakub Winter*

Main category: cs.AI

TL;DR: DARTS项目开发了一个半自动数据标注流水线，用于创建波兰驾驶场景的大规模多模态数据集，通过人机协同方法显著降低标注成本和时间。


<details>
  <summary>Details</summary>
Motivation: 手动标注异构驾驶数据成本高、耗时长，需要开发自动化解决方案来加速波兰自动驾驶研究所需的大规模数据集创建。

Method: 采用人在回路方法，结合AI与人类专业知识，包括自动生成初始标注、迭代模型重训练、数据匿名化和领域适应技术，核心使用3D目标检测算法。

Result: 开发的工具和方法实现了显著的时间节省，同时确保跨不同传感器模态的一致高质量标注，加速了DARTS项目中标准化格式的大规模标注数据集准备。

Conclusion: 该半自动标注流水线有效支持DARTS项目，为波兰自动驾驶研究强化了技术基础，通过人机协同方法解决了大规模驾驶数据标注的效率和成本挑战。

Abstract: This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.

</details>


### [20] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. Corrêa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,André G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: 迭代部署LLM模型，通过用户从前一模型部署中精心筛选数据进行微调，能显著改变模型特性，在规划任务中实现能力提升和泛化能力涌现。


<details>
  <summary>Details</summary>
Motivation: 研究迭代部署LLM模型对模型特性的影响，探索通过用户数据筛选而非显式奖励的替代训练机制，并分析其与强化学习的理论联系及AI安全意义。

Method: 在多个规划领域测试迭代部署机制：每个新模型基于用户从前一模型部署中精心筛选的数据进行微调，观察模型规划能力的变化。

Result: 后续模型在规划技能上取得显著提升，展现出涌现的泛化能力，能够发现比初始模型长得多的规划方案。

Conclusion: 迭代部署本质上实现了外层强化学习训练，具有隐式奖励函数；这对AI安全有重要启示（奖励函数未明确定义），同时提供了一种基于数据筛选而非显式奖励的替代训练机制。

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


### [21] [AMAP Agentic Planning Technical Report](https://arxiv.org/abs/2512.24957)
*Yulan Hu,Xiangwen Zhang,Sheng Ouyang,Hao Yi,Lu Xu,Qinglin Lang,Lide Tan,Xiang Cheng,Tianchen Ye,Zhicong Li,Ge Chen,Wenjin Yang,Zheng Pan,Shaopan Xiong,Siran Yang,Ju Huang,Yan Zhang,Jiamang Wang,Yong Liu,Yinfeng Huang,Tucheng Lin,Xin Li,Ning Guo*

Main category: cs.AI

TL;DR: STAgent是一个专门用于时空理解的智能大语言模型，能够处理复杂的约束性兴趣点发现和行程规划任务，通过工具交互和多阶段训练实现高性能，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂时空场景下的任务，如约束性兴趣点发现和行程规划，需要专门设计的智能体模型。现有模型在处理这类需要多工具交互和复杂推理的任务时存在局限性。

Method: 1. 构建包含10个以上领域特定工具的稳定工具环境，支持异步训练；2. 分层数据筛选框架，从海量数据中筛选高质量查询（筛选比例1:10,000）；3. 级联训练方案：种子SFT阶段评估查询难度，第二SFT阶段针对高确定性查询微调，最终RL阶段利用低确定性数据。

Result: STAgent在TravelBench上表现出色，同时在广泛的通用基准测试中保持了其通用能力，证明了所提出的智能体模型的有效性。模型基于Qwen3-30B-A3B初始化，建立了强大的SFT基础。

Conclusion: STAgent通过专门的工具环境、高质量数据筛选和级联训练方案，成功构建了一个在时空理解任务上表现优异且保持通用能力的智能体模型，为复杂时空任务提供了有效的解决方案。

Abstract: We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.

</details>
