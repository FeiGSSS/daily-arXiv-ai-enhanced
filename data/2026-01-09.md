<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出多智能体工作流，将主要任务描述与约束条件解耦优化，通过定量评分反馈迭代改进提示词，显著提升LLM输出合规性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成内容相关但不符合形式约束的输出，传统提示词优化方法主要关注重述主要任务描述，忽略了作为响应验收标准的细粒度约束条件

Method: 提出新颖的多智能体工作流，将主要任务描述的优化与其约束条件解耦，使用定量评分作为反馈，迭代重写和改进提示词

Result: 评估表明该方法生成的修订提示词在Llama 3.1 8B和Mixtral-8x 7B等模型上产生了显著更高的合规性分数

Conclusion: 通过解耦任务描述与约束条件优化，采用多智能体工作流和定量反馈机制，可以有效提升LLM输出对形式约束的遵循程度

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [2] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 论文提出了一种基于内省探索的强化学习智能体框架，通过隐马尔可夫模型推断"疼痛信念"作为学习信号，研究自我意识如何影响智能体的学习能力，并比较正常与慢性疼痛感知模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能如何建模内部心理状态对于推进AI中的心智理论至关重要。证据表明自我认知和他人认知存在统一系统，本研究通过让强化学习智能体在网格世界中推断自身内部状态来探索这种自我意识。

Method: 引入受生物疼痛启发的内省探索组件，使用隐马尔可夫模型从在线观察中推断"疼痛信念"，将该信号整合到主观奖励函数中，建立计算框架来研究正常与慢性疼痛感知模型的性能差异。

Result: 内省智能体总体上显著优于标准基线智能体，能够复现复杂的人类行为模式，展示了自我意识模型在强化学习中的有效性。

Conclusion: 通过基于疼痛信念的内省探索框架，成功展示了自我意识模型如何提升智能体的学习能力，为AI心智理论的发展提供了计算实证，并揭示了正常与慢性疼痛感知在智能体行为中的差异。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [3] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 提出基于成熟度的认证框架，通过显式测量机制对具身AI系统进行认证，以不确定性量化为例证，通过无人机系统检测案例验证可行性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对具身AI系统的结构化认证框架，需要可量化的评估机制来解决可信度评估中的多目标权衡问题。

Method: 提出基于成熟度的认证框架，包含结构化评估框架、定量评分机制和多目标权衡导航方法，以不确定性量化作为示例测量机制，通过无人机系统检测案例进行验证。

Result: 通过无人机系统检测案例研究，证明了该认证框架的可行性和实用性，展示了如何应用不确定性量化等测量机制进行系统认证。

Conclusion: 基于成熟度的认证框架为具身AI系统的可信度评估提供了结构化方法，通过显式测量机制和案例验证展示了其在解决多目标权衡问题上的有效性。

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [4] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: CPGPrompt系统将临床指南转化为结构化决策树，利用大语言模型动态导航进行患者病例评估，在专科转诊决策上表现优异，但在多类路径分类上表现有差异。


<details>
  <summary>Details</summary>
Motivation: 临床实践指南（CPGs）为患者护理提供循证建议，但将其整合到人工智能中仍面临挑战。先前基于规则的系统存在可解释性差、指南依从性不一致和领域适用性窄等限制。

Method: 开发CPGPrompt自动提示系统，将叙述性临床指南转化为结构化决策树，利用大语言模型动态导航进行患者病例评估。在三个领域（头痛、下背痛和前列腺癌）生成合成案例，测试不同决策场景。

Result: 二元专科转诊分类在所有领域表现一致强劲（F1：0.85-1.00），召回率高（1.00±0.00）。多类路径分配表现降低，领域间有差异：头痛（F1：0.47）、下背痛（F1：0.72）、前列腺癌（F1：0.77）。性能差异反映了各指南的结构特点。

Conclusion: CPGPrompt系统在将临床指南整合到AI中显示出潜力，特别是在二元决策任务上表现优异。不同指南结构对系统性能有显著影响，未来需要针对特定指南特点进行优化。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [5] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型在医疗AI中展现出潜力，但无法替代N-of-1试验。两者互补：基础模型擅长从人群数据中快速生成假设，而N-of-1试验则是个体因果验证的金标准。本文提出结合两者的混合框架来解决个性化医疗中的悖论。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在医疗AI的预防、诊断和治疗中展现出变革潜力，但能否提供真正的个性化治疗建议仍是一个开放性问题。研究发现个性化医疗面临多重挑战，包括泛化性悖论、隐私-性能悖论、规模-特异性悖论和自动化-同理心悖论。此外，个性化推荐所需的因果理解程度也尚未明确。

Method: 提出一个混合框架，结合大型基础模型和N-of-1试验的优势。大型基础模型利用多模态数据从人群模式中快速生成干预候选假设并提供不确定性估计，然后触发后续的N-of-1试验进行个体层面的因果验证。N-of-1试验作为交叉自我实验，是个体因果推断的金标准，通过本地实验保护隐私。

Result: 该框架能够解决个性化医疗中的多个悖论：通过N-of-1试验解决泛化性悖论，通过本地实验保护隐私解决隐私-性能悖论，结合人群模式和个体验证解决规模-特异性悖论，同时平衡自动化和同理心需求。明确区分预测和因果关系的边界对于负责任地整合AI到个性化医疗至关重要。

Conclusion: 大型基础模型无法替代N-of-1试验，但两者是互补的。结合两者的混合框架能够实现真正的个性化医疗，同时解决个性化医疗中的多重悖论。明确预测与因果的界限，并直接应对这些悖论性张力，对于负责任地将AI整合到个性化医疗中至关重要。

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [6] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: STAR-S框架通过自学习循环整合安全规则推理学习，有效防御LLM越狱攻击


<details>
  <summary>Details</summary>
Motivation: 当前通过训练模型在响应前进行安全规则推理的方法存在局限性，难以明确设计或直接获取有效的安全推理形式来防御越狱攻击

Method: 提出STAR-S框架，通过自学习循环整合安全规则推理学习：在安全规则指导下引发推理和反思，然后利用微调增强安全推理能力，形成协同循环

Result: 实验表明STAR-S能有效防御越狱攻击，性能优于基线方法

Conclusion: STAR-S框架通过自学习安全规则推理的方式，为LLM安全部署提供了有效的防御机制

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [7] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 本文提出神经符号框架评估LLM推理过程，发现扩展token生成并非深度推理必要条件，混合长短CoT训练会导致过早饱和，小模型蒸馏只能复制行为长度但无法重现逻辑效能。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估方法存在局限，无法区分性能提升是源于真实推理还是仅仅增加了输出长度，需要更全面的过程中心评估框架来深入理解LLM的推理能力。

Method: 提出非侵入式的神经符号框架进行过程中心推理评估，识别四种行为原型并诊断失败模式，分析推理模式、训练策略和模型规模的影响。

Result: 发现扩展token生成不是深度推理的必要条件；混合长短CoT数据训练会导致过早饱和和崩溃；小模型蒸馏能复制行为长度但无法重现逻辑效能，受限于内在能力限制。

Conclusion: 需要更精细的评估方法来区分真实推理与表面行为，训练策略需要谨慎设计以避免过早饱和，模型能力限制是逻辑效能复现的关键瓶颈。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [8] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: 本文提出SAE-Steering方法，通过稀疏自编码器解耦大推理模型的隐藏状态，识别策略特定特征作为控制向量，有效控制推理策略，提升推理可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型能自主选择推理策略，但这种自主选择常产生低效甚至错误的推理路径。现有方法难以控制细粒度推理策略，因为策略概念在隐藏状态中纠缠在一起。需要开发控制推理策略的方法来提高推理的可靠性和灵活性。

Method: 使用稀疏自编码器将策略纠缠的隐藏状态解耦到解耦特征空间。提出SAE-Steering两阶段特征识别流程：首先通过放大策略特定关键词的logits召回特征（过滤掉99%以上特征），然后按控制效果对剩余特征排序。将识别出的策略特定特征作为控制向量。

Result: SAE-Steering在控制效果上比现有方法高出15%以上。控制推理策略可以将大推理模型从错误路径重定向到正确路径，实现7%的绝对准确率提升。

Conclusion: 通过SAE-Steering方法有效控制大推理模型的推理策略，显著提升推理可靠性和准确性，为解决推理策略自主选择带来的问题提供了有效方案。

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [9] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: SafeRemind是一种解码时防御方法，通过动态注入安全提醒短语到大型推理模型的思维步骤中，显著提升模型安全性而不影响推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维步骤取得显著成功，但这些思维步骤可能放大不安全行为。传统防御机制因忽视推理模型的独特推理动态而失效，研究发现思维步骤中出现的安全提醒短语对确保模型安全起关键作用。

Method: 提出SafeRemind方法，在解码时动态将安全提醒短语注入思维步骤。利用熵触发器在决策锁定点进行干预，将潜在有害轨迹重定向到更安全的结果，无需参数更新。

Result: 在5个大型推理模型和6个基准测试上的广泛评估表明，SafeRemind显著增强安全性，提升幅度高达45.5个百分点，同时保持核心推理效用。

Conclusion: SafeRemind通过动态注入安全提醒短语到思维步骤，有效解决了大型推理模型的安全漏洞，在保持推理能力的同时大幅提升安全性。

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [10] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: SandwichR通过"答案-推理-答案"范式解决查询纠错中的延迟-准确率权衡问题，在保持CoT推理准确率的同时降低40-70%延迟


<details>
  <summary>Details</summary>
Motivation: 现代搜索管道中的查询纠错需要高准确率和实时延迟约束。CoT推理能提高准确率但延迟过高，而先输出答案再推理的方法无法利用推理能力改进准确率

Method: 提出Sandwich Reasoning方法，采用答案-推理-答案范式，通过一致性感知的强化学习策略（一致性奖励和基于边界的拒绝采样）对齐初始答案与后验推理

Result: SandwichR达到与标准CoT相当的SOTA准确率，同时实现40-70%的延迟降低，解决了在线搜索中的延迟-准确率权衡问题

Conclusion: SandwichR通过显式对齐快速初始答案与后验推理，实现了低延迟查询纠错而不牺牲推理感知的准确率，并构建了高质量的查询纠错数据集

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [11] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 本文提出使用LLM自动生成领域特定启发式函数，显著提升了个性化药物规划的可扩展性，从最多7种药物扩展到至少28种药物。


<details>
  <summary>Details</summary>
Motivation: 个性化药物规划需要为每位患者选择药物并确定给药方案以实现医疗目标。先前工作使用通用领域无关启发式函数，但实践中最多只能处理7种药物，这在临床应用中远远不够。

Method: 通过编程方式指定领域（初始状态和状态转移过程），使用LLM生成问题特定的启发式函数，结合固定的搜索算法（GBFS）进行药物规划。

Result: 该方法在覆盖率和规划时间上取得了显著改进，能够处理至少28种药物，大大提升了药物规划的实际应用潜力。

Conclusion: 自动生成的领域特定启发式函数能够显著扩展个性化药物规划的规模，使其更接近临床应用的实际需求。

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [12] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT：通过熵基分割和蒙特卡洛评估自动识别和优化低质量思维链监督数据，构建高质量数学推理数据集


<details>
  <summary>Details</summary>
Motivation: 现有思维链微调数据集存在"答案正确但推理错误"问题，即最终答案正确但中间步骤存在幻觉、冗余或逻辑错误，需要自动识别和过滤低质量推理样本

Method: 提出EntroCoT统一框架：1）基于熵的机制在不确定节点分割推理轨迹；2）蒙特卡洛rollout机制评估每个步骤对最终答案的边际贡献；3）准确过滤欺骗性推理样本，构建高质量数据集

Result: 在数学基准测试上的广泛实验表明，使用EntroCoT构建的子集进行微调，性能始终优于全数据集监督的基线方法

Conclusion: EntroCoT能有效识别和优化低质量思维链监督数据，构建的高质量数据集显著提升大语言模型的数学推理能力

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [13] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: 论文提出ROI-Reasoning框架，通过元认知微调和理性感知强化学习，让大语言模型在严格计算预算下进行推理决策，解决有序随机多选择背包问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大的推理能力，但无法自动判断不同任务所需的计算量，在严格全局token约束下需要智能分配计算资源

Method: 提出两阶段ROI-Reasoning框架：1) 元认知微调阶段，训练模型预测推理成本和预期效用；2) 理性感知强化学习阶段，在硬token预算下优化序列决策

Result: 在预算约束的数学推理基准测试中，ROI-Reasoning一致提高了总体得分，同时在严格计算预算下显著减少了遗憾

Conclusion: 该研究为LLMs引入了内在的预算感知理性能力，通过形式化为OS-MCKP问题并采用两阶段训练框架，实现了在计算约束下的智能推理资源分配

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [14] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种使用答案集编程（ASP）计算理性闭包（Rational Closure）的声明式方法，能够自动构建最小排序模型并支持查询推理，相比现有命令式实现具有更好的计算效率。


<details>
  <summary>Details</summary>
Motivation: 可废止推理关注从不完全信息中得出合理结论，KLM框架为此提供了理论基础，其中理性闭包是最重要的算法之一。现有实现多为命令式方法，本文旨在提供一种基于ASP的声明式方法，以改进计算效率并保持理论正确性。

Method: 使用答案集编程（ASP）为理性闭包提供声明式定义，通过ASP编码自动从给定知识库构建最小排序模型，并支持指定查询的推理检查。该方法将理性闭包的计算问题转化为ASP求解问题。

Result: 形式化证明了ASP编码的正确性，并通过实验评估将ASP实现与现有命令式实现（InfOCF求解器）进行性能比较。结果表明ASP方法不仅遵循理性闭包的理论基础，而且在计算效率上有所提升。

Conclusion: 基于ASP的声明式方法为计算理性闭包提供了有效的替代方案，它保持了理论正确性，同时在计算效率上优于现有的命令式实现，为可废止推理的实际应用提供了更好的工具支持。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [15] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 使用ASP对意大利刑法典建模，通过半自动学习司法判例生成法律规则，支持刑事审判推理和解释


<details>
  <summary>Details</summary>
Motivation: 为法律专家在刑事审判阶段提供推理支持和可能的法律结果，使司法决策过程更加可解释

Method: 使用答案集编程（ASP）分析和编码意大利刑法典条款，包括"人身犯罪"和财产犯罪；利用稳定模型的"支持性"提供解释；集成归纳逻辑编程系统从案例中泛化法律规则

Result: 开发了支持法律推理的工具，能够处理编码过程中的矛盾，为新案件生成可能的决策并提供解释；通过先前判决验证模型并进行了必要改进

Conclusion: ASP方法能有效建模刑法典并学习法律规则，工具提供的自动可解释性有助于澄清司法决策逻辑，使决策过程更加透明和可理解

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [16] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程（ASP）的方法，用于为决策树模型生成多种类型的解释，包括充分解释、对比解释、多数解释和树特定解释，相比SAT方法更灵活且支持枚举所有可能解释。


<details>
  <summary>Details</summary>
Motivation: 决策树模型（如随机森林和梯度提升决策树）在机器学习中广泛应用，但其复杂结构难以解释，特别是在需要正式论证模型决策的安全关键应用中。现有工作表明可以通过自动推理技术推导逻辑和溯因解释。

Method: 提出基于答案集编程（ASP）的方法来生成多种类型的解释：充分解释、对比解释、多数解释和树特定解释。相比基于SAT的方法，ASP方法在编码用户偏好方面更灵活，并支持枚举所有可能的解释。

Result: 在多样化数据集上进行了实证评估，展示了该方法与现有方法相比的有效性和局限性。

Conclusion: ASP方法为决策树模型解释提供了更灵活和全面的解决方案，能够生成多种类型的解释并枚举所有可能性，在需要正式论证模型决策的场景中具有应用价值。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [17] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: 该论文研究了ASP（答案集编程）在大规模配置问题（如包含3万多个组件的电子系统配置）中的可扩展性，重点关注解决"接地瓶颈"（内存需求急剧增加）的方法，特别是增量求解方法和新提出的约束感知猜测方法。


<details>
  <summary>Details</summary>
Motivation: ASP虽然在许多应用领域实现了"用户指定问题，计算机解决问题"的AI愿景，但当前ASP求解技术能否扩展到大规模配置问题仍存疑问。电子系统配置（可能包含超过30,000个组件）作为此类问题的基准，需要评估ASP技术的潜力和限制。

Method: 1. 研究当前ASP技术，特别关注解决接地瓶颈（内存需求随问题实例规模急剧增加）的方法；2. 探索增量求解方法，该方法在实践中证明有效；3. 基于接地分析，开发约束感知猜测方法，显著减少内存需求。

Result: 1. 增量求解方法在实践中有效，但内存需求仍构成显著限制；2. 新开发的约束感知猜测方法显著减少了内存需求，突破了原有限制。

Conclusion: ASP技术在大规模配置问题中具有潜力，但接地瓶颈是主要限制因素。通过增量求解和约束感知猜测等方法，可以显著改善内存效率，推动ASP技术向更大规模问题扩展。

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [18] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Cheng Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1框架通过过程级推理验证，将可验证奖励与随机金融环境连接，解决标准RL在金融决策中的奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: RL在数学和编码等可验证奖励领域表现优异，但金融决策面临市场随机性挑战：奖励可验证但本质嘈杂，导致标准RL退化为奖励黑客

Method: 提出Trade-R1框架，通过过程级推理验证将可验证奖励桥接到随机环境。关键创新是将冗长金融文档的推理评估转化为结构化RAG任务，构建三角一致性度量来评估检索证据、推理链和决策之间的对齐，作为嘈杂市场回报的有效性过滤器

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客问题，其中动态效应语义奖励（DSR）实现了优越的跨市场泛化，同时保持最高的推理一致性

Conclusion: Trade-R1框架通过过程级推理验证有效解决了金融决策中RL的奖励黑客问题，为随机环境中的可验证奖励应用提供了新范式

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [19] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 论文提出DOT方法解决大模型推理中的"过度思考"问题，通过动态截断冗余token减少78%推理token使用，同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习增强的大推理模型在简单查询上存在过度冗长的问题，导致部署成本高昂。现有基于长度惩罚的方法存在优化冲突，且未深入探究过度思考的生成机制。

Method: 提出动态异常截断(DOT)方法，在训练时选择性抑制冗余token，仅针对完全正确rollout组中的极端长度尾部进行干预。同时结合辅助KL正则化和预测性动态采样确保稳定收敛。

Result: 在多个模型规模上的实验表明，该方法显著扩展了效率-性能的帕累托前沿。在AIME-24上，相比初始策略减少78%推理token使用，同时提高准确率，超越现有高效推理方法。

Conclusion: DOT方法有效解决了大模型推理中的长度偏移现象，在保持复杂问题长时推理能力的同时，显著提高了推理效率，为高效推理提供了新的训练时干预方案。

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [20] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：基于世界模型的移动GUI智能体前瞻框架，通过文本草图世界模型预测动作后状态，提升长视野任务性能


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体多为反应式，主要依赖当前屏幕信息进行决策，在长视野任务中表现受限。构建世界模型能够预测动作结果并支持更好的决策，但需要平衡空间感知预测能力和部署效率。

Method: 提出MobileDreamer框架，包含文本草图世界模型和GUI智能体推演想象。文本草图世界模型通过学习过程将数字图像转换为关键任务相关草图，并设计顺序不变学习策略保持GUI元素空间信息。推演想象策略利用世界模型预测能力优化动作选择过程。

Result: 在Android World上的实验表明，MobileDreamer达到最先进性能，任务成功率提升5.25%。世界模型评估进一步验证了文本草图建模能够准确预测关键GUI元素。

Conclusion: MobileDreamer通过高效的世界模型前瞻框架，显著提升了移动GUI智能体在长视野任务中的性能，为实际部署提供了可行的解决方案。

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [21] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 本文提出了"智能体漂移"概念，即多智能体LLM系统在长期交互中行为、决策质量和协调性逐渐退化的现象，并开发了量化框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在复杂任务分解和协作解决问题方面表现出强大能力，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互中可能出现的退化现象，以确保系统可靠性和安全性。

Method: 提出了智能体漂移的理论框架，定义了三种漂移类型：语义漂移、协调漂移和行为漂移。开发了智能体稳定性指数（ASI），一个包含12个维度的复合度量框架，用于量化漂移现象。通过仿真分析和理论建模验证漂移影响，并提出三种缓解策略：情景记忆巩固、漂移感知路由协议和自适应行为锚定。

Result: 研究表明，未经控制的智能体漂移会导致任务完成准确率显著下降，并增加人工干预需求。理论分析表明，提出的缓解策略可以显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 这项工作建立了监控、测量和缓解生产级智能AI系统中智能体漂移的基础方法论，对企业部署可靠性和AI安全研究具有直接意义。为理解和应对多智能体LLM系统的长期稳定性问题提供了理论框架和实践工具。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>
