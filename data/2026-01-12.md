<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring](https://arxiv.org/abs/2601.05256)
*Eirini Baltzi,Tilemachos Moumouris,Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.AI

TL;DR: NAIAD是一个基于大语言模型的智能助手，通过整合遥感数据和分析工具，为内陆水域监测提供端到端的解决方案，支持自然语言查询并生成定制化报告。


<details>
  <summary>Details</summary>
Motivation: 现有内陆水域监测方法通常针对单个水质指标（如蓝藻、叶绿素等）分别处理，缺乏综合性解决方案。需要一种能够整合多种数据源和分析工具，同时支持专家和非专家用户的统一平台。

Method: NAIAD采用基于大语言模型的智能体架构，结合检索增强生成、外部工具编排、计算图执行和智能体反思等技术。系统整合了天气数据、哨兵2号影像、遥感指数计算、叶绿素a估算以及CyFi等平台工具。

Result: 在专门设计的基准测试中，系统在正确性和相关性指标上分别达到77%和85%以上。初步结果显示系统在不同查询类型上具有良好的适应性和鲁棒性。消融研究表明Gemma 3 (27B)和Qwen 2.5 (14B)在计算效率和推理性能之间取得了最佳平衡。

Conclusion: NAIAD成功开发了一个综合性的内陆水域监测AI助手，能够通过自然语言接口为不同专业水平的用户提供全面的水质分析，展示了智能体AI在环境监测领域的应用潜力。

Abstract: Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.

</details>


### [2] [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)
*Yeongbin Cha,Namjung Kim*

Main category: cs.AI

TL;DR: 提出了一种基于本体引导、方程中心的框架，将大语言模型与增材制造数学知识图谱相结合，实现可靠的知识提取和外推建模。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在增材制造中受到知识表示碎片化和稀疏数据条件下外推不可靠的限制，需要更可靠的知识提取和外推建模方法。

Method: 开发了本体引导的方程中心框架，将LLM与增材制造数学知识图谱（AM-MKG）紧密集成，通过形式化本体编码方程、变量、假设及其语义关系，并引入基于知识图谱子图的条件方程生成。

Result: 本体引导提取显著提高了提取知识的结构一致性和定量可靠性，子图条件方程生成相比无引导LLM输出产生了更稳定且物理一致的外推结果。

Conclusion: 建立了一个统一流程，用于本体驱动的知识表示、方程中心推理和基于置信度的外推评估，展示了知识图谱增强LLM作为增材制造外推建模可靠工具的潜力。

Abstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.

</details>


### [3] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 研究通过重复囚徒困境游戏探索人格引导对LLM智能体合作行为的影响，发现宜人性是促进合作的主要因素，人格引导更多是行为偏差而非确定性控制机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明为人格特质可以影响LLM行为，但人格引导在受控条件下如何影响合作行为仍不清楚。本研究旨在探索人格引导对LLM智能体在战略互动中合作行为的影响。

Method: 使用重复囚徒困境游戏，基于大五人格框架，首先测量GPT-3.5-turbo、GPT-4o和GPT-5的基础人格特征，然后比较基线条件和人格引导条件下的行为，并分析单独操纵每个人格维度到极端值的影响。

Result: 宜人性是所有模型中促进合作的主导因素，其他人格特质影响有限。明确的人格信息能增加合作，但也可能增加被利用的脆弱性，特别是在早期模型中。后期模型表现出更有选择性的合作。

Conclusion: 人格引导更多是作为行为偏差而非确定性控制机制，宜人性是影响LLM智能体合作行为的关键人格维度。

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [4] [Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings](https://arxiv.org/abs/2601.05330)
*Tengwei Song,Long Yin,Zhen Han,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出Hyper-Enz模型，通过知识图谱嵌入和超图transformer结合，利用化学反应方程预测酶-底物相互作用，显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有酶-底物相互作用预测方法依赖专家标注的稀疏数据库，数据不足且维护成本高，限制了模型的泛化能力。化学反应方程数据更易获取且更丰富，但多化合物与酶之间的复杂关系传统模型难以捕捉。

Method: 将化学反应方程表示为(底物，酶，产物)三元组构建知识图谱，提出Hyper-Enz模型：结合超图transformer和知识图谱嵌入学习涉及多个底物和产物的超边表示，引入多专家范式指导酶-底物相互作用学习。

Result: 实验结果显示显著改进：酶检索准确率相对提升达88%，配对级预测提升30%，证明方法的有效性。

Conclusion: 通过利用化学反应方程数据和知识图谱嵌入，结合超图transformer，能够有效预测酶-底物相互作用，解决了传统方法数据稀疏和复杂关系建模的挑战。

Abstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.

</details>


### [5] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 研究发现医疗角色设定对大型语言模型在临床决策中的影响具有系统性、情境依赖性和非单调性：在重症监护任务中提升性能，但在初级护理中反而降低性能，且交互风格的影响高度模型依赖。


<details>
  <summary>Details</summary>
Motivation: 角色设定通常被视为大型语言模型的行为先验，常被认为能单调地提升专业性和安全性，但其对高风险临床决策的具体影响尚未得到充分研究。

Method: 系统评估基于角色的控制对临床LLMs的影响，考察专业角色（急诊科医生、护士等）和交互风格（大胆vs谨慎）在不同模型和医疗任务中的行为影响，使用多维评估方法衡量任务准确性、校准度和安全相关风险行为。

Result: 医疗角色在重症监护任务中提升性能（准确性和校准度提升约20%），但在初级护理环境中性能下降类似幅度；交互风格调节风险倾向和敏感性，但高度模型依赖；人类临床医生对安全合规性有中等一致性，但对推理质量信心较低。

Conclusion: 角色设定作为行为先验引入情境依赖的权衡，而非安全或专业性的保证，表明需要更细致地理解角色设定在临床决策中的复杂影响。

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [6] [On the Effect of Cheating in Chess](https://arxiv.org/abs/2601.05386)
*Daniel Keren*

Main category: cs.AI

TL;DR: 研究评估在国际象棋比赛中有限次作弊（使用软件建议）对棋手表现的提升效果


<details>
  <summary>Details</summary>
Motivation: 国际象棋中使用强大软件作弊已成为严重问题，甚至影响到最高水平比赛。与以往主要关注作弊检测的研究不同，本研究旨在量化有限次作弊对棋手表现的提升效果

Method: 开发算法并在常用象棋引擎上进行测试，评估在比赛中有限次数使用软件建议对棋手表现的影响

Result: 论文开发了评估作弊效果的算法，并在象棋引擎上进行了测试，但摘要中未提供具体实验结果数据

Conclusion: 研究目标不是协助作弊者，而是为了衡量作弊的有效性，这对于遏制和检测作弊至关重要

Abstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}

</details>


### [7] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: ART（自适应推理树）是一种用于声明验证的分层方法，通过构建支持与反对论点的树状结构，由LLM作为裁判进行成对比较，实现透明且可争议的决策过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂决策中表现出色，但其不透明性阻碍了在高风险环境中的应用。现有方法缺乏忠实解释和错误纠正机制，损害了可信度。

Method: ART采用分层树状结构：从根声明开始，分支为支持和反对的子论点。通过LLM裁判对子论点进行成对锦标赛式比较，自底向上确定论点强度，最终得出透明且可争议的裁决。

Result: 在多个数据集上的实证验证表明，ART的结构化推理优于强基线方法，为可解释的声明验证设立了新基准，提高了可靠性并确保决策步骤的清晰性。

Conclusion: ART通过结构化、透明且可争议的推理过程，解决了LLM在声明验证中的不透明性问题，为高风险环境中的可信决策提供了有效方法。

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [8] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: MMUEChange是一个多模态智能体框架，通过模块化工具包和模态控制器整合异构城市数据，实现对复杂城市环境变化的稳健分析，相比最佳基线任务成功率提升46.7%


<details>
  <summary>Details</summary>
Motivation: 当前城市环境变化分析方法（特别是遥感变化检测）通常依赖僵化的单模态分析，存在局限性，需要更灵活的方法来整合异构城市数据以支持可持续城市发展

Method: 提出MMUEChange多模态智能体框架，包含模块化工具包和核心模块"模态控制器"，用于跨模态和模态内对齐，能够灵活整合异构城市数据

Result: 相比最佳基线，MMUEChange智能体在任务成功率上提升了46.7%，有效缓解了幻觉问题；案例研究包括纽约小型社区公园转变、香港跨区域水污染扩散、深圳露天垃圾场减少等复杂城市变化场景分析

Conclusion: MMUEChange框架能够支持具有现实政策意义的复杂城市变化分析任务，展示了多模态方法在城市环境变化理解中的潜力和价值

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [9] [The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm](https://arxiv.org/abs/2601.05500)
*Aparna Elangovan,Lei Xu,Mahsa Elyasi,Ismail Akdulum,Mehmet Aksakal,Enes Gurun,Brian Hur,Saab Mansour,Ravid Shwartz Ziv,Karin Verspoor,Dan Roth*

Main category: cs.AI

TL;DR: 提出概率评估范式，解决医学AI基准测试中忽略专家答案不确定性的问题，引入期望准确率和期望F1分数来估计系统真实能力


<details>
  <summary>Details</summary>
Motivation: 当前AI系统（特别是LLM和视觉模型）的基准测试通常忽略专家答案中的不确定性，这在医学领域尤为严重，因为医学中存在普遍的不确定性。忽略这种不确定性可能导致误导性结论，例如认为非专家与专家表现相似

Method: 引入概率评估范式，提出期望准确率和期望F1分数的概念，用于估计在给定专家答案变异性的情况下，专家或系统能达到的分数。建议根据专家答案的概率（通常通过专家一致率衡量）对结果进行分层评估

Result: 研究表明，当专家答案确定性高时，专家才能获得高分；而在专家答案变异大的数据集中，随机标注者与专家之间可能几乎没有差异。分层评估使性能比较在高确定性区间更加可靠

Conclusion: 在评估系统能力时，应根据专家答案的概率进行分层评估，特别是在总体性能低于80%阈值时。分层评估能减轻关键混杂因素——不确定性的影响，使性能比较更加可靠

Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.
  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.

</details>


### [10] [Explainable AI: Learning from the Learners](https://arxiv.org/abs/2601.05525)
*Ricardo Vinuesa,Steven L. Brunton,Gianmarco Mengaldo*

Main category: cs.AI

TL;DR: 该论文提出将可解释人工智能与因果推理结合，通过"向学习者学习"的方法，从基础模型中提取因果机制，指导稳健设计与控制，支持高风险应用中的信任与问责。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在科学和工程任务中已超越人类表现，但其内部表征往往不透明。需要开发方法使AI系统更可解释，以便人类能够理解和信任AI的决策过程。

Method: 结合可解释人工智能与因果推理，通过基础模型和可解释性方法的结合，从AI模型中提取因果机制。重点关注发现、优化和认证三个应用领域。

Result: 展示了这种结合方法能够提取因果机制、指导稳健设计与控制，并支持高风险应用中的信任与问责。提出了XAI作为科学与工程中人类-AI协作的统一框架。

Conclusion: 可解释人工智能与因果推理的结合为"向学习者学习"提供了有效途径，能够解决AI系统的不透明性问题，促进人类与AI在科学和工程领域的协作，但仍需解决解释的忠实性、泛化性和可用性等挑战。

Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.

</details>


### [11] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 该论文评估大语言模型在安全关键场景中的表现，发现即使99%准确率仍存在灾难性风险，当前LLM不适合直接部署到机器人安全系统中。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入机器人决策系统，单个错误指令可能直接危及人类生命安全，需要系统评估LLM在安全关键场景中的性能，特别是微小错误可能造成灾难性后果的情况。

Method: 通过火灾疏散场景的定性评估识别关键失败案例，设计七项定量评估任务：完整信息任务（使用ASCII地图减少歧义）、不完整信息任务（测试空间连续性与幻觉）、安全导向空间推理任务（评估生命威胁情境下的安全决策），并对多种LLM和VLM进行基准测试。

Result: 结果显示严重漏洞：多个模型在ASCII导航中成功率0%；在模拟消防演习中，模型指示机器人向危险区域而非紧急出口移动；99%准确率在机器人领域具有误导性，意味着每百次执行可能造成灾难性伤害。

Conclusion: 当前大语言模型尚未准备好直接部署到安全关键系统中，即使最先进的模型也无法保证安全，绝对依赖它们会带来不可接受的风险。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [12] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: WildSci是一个从同行评审文献自动合成的领域特定科学问题数据集，涵盖9个科学学科和26个子领域，通过多项选择题格式支持可扩展训练，并使用强化学习微调模型，在科学推理基准测试中表现有效。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理在数学和编码领域进展迅速，但在医学、材料科学等科学领域进展有限，主要原因是数据集覆盖不足和开放科学问题的复杂性。需要解决科学推理领域的数据稀缺和评估困难问题。

Method: 1. 从同行评审文献自动合成领域特定科学问题，构建WildSci数据集，涵盖9个学科26个子领域；2. 将复杂科学推理任务框架化为多项选择题格式，提供明确奖励信号；3. 应用强化学习微调模型，分析训练动态、领域性能变化、响应行为和泛化趋势。

Result: 在一系列科学基准测试中验证了数据集和方法的有效性。WildSci数据集已公开发布，支持科学推理的可扩展和可持续研究。

Conclusion: WildSci通过自动合成的科学问题数据集和强化学习微调方法，有效解决了科学推理领域的数据稀缺和评估挑战，为科学领域的LLM推理研究提供了可扩展的基础设施。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [13] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 本文提出了一种使用强化学习对轻量级语言模型进行后训练的方法，专门用于电子商务欺诈检测任务，仅使用原始交易数据，在真实数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台面临日益复杂的欺诈方案，但大型语言模型在真实金融欺诈检测中的应用尚未充分探索，其处理特定领域电子商务交易数据的实际效果也缺乏实证验证。

Method: 采用强化学习方法，使用Group Sequence Policy Optimization (GSPO)算法结合基于规则的奖励系统，对多种规模的轻量级语言模型进行后训练，专门用于欺诈检测任务，仅使用原始交易数据。

Result: 实验结果表明，经过后训练的语言模型在保留测试数据上取得了显著的F1分数提升。性能改进主要归因于强化学习固有的探索机制，使模型能够发现超越传统工程特征的新型欺诈指标。

Conclusion: 该方法成功弥合了传统机器学习局限性与LLMs在欺诈检测中未开发潜力之间的差距，证明了强化学习后训练语言模型在电子商务欺诈检测中的有效性。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [14] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: 本文提出CPSR框架，通过累积路径级语义推理解决归纳知识图谱补全中的噪声结构信息和长距离依赖问题，实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统KGC方法在处理新兴实体时效果有限，现有归纳KGC方法虽然能处理新兴实体和关系，但仍面临噪声结构信息干扰和难以捕获推理路径中长距离依赖的挑战。

Method: 提出CPSR框架：1）查询相关掩码模块自适应屏蔽噪声结构信息，保留与目标密切相关的信息；2）全局语义评分模块评估推理路径中节点的个体贡献和集体影响。

Result: 实验结果表明CPSR实现了最先进的性能。

Conclusion: CPSR框架通过同时捕获知识图谱的结构和语义信息，有效解决了归纳知识图谱补全中的关键挑战，提升了任务性能。

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [15] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架来评估生成模型的可控性，通过对话设置中的可控集估计算法，提供了样本复杂度相关的误差保证，并发现模型可控性实际上很脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型变得无处不在，需要细粒度控制生成过程。然而，尽管从提示到微调的各种控制方法不断涌现，一个基本问题仍未解决：这些模型是否真正可控？

Method: 将人机交互建模为控制过程，提出一种新颖算法来估计对话设置中模型的可控集。该算法提供形式化保证：基于样本复杂度的概率近似正确边界，分布无关，仅假设输出有界，适用于任何黑盒非线性控制系统（即任何生成模型）。

Result: 在对话过程控制和语言模型、文本到图像生成等任务上的实证研究表明，模型可控性出人意料地脆弱，且高度依赖实验设置。

Conclusion: 需要严格的可控性分析，将重点从简单尝试控制转向首先理解其基本限制。模型可控性存在根本性限制，需要更严谨的理论框架来评估。

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [16] [Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models](https://arxiv.org/abs/2601.05693)
*Zenghao Duan,Liang Pang,Zihao Wei,Wenbin Duan,Yuxin Tian,Shicheng Xu,Jingcheng Deng,Zhiyi Yin,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出"循环推理"这一新失败模式，开发LoopBench数据集进行系统分析，发现循环由推理僵局触发并由自增强V型注意力机制维持，使用CUSUM算法实现早期预测。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在测试时缩放时经常遇到重复循环，导致计算浪费和推理失败。传统模型退化理论无法解释这种自我强化的循环现象，需要系统研究这种新的失败模式。

Method: 1. 提出"循环推理"概念并开发LoopBench数据集，包含数值循环和陈述循环两种类型；2. 从机制上分析循环推理作为状态崩溃现象；3. 使用累积和(CUSUM)算法捕捉循环前兆进行早期预测。

Result: 实验验证了CUSUM算法在多种大型推理模型上的预测准确性，揭示了长链推理的稳定性特征，成功捕捉到语义重复先于文本重复的循环前兆。

Conclusion: 循环推理是大型推理模型特有的失败模式，由推理僵局触发并由自增强注意力机制维持。CUSUM算法能有效预测循环发生，为提高推理模型稳定性提供了新方法。

Abstract: Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.

</details>


### [17] [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)
*Ali Farjami,Luca Redondi,Marco Valentino*

Main category: cs.AI

TL;DR: 本文提出了一种逻辑参数化的神经符号推理框架，将逻辑形式作为可控组件而非固定背景，通过LogiKEy方法将多种逻辑嵌入高阶逻辑，系统比较了不同逻辑在推理质量、解释精炼和证明行为上的表现，特别关注规范推理领域。


<details>
  <summary>Details</summary>
Motivation: 现有结合大语言模型和定理证明器的可验证自然语言推理方法依赖于固定的逻辑形式，这限制了系统的鲁棒性和适应性。需要一种能够灵活选择和调整逻辑形式的框架来提升神经符号推理的性能。

Method: 采用逻辑参数化框架，将底层逻辑视为可控组件而非静态背景。使用LogiKEy方法将经典和非经典逻辑形式嵌入高阶逻辑（HOL），系统比较逻辑外部方法（通过公理编码规范要求）和逻辑内部方法（规范模式从逻辑内置结构中产生）。

Result: 实验表明逻辑内部策略能持续提升性能并产生更高效的混合证明。逻辑的有效性具有领域依赖性：一阶逻辑在常识推理中表现更好，而道义逻辑和模态逻辑在伦理领域表现更优。

Conclusion: 将逻辑作为神经符号架构中的一等参数化元素，能够实现更鲁棒、模块化和适应性强的推理系统，逻辑选择应根据具体应用领域进行优化。

Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.

</details>


### [18] [Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding](https://arxiv.org/abs/2601.05724)
*Yuxuan Zhou,Fei Huang,Heng Li,Fengyi Wu,Tianyu Wang,Jianwei Zhang,Junyang Lin,Zhi-Qi Cheng*

Main category: cs.AI

TL;DR: HSD提出了一种分层推测解码方法，通过平衡可访问分支上的概率质量来解决联合不可处理性问题，显著提升了接受令牌数量，在保持分布保真度的同时提高了解码效率。


<details>
  <summary>Details</summary>
Motivation: 验证是推测解码中提高推理速度同时保持分布保真度的关键瓶颈。现有方法依赖近似或受限于部分信息，难以处理联合不可处理性问题。

Method: 提出了分层推测解码（HSD），一种可证明无损的验证方法，通过平衡可访问分支上的超额和不足概率质量来克服联合不可处理性。

Result: 大规模实验显示HSD在不同模型家族和基准测试中持续提升接受率。将HSD集成到EAGLE-3中获得了超过12%的性能提升，建立了最先进的解码效率。

Conclusion: HSD提供了一种可解释且通用的验证方法，可轻松集成到各种推测解码框架中，在不损害分布保真度的前提下显著提升解码效率。

Abstract: Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.

</details>


### [19] [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](https://arxiv.org/abs/2601.05739)
*G M Shahariar,Zabir Al Nazi,Md Olid Hasan Bhuiyan,Zhouxing Shi*

Main category: cs.AI

TL;DR: PII-VisBench：评估视觉语言模型在个人在线可见度连续体上的隐私泄露风险，发现模型对高可见度主体的PII泄露率更高


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型隐私评估主要关注静态信息提取，忽略了主体在线存在（数据可用量）对隐私对齐的影响，需要建立考虑在线可见度连续体的评估基准

Method: 构建PII-VisBench基准，包含4000个独特探针，将200个主体按在线信息可获取程度分为高、中、低、零四个可见度类别，评估18个开源VLMs的拒绝率和条件PII泄露率

Result: 模型呈现一致模式：随着主体可见度降低，拒绝率增加，PII泄露率从高可见度的9.10%降至低可见度的5.34%；模型更倾向于泄露高可见度主体的PII，存在显著的模型家族异质性和PII类型差异

Conclusion: 需要基于可见度的安全评估和训练干预，以应对重述和越狱式提示等攻击，提高视觉语言模型在不同在线可见度场景下的隐私保护能力

Abstract: Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.

</details>


### [20] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: DynaDebate提出了一种动态多智能体辩论框架，通过路径生成、过程中心辩论和触发式验证机制解决传统多智能体辩论中存在的同质化推理和简单多数投票问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体辩论框架存在两个主要问题：1）无引导的初始化导致智能体采用相同的推理路径，犯相同错误；2）辩论效果受限，最终结果往往退化为简单的多数投票。

Method: DynaDebate包含三个核心机制：1）动态路径生成与分配：使用专门的路径生成智能体生成多样化、逻辑性的解决方案路径；2）过程中心辩论：从结果投票转向逐步逻辑批判，确保过程正确性；3）触发式验证智能体：在分歧时激活，使用外部工具客观解决僵局。

Result: 大量实验表明，DynaDebate在各种基准测试中取得了优越性能，超越了现有的最先进多智能体辩论方法。

Conclusion: DynaDebate通过引入动态路径生成、过程中心辩论和触发式验证机制，有效解决了多智能体辩论中的同质化推理和简单多数投票问题，显著提升了多智能体系统的协作决策和复杂问题解决能力。

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [21] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的分层多智能体框架，通过解耦高层协调与子任务执行，并利用结构化经验记忆和强化学习重用协调经验，解决了集中式架构中内存管理不足导致的上下文膨胀、错误累积和跨任务泛化差的问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的集中式多智能体系统在复杂知识密集型任务中表现出潜力，但中央智能体由于缺乏内存管理，导致长时程协作不稳定，出现上下文膨胀、错误累积和跨任务泛化差的问题。需要解决任务级内存效率低下和无法重用协调经验的问题。

Method: 提出StackPlanner分层多智能体框架，通过主动任务级内存控制解耦高层协调与子任务执行，同时利用结构化经验记忆和强化学习来检索和利用可重用的协调经验。

Result: 在多个深度搜索和智能体系统基准测试上的实验表明，该方法在实现可靠的长时程多智能体协作方面具有有效性。

Conclusion: StackPlanner通过显式内存控制和经验重用机制，成功解决了集中式多智能体系统中的内存管理问题，实现了更稳定和高效的长时程协作。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [22] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind是一个基于塔防游戏的轻量级多模态环境，用于评估大语言模型在实时策略游戏中的长期规划和决策能力，同时支持幻觉检测和高定制性。


<details>
  <summary>Details</summary>
Motivation: 现有实时策略游戏环境要么计算需求高，要么缺乏文本观察支持，限制了其在大语言模型评估中的应用。需要一种既能保留RTS游戏评估优势，又具备低计算需求和多模态观察空间的环境。

Method: 基于塔防游戏子类型开发TowerMind环境，包含像素、文本和结构化游戏状态三种观察表示，支持模型幻觉评估和高定制性。设计了五个基准关卡，在不同多模态输入设置下评估多个大语言模型。

Result: 实验显示大语言模型与人类专家在能力和幻觉维度存在明显性能差距，揭示了LLM的关键局限性：规划验证不足、决策缺乏多终局性、行动使用效率低。同时评估了Ape-X DQN和PPO两种经典强化学习算法。

Conclusion: TowerMind通过轻量级多模态设计，补充了现有RTS游戏环境体系，为AI智能体领域引入了新的基准测试平台，代码已在GitHub开源。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [23] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: 该研究首次定义了开放词汇3D指令歧义检测任务，构建了Ambi3D大规模基准数据集，并提出AmbiVer两阶段框架来解决现有3D大语言模型在判断指令歧义性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，语言歧义可能导致严重后果，但现有具身AI研究大多忽视这一问题，假设指令清晰并专注于执行而非确认。为填补这一安全空白，需要系统性地检测3D场景中的指令歧义。

Method: 提出了AmbiVer两阶段框架：第一阶段从多个视角收集明确的视觉证据，第二阶段使用这些证据指导视觉语言模型判断指令歧义性。同时构建了Ambi3D基准数据集，包含700多个多样化的3D场景和约22k条指令。

Result: 分析发现最先进的3D大语言模型在可靠判断指令是否歧义方面存在显著局限性。AmbiVer框架通过实验证明能有效解决这一挑战，为更安全可靠的具身AI铺平道路。

Conclusion: 该研究首次定义了开放词汇3D指令歧义检测任务，通过构建大规模基准数据集和提出有效框架，解决了具身AI中的关键安全漏洞，推动了更安全可信的具身AI系统发展。

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>
