<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning](https://arxiv.org/abs/2601.06098)
*Nicholas X. Wang,Neel V. Parpia,Aaryan D. Parikh,Aggelos K. Katsaggelos*

Main category: cs.AI

TL;DR: 提出结合因果图引导的思维链推理与多智能体LLM架构的新框架，用于生成准确、有意义且与课程对齐的问题，显著减少幻觉问题


<details>
  <summary>Details</summary>
Motivation: 直觉学习对STEM教育中发展深度概念理解至关重要，但自动问题生成受到LLM幻觉问题的限制，可能生成事实错误、模糊或教学不一致的问题

Method: 结合因果图引导的思维链推理与多智能体LLM架构，因果图提供领域知识的显式表示，思维链推理实现结构化逐步遍历相关概念，专用LLM智能体负责图路径查找、推理、验证和输出等任务

Result: 实验结果显示质量相比参考方法提升高达70%，主观评估获得高度积极结果，双重验证机制显著减少幻觉

Conclusion: 提出的框架能有效生成准确、有意义且与课程对齐的问题，为个性化自适应学习提供可靠支持

Abstract: Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.

</details>


### [2] [Dynamic Intelligence Ceilings: Measuring Long-Horizon Limits of Planning and Creativity in Artificial Systems](https://arxiv.org/abs/2601.06102)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出"动态智能上限"概念，通过轨迹中心评估框架衡量智能系统随时间演化的能力边界，而非静态性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在长期发展行为中存在局限，容易收敛到重复解决方案模式而非持续增长。主要问题不是能力本身，而是性能边界的过早固化。

Method: 提出动态智能上限概念，建立轨迹中心评估框架，使用渐进难度上限和上限漂移率两个估计器，通过程序生成基准测试环境评估长期规划和结构创造力。

Result: 研究揭示了在固定解决方案流形内深化利用的系统与随时间持续扩展边界的系统之间的质性区别，表明智能限制是动态且轨迹依赖的，而非静态固定的。

Conclusion: 智能系统的限制应被重新定义为动态且轨迹依赖的，而非静态和过早固定的。提出的框架为评估AI系统的长期发展行为提供了新视角。

Abstract: Recent advances in artificial intelligence have produced systems capable of remarkable performance across a wide range of tasks. These gains, however, are increasingly accompanied by concerns regarding long-horizon developmental behavior, as many systems converge toward repetitive solution patterns rather than sustained growth.
  We argue that a central limitation of contemporary AI systems lies not in capability per se, but in the premature fixation of their performance frontier. To address this issue, we introduce the concept of a \emph{Dynamic Intelligence Ceiling} (DIC), defined as the highest level of effective intelligence attainable by a system at a given time under its current resources, internal intent, and structural configuration.
  To make this notion empirically tractable, we propose a trajectory-centric evaluation framework that measures intelligence as a moving frontier rather than a static snapshot. We operationalize DIC using two estimators: the \emph{Progressive Difficulty Ceiling} (PDC), which captures the maximal reliably solvable difficulty under constrained resources, and the \emph{Ceiling Drift Rate} (CDR), which quantifies the temporal evolution of this frontier. These estimators are instantiated through a procedurally generated benchmark that jointly evaluates long-horizon planning and structural creativity within a single controlled environment.
  Our results reveal a qualitative distinction between systems that deepen exploitation within a fixed solution manifold and those that sustain frontier expansion over time. Importantly, our framework does not posit unbounded intelligence, but reframes limits as dynamic and trajectory-dependent rather than static and prematurely fixed.
  \vspace{0.5em} \noindent\textbf{Keywords:} AI evaluation, planning and creativity, developmental intelligence, dynamic intelligence ceilings, complex adaptive systems

</details>


### [3] [Comment on arXiv:2511.21731v1: Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition](https://arxiv.org/abs/2601.06104)
*Krzysztof Sienicki*

Main category: cs.AI

TL;DR: 对arXiv:2511.21731v1论文的技术性检查，指出其在CHSH/Bell型计算和玻色-爱因斯坦拟合方面的解释超出了方法本身能支持的范围，并发现"能级间距"类比存在内部不一致性


<details>
  <summary>Details</summary>
Motivation: 对一篇声称在语言数据中发现量子纠缠特征的论文进行建设性的技术检查，旨在澄清其观察结果对希尔伯特空间中量子纠缠的真正含义，特别是当"能量"由词频排名定义时

Method: 采用技术检查方法，分析论文中的CHSH/Bell型计算、玻色-爱因斯坦拟合到词频排名数据的方法，以及"能级间距"类比的逻辑一致性

Result: 发现论文的解释超出了其方法能支持的范围，CHSH/Bell计算和BE拟合的解读存在过度延伸，"能级间距"类比存在内部不一致性

Conclusion: 虽然论文中的经验观察值得关注，但需要明确这些观察结果对量子纠缠（在通常的希尔伯特空间意义上）的实际含义，特别是在"能量"由排名定义的情况下

Abstract: This note is a friendly technical check of arXiv:2511.21731v1. I highlight a few places where the manuscript's interpretation of (i) the reported CHSH/Bell-type calculations and (ii) Bose--Einstein (BE) fits to rank-frequency data seems to go beyond what the stated procedures can firmly support. I also point out one internal inconsistency in the "energy-level spacing" analogy. The aim is constructive: to keep the interesting empirical observations, while making clear what they do (and do not) imply about quantum entanglement in the usual Hilbert-space sense, especially when "energy" is defined by rank.

</details>


### [4] [From RLHF to Direct Alignment: A Theoretical Unification of Preference Learning for Large Language Models](https://arxiv.org/abs/2601.06108)
*Tarun Raheja,Nilay Pochhi*

Main category: cs.AI

TL;DR: 该论文提出一个理论框架，将各种偏好学习方法统一为三个正交轴的选择：偏好模型、正则化机制和数据分布，揭示了看似多样的方法背后的共同原理。


<details>
  <summary>Details</summary>
Motivation: 随着RLHF成为主流，出现了大量替代方法（DPO、IPO、KTO、SimPO等），但缺乏清晰的方法选择指导。需要理论框架来统一理解这些方法，为实践者提供选择依据。

Method: 提出理论统一框架，将偏好学习方法分解为三个正交轴：1) 偏好模型（目标函数的基础似然模型）；2) 正则化机制（控制与参考策略的偏差）；3) 数据分布（在线vs离线学习及覆盖要求）。通过形式化定义和定理分析每个轴。

Result: 建立了关键理论结果：在线与离线方法的覆盖分离、奖励过度优化的缩放定律、直接对齐方法失败的条件。揭示了失败模式（长度攻击、模式崩溃、似然位移）源于特定的设计选择组合。综合了50多篇论文的实证发现。

Conclusion: 该框架将偏好学习从经验艺术转变为理论基础的学科，提供了实践者的决策指南，揭示了方法多样性的本质是三个正交轴上的原则性选择。

Abstract: Aligning large language models (LLMs) with human preferences has become essential for safe and beneficial AI deployment. While Reinforcement Learning from Human Feedback (RLHF) established the dominant paradigm, a proliferation of alternatives -- Direct Preference Optimization (DPO), Identity Preference Optimization (IPO), Kahneman-Tversky Optimization (KTO), Simple Preference Optimization (SimPO), and many others -- has left practitioners without clear guidance on method selection. This survey provides a \textit{theoretical unification} of preference learning methods, revealing that the apparent diversity reduces to principled choices along three orthogonal axes: \textbf{(I) Preference Model} (what likelihood model underlies the objective), \textbf{(II) Regularization Mechanism} (how deviation from reference policies is controlled), and \textbf{(III) Data Distribution} (online vs.\ offline learning and coverage requirements). We formalize each axis with precise definitions and theorems, establishing key results including the coverage separation between online and offline methods, scaling laws for reward overoptimization, and conditions under which direct alignment methods fail. Our analysis reveals that failure modes -- length hacking, mode collapse, likelihood displacement -- arise from specific, predictable combinations of design choices. We synthesize empirical findings across 50+ papers and provide a practitioner's decision guide for method selection. The framework transforms preference learning from an empirical art into a theoretically grounded discipline.

</details>


### [5] [CBMAS: Cognitive Behavioral Modeling via Activation Steering](https://arxiv.org/abs/2601.06109)
*Ahmed H. Ismail,Anthony Kuang,Ayo Akinkugbe,Kevin Zhu,Sean O'Brien*

Main category: cs.AI

TL;DR: CBMAS是一个用于连续激活导向的诊断框架，将认知偏差分析从离散干预扩展到可解释的轨迹，通过密集α扫描、logit lens偏差曲线和层位点敏感性分析揭示模型行为的临界点和跨层演化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同提示、层和上下文中编码的认知行为难以预测，使得诊断和控制变得困难，需要一种能够分析连续激活导向的框架来提升模型的可解释性。

Method: 结合导向向量构建与密集α扫描、基于logit lens的偏差曲线和层位点敏感性分析，通过连续诊断揭示小干预强度下模型行为翻转的临界点，并展示导向效应在层深度上的演化。

Result: 该框架能够揭示模型行为的临界点，展示导向效应在不同层深度上的演化，为高层行为评估和低层表征动态之间搭建桥梁，提升LLMs的认知可解释性。

Conclusion: 连续诊断为LLMs的认知可解释性提供了重要工具，作者提供了CLI和多种认知行为的数据集，促进了该领域的研究和应用。

Abstract: Large language models (LLMs) often encode cognitive behaviors unpredictably across prompts, layers, and contexts, making them difficult to diagnose and control. We present CBMAS, a diagnostic framework for continuous activation steering, which extends cognitive bias analysis from discrete before/after interventions to interpretable trajectories. By combining steering vector construction with dense α-sweeps, logit lens-based bias curves, and layer-site sensitivity analysis, our approach can reveal tipping points where small intervention strengths flip model behavior and show how steering effects evolve across layer depth. We argue that these continuous diagnostics offer a bridge between high-level behavioral evaluation and low-level representational dynamics, contributing to the cognitive interpretability of LLMs. Lastly, we provide a CLI and datasets for various cognitive behaviors at the project repository, https://github.com/shimamooo/CBMAS.

</details>


### [6] [ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions](https://arxiv.org/abs/2601.06112)
*Aayush Gupta*

Main category: cs.AI

TL;DR: ReliabilityBench是一个评估LLM智能体可靠性的新基准，关注一致性、鲁棒性和容错性三个维度，通过统一可靠性表面和混沌工程式故障注入框架进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用LLM智能体基准主要报告单次运行成功率，缺乏生产环境所需的可靠性属性评估，需要系统化的可靠性评估框架。

Method: 提出ReliabilityBench基准，包含三个可靠性维度：重复执行的一致性（使用pass^k）、语义等效任务扰动的鲁棒性（强度ε）、受控工具/API故障的容错性（强度λ）。采用统一可靠性表面R(k,ε,λ)、动作变形关系（通过最终状态等价而非文本相似性定义正确性）和混沌工程式故障注入框架（超时、速率限制、部分响应、模式漂移）。

Result: 评估了两个模型（Gemini 2.0 Flash、GPT-4o）和两种智能体架构（ReAct、Reflexion）在四个领域（调度、旅行、客户支持、电子商务）的1,280个任务。仅扰动就将成功率从ε=0时的96.9%降低到ε=0.2时的88.1%。速率限制是消融研究中最具破坏性的故障。在组合压力下，ReAct比Reflexion更鲁棒，Gemini 2.0 Flash以低得多的成本实现了与GPT-4o相当的可靠性。

Conclusion: ReliabilityBench为评估LLM智能体的生产就绪性提供了系统化框架，能够全面评估智能体在真实生产环境中的可靠性表现。

Abstract: Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.

</details>


### [7] [Towards Infinite Length Extrapolation: A Unified Approach](https://arxiv.org/abs/2601.06113)
*Nitin Vetcha*

Main category: cs.AI

TL;DR: 该论文提出了一种新的自适应位置编码方法（APE），通过频率调制和精心设计的衰减偏置来解决大语言模型处理长序列时的上下文窗口限制问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长序列时受限于训练时的上下文窗口大小，现有的长度外推方法存在性能下降或计算效率低的问题，需要一种更好的方法来处理长距离依赖关系。

Method: 提出了一个统一框架，将位置编码方法重新解释为注意力分数的乘性变换和加性偏置分解。基于此框架，引入了自适应位置编码（APE），利用自适应频率调制和包含线性、对数和平方根项的复杂衰减偏置设计。

Result: 理论分析建立了无限上下文外推的条件，确保softmax归一化在无界序列上保持良好定义，同时保留长距离相关性、熵有界性和梯度位置敏感性。在TinyStories数据集和新的Long Tiny Stories数据集（包含长达32,000词的故事）上进行了实验验证。

Conclusion: APE方法能够有效处理长序列，解决了现有位置编码方法在处理长距离依赖时的局限性，为无限上下文外推提供了理论保证和实际可行性。

Abstract: Large language models (LLMs) have revolutionized natural language processing, but their ability to process long sequences is fundamentally limited by the context window size during training. Existing length extrapolation methods often suffer from performance degradation or computational inefficiencies. We thereby use a unified framework that reinterprets positional encoding methods as a decomposition of the attention score into a multiplicative transformation and an additive bias. This perspective not only subsumes popular approaches such as relative position embeddings and attention-bias moderated approaches but also exposes their inherent limitations in handling long-range dependencies. To address these shortcomings, motivated by our framework, we introduce Adaptive Positional Encoding (APE), which leverages adaptive frequency modulation and an intricately designed decay bias that incorporates linear, logarithmic, and square-root terms. Our theoretical analysis establishes conditions for infinite-context extrapolation, ensuring that the softmax normalization remains well-defined over unbounded sequences while preserving long-distance correlations, entropy boundedness and gradient positional sensitivity. We substantiate our claims with an experimental case study on TinyStories dataset as well as a new synthetic dataset, \emph{Long Tiny Stories} featuring stories up to 32,000 words. Relevant code, dataset and model weights are available at https://anonymous.4open.science/r/Check-2DAD/.

</details>


### [8] [Structure-Aware Diversity Pursuit as an AI Safety Strategy against Homogenization](https://arxiv.org/abs/2601.06116)
*Ian Rios-Sialer*

Main category: cs.AI

TL;DR: 论文提出生成式AI存在同质化问题，即模型会复制并放大训练数据中的偏见，导致多样性丧失。作者主张将同质化作为AI安全的核心关注点，并引入异质再生产作为缓解策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型会复制训练数据中的偏见，并通过模式崩溃进一步放大这些偏见，导致有害的多样性丧失（同质化）。作者认为同质化应该成为AI安全的主要关注点。

Method: 引入异质再生产作为缓解同质化的策略。对于自回归大语言模型，将异质再生产形式化为结构感知的多样性追求。

Result: 提出了一个基础性框架，旨在开启重要的研究方向并邀请合作推进多样性研究。

Conclusion: 同质化是AI安全的关键问题，需要系统性的缓解策略。异质再生产为解决这一问题提供了理论基础，需要进一步研究来推进AI系统的多样性。

Abstract: Generative AI models reproduce the biases in the training data and can further amplify them through mode collapse. We refer to the resulting harmful loss of diversity as homogenization. Our position is that homogenization should be a primary concern in AI safety. We introduce xeno-reproduction as the strategy that mitigates homogenization. For auto-regressive LLMs, we formalize xeno-reproduction as a structure-aware diversity pursuit. Our contribution is foundational, intended to open an essential line of research and invite collaboration to advance diversity.

</details>


### [9] [Beyond Reproducibility: Token Probabilities Expose Large Language Model Nondeterminism](https://arxiv.org/abs/2601.06118)
*Tairan Fu,Gonzalo Martínez,Javier Conde,Carlos Arriaga,Pedro Reviriego,Xiuyuan Qi,Shanshan Liu*

Main category: cs.AI

TL;DR: LLM在GPU上执行时即使配置为确定性也会产生非确定性结果，本文通过分析token概率变化而非生成文本来深入研究非确定性现象。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注非确定性对LLM生成文本的影响或提出实现确定性执行的机制，但缺乏对token概率层面变化的深入分析。本文旨在填补这一空白，探究非确定性在概率层面的具体表现。

Method: 通过评估多个LLM模型，分析GPU执行中的非确定性对token概率变化的影响，特别关注概率值在0.1-0.9范围内与接近0或1时的变化差异。

Result: 所有评估模型在概率变化趋势和实际值上表现相似：非确定性对概率在0.1-0.9范围内的token影响显著，而对接近0或1的概率影响很小。

Conclusion: 非确定性在温度不为零时对生成文本有不可忽视的影响；不同模型在token概率层面有相似的非确定性变化；可通过单次推理分析token概率来估计非确定性影响，无需多次重复运行。

Abstract: The execution of Large Language Models (LLMs) has been shown to produce nondeterministic results when run on Graphics Processing Units (GPUs), even when they are configured to produce deterministic results. This is due to the finite precision effects of the arithmetic operations, which depend on the order in which they are executed. This order, in turn, depends on the processes that are running concurrently on the GPU. Previous studies have focused on the impact of nondeterminism on the text generated by the LLMs or on proposing mechanisms to achieve deterministic execution. This work takes a closer look at nondeterminism by analyzing the variations on the token probabilities, not on the generated text. Interestingly, all the models evaluated have similar results in both the trends and the actual values of the variations of the probabilities. In particular, the results show that the effects of nondeterminism are significant for token probabilities that are in the range of 0.1 to 0.9, while they are much smaller when the probabilities are close to 0 or 1. This has significant implications for our understanding of nondeterminism. The first is that nondeterminism will likely have a non-negligible impact on generated text when the temperature is not zero, as it introduces significant variations in the token probabilities except when they are close to 0 or 1. Secondly, it suggests that all models have similar non deterministic variations at the token probability level. Therefore, different variations in the performance of the generated text, for example, when measuring accuracy on a benchmark, seem to come from different token probabilities or response lengths. A third implication is that we may be able to estimate the impact of nondeterminism by running a single inference and analyzing the token level probabilities, instead of having to run the same inference many times.

</details>


### [10] [NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs](https://arxiv.org/abs/2601.06126)
*Boshen Shi,Kexin Yang,Yuanbo Yang,Guanguang Chang,Ce Chi,Zhendong Wang,Xing Wang,Junlan Feng*

Main category: cs.AI

TL;DR: NL2Dashboard是一个轻量级框架，通过分析-呈现解耦原则，使用结构化中间表示来生成综合仪表板，显著提升视觉质量、令牌效率和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端仪表板生成方法存在两个根本性限制：1) 由于大量令牌用于视觉渲染导致的表示冗余；2) 分析推理与呈现纠缠导致的低可控性。

Method: 提出NL2Dashboard框架，基于分析-呈现解耦原则，引入结构化中间表示来封装仪表板的内容、布局和视觉元素，将LLM角色限制在数据分析和意图翻译，而将视觉合成卸载给确定性渲染引擎。

Result: NL2Dashboard在多个领域显著优于现有最先进基线，实现了卓越的视觉质量、显著更高的令牌效率，以及在生成和修改任务中的精确可控性。

Conclusion: 通过分析-呈现解耦和结构化中间表示，NL2Dashboard有效解决了现有仪表板生成方法的局限性，为综合仪表板合成提供了轻量级且高效的解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.

</details>


### [11] [PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction](https://arxiv.org/abs/2601.06158)
*Zibin Meng,Kani Chen*

Main category: cs.AI

TL;DR: PsyAgent是一个结合Big Five人格特质和Bourdieu认知社会共同结构的人类智能体模型，通过个体结构和多场景上下文框架实现稳定且情境敏感的行为生成。


<details>
  <summary>Details</summary>
Motivation: 人类智能体需要建模性格特质如何与社会结构互动，现有方法在人格一致性和情境敏感性方面存在不足，需要更精确、数据高效的架构。

Method: 1) 个体结构(IS)：编码特质、认知风格、价值观、文化资本等的人格档案；2) 多场景上下文(MSC)：涵盖八个生活领域的角色-关系-规范框架；3) 通过结构化提示将活跃场景绑定到智能体档案；4) 生成监督数据并微调小型LLM。

Result: PsyAgent在人格一致性、情境适当性、风格匹配、特质可识别性和长期稳定性等指标上，匹配或超越了多个更大的未调优LLM和其他基线模型。消融实验显示IS主要提升特质保真度和风格稳定性，MSC驱动规范意识和决策适应性。

Conclusion: PsyAgent提供了一个精确、数据高效的人格基础智能体架构，通过结合个体结构和多场景框架，实现了稳定且情境敏感的行为生成，为人类智能体建模提供了新方法。

Abstract: Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.

</details>


### [12] [Beyond Accuracy: A Decision-Theoretic Framework for Allocation-Aware Healthcare AI](https://arxiv.org/abs/2601.06161)
*Rifa Ferzana*

Main category: cs.AI

TL;DR: 该论文提出了"分配差距"概念，解释AI预测准确性提升为何未能改善患者结局，并通过约束优化和马尔可夫决策过程建立理论框架，证明在资源约束下分配感知策略优于传统风险阈值方法。


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统在医疗保健领域达到专家级预测准确性，但模型性能改进往往未能转化为患者结局的相应改善。作者旨在解释这种"分配差距"，为资源受限环境中的医疗AI评估和部署提供理论基础。

Method: 将医疗交付建模为约束资源下的随机分配问题，使用约束优化和马尔可夫决策过程理论框架，将AI视为估计效用的决策基础设施而非自主决策者，并通过合成分诊模拟验证分配感知策略。

Result: 理论分析表明改进的估计在稀缺条件下影响最优分配，合成模拟证明分配感知策略在实现效用方面显著优于风险阈值方法，即使预测准确性相同。

Conclusion: 该框架为在资源约束环境中评估和部署医疗AI提供了原则性基础，强调AI应作为决策基础设施估计效用，而非直接做出自主决策，以弥合预测准确性与患者结局改善之间的差距。

Abstract: Artificial intelligence (AI) systems increasingly achieve expert-level predictive accuracy in healthcare, yet improvements in model performance often fail to produce corresponding gains in patient outcomes. We term this disconnect the allocation gap and provide a decision-theoretic explanation by modelling healthcare delivery as a stochastic allocation problem under binding resource constraints. In this framework, AI acts as decision infrastructure that estimates utility rather than making autonomous decisions. Using constrained optimisation and Markov decision processes, we show how improved estimation affects optimal allocation under scarcity. A synthetic triage simulation demonstrates that allocation-aware policies substantially outperform risk-threshold approaches in realised utility, even with identical predictive accuracy. The framework provides a principled basis for evaluating and deploying healthcare AI in resource-constrained settings.

</details>


### [13] [Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation](https://arxiv.org/abs/2601.06188)
*Itai Zilberstein,Steve Chien*

Main category: cs.AI

TL;DR: 提出DCOSP问题框架和D-NSS算法，用于大规模动态卫星星座观测调度，实现分布式自主控制，在NASA FAME任务中进行验证。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星星座规模快速增长，需要分布式机载控制来实现时间敏感的测量和响应，但部署自主性面临计算和通信效率挑战。

Method: 提出DCOSP问题框架（动态分布式约束优化问题的新形式），构建离线全知算法计算最优性条件，并提出D-NSS在线分解算法动态修复和解决子问题。

Result: 仿真显示D-NSS收敛到接近最优解，在解质量、计算时间和消息量方面优于DDCOP基线算法。

Conclusion: DCOSP和D-NSS将成为NASA FAME任务中最大规模分布式多智能体AI在轨演示的基础，推动卫星自主观测调度技术发展。

Abstract: The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to satellites requires efficient computation and communication. This work tackles the challenge of efficiently scheduling observations for hundreds of satellites in a dynamic, large-scale problem with millions of variables. We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of Dynamic Distributed Constraint Optimization Problems (DDCOP) that models integrated scheduling and execution. DCOSP has a novel optimality condition for which we construct an omniscient offline algorithm for its computation. We also present the Dynamic Incremental Neighborhood Stochastic Search algorithm (D-NSS), an incomplete online decomposition-based DDCOP algorithm that repairs and solves sub-problems when problem dynamics occur. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. As part of the NASA FAME mission, DCOSP and D-NSS will be the foundation of the largest in-space demonstration of distributed multi-agent AI to date.

</details>


### [14] [Rational Synthesizers or Heuristic Followers? Analyzing LLMs in RAG-based Question-Answering](https://arxiv.org/abs/2601.06189)
*Atharv Naphade*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在检索增强生成中如何整合冲突证据，发现模型倾向于使用启发式而非事实推理，解释不可靠，对RAG系统设计有重要启示。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成是当前大型语言模型的主流范式，但模型如何整合冲突证据的机制仍不透明。需要探究模型是基于事实强度、先验信念还是重复频率来做出回答。

Method: 引入GroupQA数据集，包含1,635个争议性问题配对的15,058份多样化来源证据文档，标注立场和定性强度。通过控制实验分析群体层面的证据聚合动态。

Result: 研究发现：1）重述论点比提供独立支持更具说服力；2）模型偏好先出现的证据而非后出现的；3）模型越大越抗拒适应呈现的证据；4）LLM对群体答案的解释不可靠。

Conclusion: 大型语言模型在证据整合中表现为易受影响的启发式追随者，而非基于事实的推理者，这对改进RAG系统设计有直接启示。

Abstract: Retrieval-Augmented Generation (RAG) is the prevailing paradigm for grounding Large Language Models (LLMs), yet the mechanisms governing how models integrate groups of conflicting retrieved evidence remain opaque. Does an LLM answer a certain way because the evidence is factually strong, because of a prior belief, or merely because it is repeated frequently? To answer this, we introduce GroupQA, a curated dataset of 1,635 controversial questions paired with 15,058 diversely-sourced evidence documents, annotated for stance and qualitative strength. Through controlled experiments, we characterize group-level evidence aggregation dynamics: Paraphrasing an argument can be more persuasive than providing distinct independent support; Models favor evidence presented first rather than last, and Larger models are increasingly resistant to adapt to presented evidence. Additionally, we find that LLM explanations to group-based answers are unfaithful. Together, we show that LLMs behave consistently as vulnerable heuristic followers, with direct implications for improving RAG system design.

</details>


### [15] [PCoKG: Personality-aware Commonsense Reasoning with Debate](https://arxiv.org/abs/2601.06234)
*Weijie Li,Zhongqing Wang,Guodong Zhou*

Main category: cs.AI

TL;DR: 提出PCoKG人格感知常识知识图谱，包含521,316个四元组，通过LLM角色扮演和辩论机制构建，用于个性化对话生成，提升响应与人格的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有常识推理模型忽视了人格特质的影响，限制了在个性化系统（如对话生成）中的有效性。需要构建能够反映人格差异的常识知识资源。

Method: 1) 从ATOMIC数据集中筛选可能引发不同人格类型多样化推理模式的事件；2) 利用大语言模型的角色扮演能力进行推理；3) 引入辩论机制（支持者、反对者、裁判）通过反馈循环迭代优化生成的知识质量。

Result: 构建了包含521,316个四元组的PCoKG数据集。LoRA微调实验显示模型性能与基础模型参数规模呈正相关。在基于人格的对话生成任务中，PCoKG提升了生成响应与参考输出之间的一致性。

Conclusion: PCoKG填补了常识推理与个体认知差异之间的空白，为开发更个性化、情境感知的AI系统提供了支持，特别是在人格感知的对话生成方面表现出改进效果。

Abstract: Most commonsense reasoning models overlook the influence of personality traits, limiting their effectiveness in personalized systems such as dialogue generation. To address this limitation, we introduce the Personality-aware Commonsense Knowledge Graph (PCoKG), a structured dataset comprising 521,316 quadruples. We begin by employing three evaluators to score and filter events from the ATOMIC dataset, selecting those that are likely to elicit diverse reasoning patterns across different personality types. For knowledge graph construction, we leverage the role-playing capabilities of large language models (LLMs) to perform reasoning tasks. To enhance the quality of the generated knowledge, we incorporate a debate mechanism consisting of a proponent, an opponent, and a judge, which iteratively refines the outputs through feedback loops. We evaluate the dataset from multiple perspectives and conduct fine-tuning and ablation experiments using multiple LLM backbones to assess PCoKG's robustness and the effectiveness of its construction pipeline. Our LoRA-based fine-tuning results indicate a positive correlation between model performance and the parameter scale of the base models. Finally, we apply PCoKG to persona-based dialogue generation, where it demonstrates improved consistency between generated responses and reference outputs. This work bridges the gap between commonsense reasoning and individual cognitive differences, enabling the development of more personalized and context-aware AI systems.

</details>


### [16] [Kolmogorov-Arnold Networks-Based Tolerance-Aware Manufacturability Assessment Integrating Design-for-Manufacturing Principles](https://arxiv.org/abs/2601.06334)
*Masoud Deylami,Negar Izadipour,Adel Alaeddini*

Main category: cs.AI

TL;DR: 该研究提出了一种基于参数化设计特征的制造可行性评估方法，使用Kolmogorov-Arnold Networks直接学习设计参数、公差与制造可行性之间的关系，避免了传统几何驱动方法的信息损失和预处理需求。


<details>
  <summary>Details</summary>
Motivation: 现有AI制造可行性评估框架大多依赖几何驱动方法，需要大量预处理、存在信息损失且可解释性有限。设计到生产之间的鸿沟需要更直接的评估方法，能够明确纳入尺寸公差而无需CAD处理。

Method: 采用Kolmogorov-Arnold Networks直接从参数化设计特征评估制造可行性，生成包含30万个标记设计的合成数据集，涵盖钻孔、铣削和组合加工三种代表性场景，同时考虑加工约束和DFM规则。

Result: 与14种机器学习和深度学习模型对比，KAN在所有场景中表现最佳：钻孔AUC 0.9919、铣削AUC 0.9841、组合加工AUC 0.9406。框架通过样条函数可视化和潜在空间投影提供高可解释性。

Conclusion: 该方法能够直接从参数化特征评估制造可行性，提供高精度和可解释性，通过工业案例展示了如何通过参数级设计修改将不可制造组件转化为可制造组件，为设计-生产集成提供了有效工具。

Abstract: Manufacturability assessment is a critical step in bridging the persistent gap between design and production. While artificial intelligence (AI) has been widely applied to this task, most existing frameworks rely on geometry-driven methods that require extensive preprocessing, suffer from information loss, and offer limited interpretability. This study proposes a methodology that evaluates manufacturability directly from parametric design features, enabling explicit incorporation of dimensional tolerances without requiring computer-aided design (CAD) processing. The approach employs Kolmogorov-Arnold Networks (KANs) to learn functional relationships between design parameters, tolerances, and manufacturability outcomes. A synthetic dataset of 300,000 labeled designs is generated to evaluate performance across three representative scenarios: hole drilling, pocket milling, and combined drilling-milling, while accounting for machining constraints and design-for-manufacturing (DFM) rules. Benchmarking against fourteen machine learning (ML) and deep learning (DL) models shows that KAN achieves the highest performance in all scenarios, with AUC values of 0.9919 for drilling, 0.9841 for milling, and 0.9406 for the combined case. The proposed framework provides high interpretability through spline-based functional visualizations and latent-space projections, enabling identification of the design and tolerance parameters that most strongly influence manufacturability. An industrial case study further demonstrates how the framework enables iterative, parameter-level design modifications that transform a non-manufacturable component into a manufacturable one.

</details>


### [17] [Circuit Mechanisms for Spatial Relation Generation in Diffusion Transformers](https://arxiv.org/abs/2601.06338)
*Binxu Wang,Jingxuan Fan,Xu Pan*

Main category: cs.AI

TL;DR: 研究采用机制可解释性方法分析DiT模型如何生成文本提示中指定的物体空间关系，发现不同文本编码器导致完全不同的工作机制。


<details>
  <summary>Details</summary>
Motivation: 尽管DiT在文本到图像生成方面取得进展，但模型在生成文本提示中指定的正确物体空间关系方面仍有困难。本研究旨在理解DiT如何生成正确的空间关系机制。

Method: 采用机制可解释性方法，从头训练不同规模的DiT模型，使用不同文本编码器（随机嵌入和预训练T5），学习生成包含两个物体及其属性和空间关系的图像。

Result: 所有模型都能近乎完美地学习该任务，但工作机制因文本编码器选择而完全不同：使用随机嵌入时通过两阶段电路（两个交叉注意力头分别读取空间关系和单物体属性）；使用T5时通过不同电路（利用文本令牌中的信息融合，从单个文本令牌同时读取空间关系和单物体信息）。

Conclusion: 两种设置下域内性能相似，但对域外扰动的鲁棒性不同，这可能暗示了在真实场景中生成正确关系的困难。

Abstract: Diffusion Transformers (DiTs) have greatly advanced text-to-image generation, but models still struggle to generate the correct spatial relations between objects as specified in the text prompt. In this study, we adopt a mechanistic interpretability approach to investigate how a DiT can generate correct spatial relations between objects. We train, from scratch, DiTs of different sizes with different text encoders to learn to generate images containing two objects whose attributes and spatial relations are specified in the text prompt. We find that, although all the models can learn this task to near-perfect accuracy, the underlying mechanisms differ drastically depending on the choice of text encoder. When using random text embeddings, we find that the spatial-relation information is passed to image tokens through a two-stage circuit, involving two cross-attention heads that separately read the spatial relation and single-object attributes in the text prompt. When using a pretrained text encoder (T5), we find that the DiT uses a different circuit that leverages information fusion in the text tokens, reading spatial-relation and single-object information together from a single text token. We further show that, although the in-domain performance is similar for the two settings, their robustness to out-of-domain perturbations differs, potentially suggesting the difficulty of generating correct relations in real-world scenarios.

</details>


### [18] [CARD: Cluster-level Adaptation with Reward-guided Decoding for Personalized Text Generation](https://arxiv.org/abs/2601.06352)
*Yutong Song,Jiang Wu,Weijia Zhang,Chengze Shen,Shaofan Yuan,Weitao Lu,Jian Wang,Amir Rahmani,Nikil Dutt,Yu Wang*

Main category: cs.AI

TL;DR: CARD是一个分层个性化框架，通过聚类用户共享风格模式学习集群适配器，再通过对比学习推断用户特定偏好，在推理时仅通过轻量级偏好向量和低秩logit修正实现个性化，保持基础模型冻结。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化适应上面临细粒度个性化与可扩展部署之间的张力，需要一种既能有效个性化又能保持部署效率的解决方案。

Method: CARD采用分层渐进细化框架：1）首先根据共享风格模式聚类用户，学习集群特定的LoRA适配器；2）通过对比用户撰写文本与集群级生成的隐式偏好学习机制，推断用户特定风格偏好；3）推理时仅通过轻量级用户偏好向量和低秩logit修正注入个性化，保持基础模型冻结。

Result: 在LaMP和LongLaMP基准测试中，CARD实现了与最先进基线相当或更优的生成质量，同时显著提高了实际个性化文本生成的效率和可扩展性。

Conclusion: CARD框架通过分层渐进细化的方法，在保持基础模型冻结的同时实现了有效的个性化，平衡了生成质量与部署效率，为可扩展的个性化文本生成提供了实用解决方案。

Abstract: Adapting large language models to individual users remains challenging due to the tension between fine-grained personalization and scalable deployment. We present CARD, a hierarchical framework that achieves effective personalization through progressive refinement. CARD first clusters users according to shared stylistic patterns and learns cluster-specific LoRA adapters, enabling robust generalization and strong low-resource performance. To capture individual differences within each cluster, we propose an implicit preference learning mechanism that contrasts user-authored text with cluster-level generations, allowing the model to infer user-specific style preferences without manual annotation. At inference time, CARD injects personalization exclusively at decoding via lightweight user preference vectors and low-rank logit corrections, while keeping the base model frozen. Experiments on the LaMP and LongLaMP benchmarks show that CARD achieves competitive or superior generation quality compared to state-of-the-art baselines, while significantly improving efficiency and scalability for practical personalized text generation.

</details>


### [19] [Styles + Persona-plug = Customized LLMs](https://arxiv.org/abs/2601.06362)
*Yutong Song,Jiang Wu,Shaofan Yuan,Chengze Shen,Jian Wang,Amir Rahmani,Nikil Dutt,Yu Wang*

Main category: cs.AI

TL;DR: PsPLUG：一种轻量级软提示插件，通过风格条件偏好对比训练，将个性化建模为分布残差，在保持风格忠实度的同时提升个性化对齐


<details>
  <summary>Details</summary>
Motivation: 发现个性化文本生成中一个被忽视的挑战：个性化方法越来越多地在显式风格指令下应用，但在此约束下的行为仍未被充分理解。需要在隐式个性化和显式风格之间取得平衡。

Method: 将个性化建模为分布残差，提出PsPLUG——一种轻量级软提示插件，通过风格条件偏好对比进行训练。

Result: 在LaMP基准测试中，该框架提升了个性化对齐度，保持了风格忠实度，并以最小计算量优于基于检索和软提示的基线方法。

Conclusion: 残差建模为可控、风格感知的LLM个性化提供了一个简单而原则性的基础。

Abstract: We discover a previously overlooked challenge in personalized text generation: personalization methods are increasingly applied under explicit style instructions, yet their behavior under such constraints remains poorly understood. To balance implicit personalization and explicit style, we formulate personalization as a distributional residual and propose PsPLUG, a lightweight soft-prompt plug-in trained with style-conditioned preference contrasts. Across LaMP benchmark, our framework improves persona alignment, maintains stylistic fidelity, and outperforms retrieval-based and soft-prompt baselines with minimal computation. These results show that residual modeling provides a simple and principled foundation for controllable, style-aware LLM personalization.

</details>


### [20] [BizFinBench.v2: A Unified Dual-Mode Bilingual Benchmark for Expert-Level Financial Capability Alignment](https://arxiv.org/abs/2601.06401)
*Xin Guo,Rongjunchen Zhang,Guilong Lu,Xuntao Guo,Shuai Jia,Zhi Yang,Liwen Zhang*

Main category: cs.AI

TL;DR: BizFinBench.v2是首个基于中美股市真实业务数据的大规模金融LLM评估基准，包含8个基础任务和2个在线任务，共29,578个专家级问答对，旨在解决现有基准在真实性和实时性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有金融LLM基准存在依赖模拟或通用样本、关注单一离线静态场景等问题，导致基准性能与实际运营效果存在显著差距，无法满足金融服务对真实性和实时响应的要求。

Method: 基于中美股市真实业务数据构建基准，对金融平台真实用户查询进行聚类分析，形成8个基础任务和2个在线任务，涵盖4个核心业务场景，共29,578个专家级问答对。

Result: 实验结果显示，ChatGPT-5在主任务中达到61.5%准确率，但与金融专家仍有显著差距；在在线任务中，DeepSeek-R1优于所有其他商业LLM。错误分析揭示了现有模型在实际金融业务场景中的具体能力缺陷。

Conclusion: BizFinBench.v2超越了现有基准的局限性，实现了对LLM金融能力的业务级解构，为评估LLM在金融领域广泛部署的效能提供了精确依据。

Abstract: Large language models have undergone rapid evolution, emerging as a pivotal technology for intelligence in financial operations. However, existing benchmarks are often constrained by pitfalls such as reliance on simulated or general-purpose samples and a focus on singular, offline static scenarios. Consequently, they fail to align with the requirements for authenticity and real-time responsiveness in financial services, leading to a significant discrepancy between benchmark performance and actual operational efficacy. To address this, we introduce BizFinBench.v2, the first large-scale evaluation benchmark grounded in authentic business data from both Chinese and U.S. equity markets, integrating online assessment. We performed clustering analysis on authentic user queries from financial platforms, resulting in eight fundamental tasks and two online tasks across four core business scenarios, totaling 29,578 expert-level Q&A pairs. Experimental results demonstrate that ChatGPT-5 achieves a prominent 61.5% accuracy in main tasks, though a substantial gap relative to financial experts persists; in online tasks, DeepSeek-R1 outperforms all other commercial LLMs. Error analysis further identifies the specific capability deficiencies of existing models within practical financial business contexts. BizFinBench.v2 transcends the limitations of current benchmarks, achieving a business-level deconstruction of LLM financial capabilities and providing a precise basis for evaluating efficacy in the widespread deployment of LLMs within the financial domain. The data and code are available at https://github.com/HiThink-Research/BizFinBench.v2.

</details>


### [21] [LSRIF: Logic-Structured Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2601.06431)
*Qingyu Ren,Qianyu He,Jingwen Chang,Jie Zeng,Jiaqing Liang,Yanghua Xiao,Han Xia,Zeye Sun,Fei Yu*

Main category: cs.AI

TL;DR: LSRIF：一种逻辑结构化训练框架，通过显式建模指令逻辑结构（并行、顺序、条件分支）来提升大语言模型的指令跟随能力


<details>
  <summary>Details</summary>
Motivation: 现实世界中的指令通常包含逻辑结构（如顺序依赖和条件分支），但现有方法通常构建具有并行约束的数据集并优化平均奖励，忽略了逻辑依赖关系，导致噪声信号

Method: 提出LSRIF框架：1) 构建LSRInstruct数据集，包含并行、顺序和条件类型的约束结构；2) 设计结构感知奖励方法，包括并行结构的平均聚合、顺序结构的失败惩罚传播和条件分支的选择性奖励

Result: 实验显示LSRIF在指令跟随（领域内和领域外）和一般推理方面带来显著改进。分析表明，通过显式逻辑结构学习带来了注意力层的参数更新，并增强了token级对约束和逻辑运算符的关注

Conclusion: 显式建模指令逻辑结构能有效提升大语言模型的指令跟随能力，LSRIF框架通过结构感知奖励方法实现了这一目标

Abstract: Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.

</details>


### [22] [ConSensus: Multi-Agent Collaboration for Multimodal Sensing](https://arxiv.org/abs/2601.06453)
*Hyungjun Yoon,Mohammad Malekzadeh,Sung-Ju Lee,Fahim Kawsar,Lorena Qendro*

Main category: cs.AI

TL;DR: ConSensus是一个无需训练的多智能体协作框架，通过专门的模态感知智能体分解多模态感知任务，结合语义聚合和统计共识的混合融合机制，在传感器噪声和缺失数据下实现可靠推理，相比单智能体基线平均准确率提升7.1%，融合token成本降低12.7倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在感知和推理人类生理及物理世界时面临准确解释异构多模态传感器数据的挑战。单一LLM在多模态推理中往往表现不一致，导致解释不完整和先验知识偏差。

Method: 提出ConSensus框架：1）将多模态感知任务分解为专门的模态感知智能体；2）提出混合融合机制，结合语义聚合（实现跨模态推理和上下文理解）和统计共识（通过跨模态一致性提供鲁棒性）；3）采用单轮混合融合协议降低计算成本。

Result: 在五个多模态感知基准测试中，ConSensus相比单智能体基线平均准确率提升7.1%。与迭代多智能体辩论方法相比，性能相当或更好，同时通过单轮融合协议将平均融合token成本降低12.7倍。

Conclusion: ConSensus通过多智能体协作和混合融合机制，在传感器噪声和缺失数据下实现了鲁棒且高效的多模态感知推理，为现实世界多模态感知任务提供了有效解决方案。

Abstract: Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.

</details>


### [23] [QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models](https://arxiv.org/abs/2601.06573)
*Zixing Lin,Jiale Wang,Gee Wah Ng,Lee Onn Mak,Chan Zhi Yang Jeriel,Jun Yang Lee,Yaohao Li*

Main category: cs.AI

TL;DR: QMAVIS是一个通过后期融合LMMs、LLMs和语音识别模型构建的长视频-音频理解管道，在长视频分析方面相比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型主要针对短视频（几分钟）进行评估，缺乏对长视频（几分钟到超过一小时）的理解能力，限制了在视频内容分析、感知理解、具身AI等领域的应用。

Method: 采用后期融合策略，将大型多模态模型、大型语言模型和语音识别模型相结合，构建了一个专门用于长视频-音频理解的管道系统。

Result: 在VideoMME（带字幕）数据集上相比VideoLlaMA2和InternVL2等最先进视频-音频LMMs提升了38.75%；在PerceptionTest和EgoSchema数据集上也有最高2%的提升；定性实验显示能够提取长视频中不同场景的细微差别并理解整体叙事。

Conclusion: QMAVIS填补了长视频分析领域的空白，通过多模型融合实现了对长视频内容的有效理解，为感知理解、视频内容分析等应用开辟了新可能性。

Abstract: Large Multimodal Models (LMMs) for video-audio understanding have traditionally been evaluated only on shorter videos of a few minutes long. In this paper, we introduce QMAVIS (Q Team-Multimodal Audio Video Intelligent Sensemaking), a novel long video-audio understanding pipeline built through a late fusion of LMMs, Large Language Models, and speech recognition models. QMAVIS addresses the gap in long-form video analytics, particularly for longer videos of a few minutes to beyond an hour long, opening up new potential applications in sensemaking, video content analysis, embodied AI, etc. Quantitative experiments using QMAVIS demonstrated a 38.75% improvement over state-of-the-art video-audio LMMs like VideoLlaMA2 and InternVL2 on the VideoMME (with subtitles) dataset, which comprises long videos with audio information. Evaluations on other challenging video understanding datasets like PerceptionTest and EgoSchema saw up to 2% improvement, indicating competitive performance. Qualitative experiments also showed that QMAVIS is able to extract the nuances of different scenes in a long video audio content while understanding the overarching narrative. Ablation studies were also conducted to ascertain the impact of each component in the fusion pipeline.

</details>


### [24] [FinForge: Semi-Synthetic Financial Benchmark Generation](https://arxiv.org/abs/2601.06747)
*Glenn Matlin,Akhil Theerthala,Anant Gupta,Anirudh JM,Rayan Castilla,Yi Mei Ng,Sudheer Chava*

Main category: cs.AI

TL;DR: FinForge是一个用于构建金融领域评估基准的半合成管道，通过专家指导的数据策展和受控的LM合成相结合，创建了包含5000多个经过人工验证的问答对的FinForge-5k基准，用于评估语言模型在金融领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前在金融等高风险专业领域评估语言模型面临重大挑战，因为缺乏开放、高质量、领域特定的数据集。现有的通用基准虽然覆盖广泛，但缺乏深度和领域保真度，无法充分评估语言模型在真实世界金融推理中所需的概念理解和定量严谨性。

Method: FinForge采用可扩展的半合成管道，结合专家指导的数据策展和受控的LM合成。方法包括：1）从权威金融来源进行手动和程序化语料库构建；2）使用Gemini 2.5 Flash进行结构化问题生成和验证；3）创建包含100,000个验证文档（总计1.43亿标记）的精选语料库；4）生成包含11个金融子领域的5000多个经过人工验证的问答对。

Result: 评估最先进的开源和闭源模型在FinForge-5k上的表现显示，金融推理能力存在显著差异，领先模型的准确率接近80%。这些发现凸显了该框架在诊断当前模型局限性和指导未来金融领域能力改进方面的实用性。

Conclusion: FinForge提供了一个有效的框架来构建金融特定评估基准，填补了专业领域评估的空白。该框架能够揭示语言模型在金融推理方面的能力差异，并为未来改进提供指导。所有代码和数据都已开源。

Abstract: Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.

</details>


### [25] [From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design](https://arxiv.org/abs/2601.06776)
*Xufei Tian,Wenli Du,Shaoyi Yang,Han Hu,Hui Xin,Shifeng Qu,Ke Ye*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体工作流，实现从文本工艺描述到可执行模拟配置的端到端自动化化工过程设计


<details>
  <summary>Details</summary>
Motivation: 当前化工设计自动化方法主要关注流程图表示，但将其转换为可执行模拟流程仍需要大量手动参数配置，耗时耗力。需要解决从概念设计到实际实施之间的鸿沟。

Method: 采用多智能体工作流，包含任务理解、拓扑生成、参数配置和评估分析四个专门智能体，结合增强型蒙特卡洛树搜索，通过与大语言模型和化工模拟软件的迭代交互实现自动化配置。

Result: 在Simona大规模工艺描述数据集上评估，模拟收敛率相比最先进基线提高31.1%，设计时间相比专家手动设计减少89.0%。

Conclusion: 展示了AI辅助化工过程设计的潜力，为制药、石化、食品加工和制造等过程导向行业提供了通用化的自动化工艺设计解决方案。

Abstract: Process simulation is a critical cornerstone of chemical engineering design. Current automated chemical design methodologies focus mainly on various representations of process flow diagrams. However, transforming these diagrams into executable simulation flowsheets remains a time-consuming and labor-intensive endeavor, requiring extensive manual parameter configuration within simulation software. In this work, we propose a novel multi-agent workflow that leverages the semantic understanding capabilities of large language models(LLMs) and enables iterative interactions with chemical process simulation software, achieving end-to-end automated simulation from textual process specifications to computationally validated software configurations for design enhancement. Our approach integrates four specialized agents responsible for task understanding, topology generation, parameter configuration, and evaluation analysis, respectively, coupled with Enhanced Monte Carlo Tree Search to accurately interpret semantics and robustly generate configurations. Evaluated on Simona, a large-scale process description dataset, our method achieves a 31.1% improvement in the simulation convergence rate compared to state-of-the-art baselines and reduces the design time by 89. 0% compared to the expert manual design. This work demonstrates the potential of AI-assisted chemical process design, which bridges the gap between conceptual design and practical implementation. Our workflow is applicable to diverse process-oriented industries, including pharmaceuticals, petrochemicals, food processing, and manufacturing, offering a generalizable solution for automated process design.

</details>


### [26] [GDEPO: Group Dual-dynamic and Equal-right-advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning](https://arxiv.org/abs/2601.06795)
*Zhengqing Yan,Xinyang Liu,Yi Zhang,Fan Guo,Yao Liu,Junchen Wan,Kang Song*

Main category: cs.AI

TL;DR: 提出GDEPO方法解决自动定理证明中GRPO算法的两个关键问题：复合奖励与形式验证器反馈冲突，以及静态采样策略导致数据浪费。通过动态额外采样、平等权利优势和动态额外迭代三个机制提升数据利用率和优化效率。


<details>
  <summary>Details</summary>
Motivation: 在自动定理证明任务中，GRPO算法面临两个关键问题：1）使用复合奖励时，相对优势估计与形式验证器的二元反馈可能冲突；2）静态采样策略在找不到有效证明时会丢弃整个批次数据，造成数据浪费和模型更新贡献为零。

Method: 提出GDEPO方法，包含三个核心机制：1）动态额外采样：对无效批次重新采样直到发现有效证明；2）平等权利优势：将优势函数的符号（基于正确性）与幅度（由辅助奖励调节）解耦，确保稳定正确的策略更新；3）动态额外迭代：对初始失败但最终成功的样本应用额外梯度步骤，加速困难案例学习。

Result: 在三个不同难度数据集（MinF2F-test、MathOlympiadBench、PutnamBench）上的实验证实了GDEPO的有效性，消融研究验证了其协同组件的必要性。该方法提升了数据利用率和优化效率。

Conclusion: GDEPO方法通过解决GRPO在自动定理证明中的关键限制，提供了一种新的训练范式，增强了数据利用和优化效率，为ATP任务提供了有效的解决方案。

Abstract: Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.

</details>


### [27] [Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy](https://arxiv.org/abs/2601.06801)
*Shujian Gao,Yuan Wang,Jiangtao Yan,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 论文提出"Thinking with Deltas"框架，通过差分视觉推理策略解决多模态强化学习中感知与推理脱耦问题，防止模型成为"盲推理器"。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本奖励的多模态强化学习方法存在感知-推理脱耦问题，模型倾向于绕过视觉感知，仅依赖语言先验生成答案，成为"盲推理器"。

Method: 提出差分视觉推理策略，使用原始、掩码和扰动三种视觉输入构成视觉三元组，通过最大化与掩码输入的推理差异（增强视觉敏感性）和最小化与扰动输入的推理差异（确保视觉鲁棒性）来对齐推理变化与视觉信息差异。

Result: 该方法在通用和医学基准测试中显著优于现有方法，无需外部标注或辅助工具，有效增强了视觉理解能力。

Conclusion: 通过差分视觉推理策略强制模型关注视觉证据，解决了多模态强化学习中的感知-推理脱耦问题，提升了模型的视觉理解能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \textbf{Thinking with Deltas}, a framework driven by a \textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \textit{visual robustness}). By aligning reasoning variations strictly with the \textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.

</details>


### [28] [Seeing through the Conflict: Transparent Knowledge Conflict Handling in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.06842)
*Hua Ye,Siyuan Chen,Ziqi Zhong,Canran Xiao,Haoliang Zhang,Yuhan Wu,Fei Shen*

Main category: cs.AI

TL;DR: TCR框架通过双对比编码器分离语义匹配与事实一致性、评估自答能力、使用SNR加权轻量级软提示，提升RAG系统的冲突检测和知识恢复能力，减少误导上下文影响。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成模型存在幻觉、过度信任噪声片段或忽略关键上下文的问题，需要使决策过程可观察和可控。

Method: TCR框架包含三个核心组件：1) 双对比编码器分离语义匹配和事实一致性；2) 评估自答能力以衡量内部记忆置信度；3) 通过SNR加权的轻量级软提示将三个标量信号传递给生成器。

Result: 在七个基准测试中，TCR将冲突检测F1分数提升5-18点，知识缺口恢复率提高21.4个百分点，误导上下文覆盖减少29.3个百分点，仅增加0.3%参数，信号与人类判断一致并能揭示时间决策模式。

Conclusion: TCR框架通过透明化决策过程，有效解决了RAG系统中的幻觉和上下文误用问题，为检索增强生成提供了可观察和可控的解决方案。

Abstract: Large language models (LLMs) equipped with retrieval--the Retrieval-Augmented Generation (RAG) paradigm--should combine their parametric knowledge with external evidence, yet in practice they often hallucinate, over-trust noisy snippets, or ignore vital context. We introduce TCR (Transparent Conflict Resolution), a plug-and-play framework that makes this decision process observable and controllable. TCR (i) disentangles semantic match and factual consistency via dual contrastive encoders, (ii) estimates self-answerability to gauge confidence in internal memory, and (iii) feeds the three scalar signals to the generator through a lightweight soft-prompt with SNR-based weighting. Across seven benchmarks TCR improves conflict detection (+5-18 F1), raises knowledge-gap recovery by +21.4 pp and cuts misleading-context overrides by -29.3 pp, while adding only 0.3% parameters. The signals align with human judgements and expose temporal decision patterns.

</details>


### [29] [A Brain-like Synergistic Core in LLMs Drives Behaviour and Learning](https://arxiv.org/abs/2601.06851)
*Pedro Urbina-Rodriguez,Zafeirios Fountas,Fernando E. Rosas,Jun Wang,Andrea I. Luppi,Haitham Bou-Ammar,Murray Shanahan,Pedro A. M. Mediano*

Main category: cs.AI

TL;DR: 大型语言模型自发形成类似人脑的协同信息处理核心，这种协同信息组织是智能系统的根本计算原理


<details>
  <summary>Details</summary>
Motivation: 研究生物和人工系统中智能的独立演化，以识别其根本计算原理，探索智能系统信息处理的共同特征

Method: 使用信息分解原理分析多种LLM模型家族和架构，识别协同信息处理区域，通过消融实验和强化学习微调验证协同组件的重要性

Result: 发现LLM中间层存在协同信息处理区域（信息整合超过各部分之和），早期和晚期层依赖冗余处理，这与生物大脑的信息组织方式相似；消融协同组件导致不成比例的行为变化和性能损失；强化学习微调协同区域比训练冗余组件获得更大性能提升

Conclusion: 协同信息处理是智能的根本属性，为模型设计提供原则性目标，并为生物智能提供可检验的预测

Abstract: The independent evolution of intelligence in biological and artificial systems offers a unique opportunity to identify its fundamental computational principles. Here we show that large language models spontaneously develop synergistic cores -- components where information integration exceeds individual parts -- remarkably similar to those in the human brain. Using principles of information decomposition across multiple LLM model families and architectures, we find that areas in middle layers exhibit synergistic processing while early and late layers rely on redundancy, mirroring the informational organisation in biological brains. This organisation emerges through learning and is absent in randomly initialised networks. Crucially, ablating synergistic components causes disproportionate behavioural changes and performance loss, aligning with theoretical predictions about the fragility of synergy. Moreover, fine-tuning synergistic regions through reinforcement learning yields significantly greater performance gains than training redundant components, yet supervised fine-tuning shows no such advantage. This convergence suggests that synergistic information processing is a fundamental property of intelligence, providing targets for principled model design and testable predictions for biological intelligence.

</details>


### [30] [LLM Performance Predictors: Learning When to Escalate in Hybrid Human-AI Moderation Systems](https://arxiv.org/abs/2601.07006)
*Or Bachar,Or Levi,Sardhendu Mishra,Adi Levi,Manpreet Singh Minhas,Justin Miller,Omer Ben-Porat,Eilon Sheetrit,Jonathan Morra*

Main category: cs.AI

TL;DR: 提出基于LLM性能预测器(LPPs)的监督式不确定性量化框架，用于内容审核中的人机协作决策，实现成本感知的选择性分类


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到人机协作内容审核系统中，核心挑战在于判断何时可以信任LLM输出，何时需要升级到人工审核。需要一种可靠的不确定性量化方法来优化人机工作流程

Method: 提出监督式LLM不确定性量化框架，学习基于LLM输出衍生的性能预测器(LPPs)的元模型，包括：对数概率、熵和新型不确定性归因指标。该方法支持成本感知的选择性分类

Result: 在包括现成模型(Gemini、GPT)和开源模型(Llama、Qwen)在内的最先进LLM上，在多模态和多语言审核任务中，相比现有不确定性估计器在准确率-成本权衡方面取得显著改进

Conclusion: 建立了一个原则性的不确定性感知、可扩展且负责任的人机审核工作流程框架，LPPs不仅提升不确定性估计，还通过提供对失败条件的新见解增强了可解释性

Abstract: As LLMs are increasingly integrated into human-in-the-loop content moderation systems, a central challenge is deciding when their outputs can be trusted versus when escalation for human review is preferable. We propose a novel framework for supervised LLM uncertainty quantification, learning a dedicated meta-model based on LLM Performance Predictors (LPPs) derived from LLM outputs: log-probabilities, entropy, and novel uncertainty attribution indicators. We demonstrate that our method enables cost-aware selective classification in real-world human-AI workflows: escalating high-risk cases while automating the rest. Experiments across state-of-the-art LLMs, including both off-the-shelf (Gemini, GPT) and open-source (Llama, Qwen), on multimodal and multilingual moderation tasks, show significant improvements over existing uncertainty estimators in accuracy-cost trade-offs. Beyond uncertainty estimation, the LPPs enhance explainability by providing new insights into failure conditions (e.g., ambiguous content vs. under-specified policy). This work establishes a principled framework for uncertainty-aware, scalable, and responsible human-AI moderation workflows.

</details>


### [31] [Automated Domain Question Mapping (DQM) with Educational Learning Materials](https://arxiv.org/abs/2601.07062)
*Jiho Noh,Mukhesh Raghava Katragadda,Dabae Lee*

Main category: cs.AI

TL;DR: 该研究提出了一种从非结构化教育材料自动构建领域问题地图（DQMs）的创新方法，以替代传统概念地图，解决教育内容复杂性和标注数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统概念地图在自动构建时面临两大挑战：1）缺乏针对多层次教学目的（从低阶到高阶思维）的学科概念设计；2）关于学科概念及其相互关系的标注数据有限。这些问题限制了从非结构化教育材料自动构建概念地图的能力。

Method: 研究引入了一种创新方法，构建领域问题地图（DQMs）而非传统概念地图。通过制定与学习目标相一致的具体问题，DQMs增强了知识表示并提高了学习者的参与准备度。

Result: 研究结果表明，所提出的方法能够有效生成教育问题并识别问题之间的层次关系，从而形成结构化的问题地图，促进下游应用中的个性化和适应性学习。

Conclusion: 领域问题地图（DQMs）作为一种替代传统概念地图的方法，能够更好地应对教育内容的复杂性，通过问题导向的知识表示支持个性化学习应用。

Abstract: Concept maps have been widely utilized in education to depict knowledge structures and the interconnections between disciplinary concepts. Nonetheless, devising a computational method for automatically constructing a concept map from unstructured educational materials presents challenges due to the complexity and variability of educational content. We focus primarily on two challenges: (1) the lack of disciplinary concepts that are specifically designed for multi-level pedagogical purposes from low-order to high-order thinking, and (2) the limited availability of labeled data concerning disciplinary concepts and their interrelationships. To tackle these challenges, this research introduces an innovative approach for constructing Domain Question Maps (DQMs), rather than traditional concept maps. By formulating specific questions aligned with learning objectives, DQMs enhance knowledge representation and improve readiness for learner engagement. The findings indicate that the proposed method can effectively generate educational questions and discern hierarchical relationships among them, leading to structured question maps that facilitate personalized and adaptive learning in downstream applications.

</details>


### [32] [ENTRA: Entropy-Based Redundancy Avoidance in Large Language Model Reasoning](https://arxiv.org/abs/2601.07123)
*Ruichu Cai,Haopeng Du,Qingwen Lin,Yutong Chen,Zijian Li,Boyan Xu*

Main category: cs.AI

TL;DR: ENTRA：基于熵的训练框架，通过抑制冗余推理来减少大型推理模型的过度思考问题，在保持准确性的同时显著缩短输出长度


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在过度思考问题，即使处理简单任务也会生成过长的推理链，导致计算开销大但性能提升有限。现有方法通常限制输出长度或优化正确性，这种粗粒度监督无法引导模型进行简洁而准确的推理。

Method: 提出ENTRA框架：1）使用轻量级双向重要性估计方法评估token级别的重要性，考虑预测置信度和前向影响；2）基于低重要性token的熵计算冗余奖励，并进行归一化处理；3）通过强化学习优化该奖励。

Result: 在数学推理基准测试中，ENTRA将输出长度减少了37%到53%，同时没有损失准确性，在某些情况下甚至提高了准确性。

Conclusion: ENTRA为减少大型推理模型的过度思考问题提供了一个原则性且高效的解决方案，并为冗余感知的推理优化提供了可推广的路径。

Abstract: Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.

</details>


### [33] [AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units](https://arxiv.org/abs/2601.07160)
*Xinzi Cao,Jianyang Zhai,Pengfei Li,Zhiheng Hu,Cen Yan,Bingxu Mu,Guanghuan Fang,Bin She,Jiayu Li,Yihan Su,Dongyang Tao,Xiansong Huang,Fan Xu,Feidiao Yang,Yao Lu,Chang-Dong Wang,Yutong Lu,Weicheng Xue,Bin Zhou,Yonghong Tian*

Main category: cs.AI

TL;DR: AscendKernelGen框架通过领域自适应模型和高质量数据集，显著提升了NPU内核代码生成的编译成功率和功能正确性


<details>
  <summary>Details</summary>
Motivation: NPU在AI基础设施中日益重要，但使用厂商特定DSL开发高性能内核需要深厚的硬件专业知识且劳动密集。通用LLM在NPU领域因严格约束和训练数据稀缺而表现不佳，无法生成功能复杂的NPU内核

Method: 提出AscendKernelGen框架，包含：1) Ascend-CoT数据集（从真实内核实现中提取的思维链推理数据）；2) KernelGen-LM模型（通过监督微调和带执行反馈的强化学习进行领域自适应训练）；3) NPUKernelBench基准（评估编译、正确性和性能的综合测试集）

Result: 在复杂Level-2内核上，编译成功率从0%提升到95.5%（Pass@10），功能正确率达到64.3%，而基线模型完全失败。显著缩小了通用LLM与硬件特定编码之间的差距

Conclusion: 领域特定推理和严格评估在自动化加速器感知代码生成中起关键作用，AscendKernelGen框架有效解决了NPU内核开发的挑战

Abstract: To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline's complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.

</details>


### [34] [Active Context Compression: Autonomous Memory Management in LLM Agents](https://arxiv.org/abs/2601.07190)
*Nikhil Verma*

Main category: cs.AI

TL;DR: Focus是一种受黏菌启发的LLM智能体架构，通过自主压缩交互历史减少上下文膨胀，在保持相同准确率的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在处理长期软件工程任务时面临"上下文膨胀"问题：随着交互历史增长，计算成本激增、延迟增加，且不相关的历史错误会分散注意力，降低推理能力。现有解决方案通常依赖被动的外部摘要机制，智能体无法控制。

Method: 提出Focus架构，受黏菌探索策略启发，让智能体自主决定何时将关键学习内容整合到持久的"知识"块中，并主动修剪原始交互历史。采用优化的脚手架（持久bash + 字符串替换编辑器），在SWE-bench Lite的5个上下文密集型实例上使用Claude Haiku 4.5进行评估。

Result: 通过鼓励频繁压缩的积极提示，Focus实现了22.7%的token减少（1490万→1150万），同时保持相同的准确率（3/5 = 60%）。平均每个任务执行6.0次自主压缩，单个实例的token节省高达57%。

Conclusion: 研究表明，当给予适当的工具和提示时，能力强的模型可以自主调节其上下文，为不牺牲任务性能的成本感知智能体系统开辟了途径。

Abstract: Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to "Context Bloat." As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent "Knowledge" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.

</details>


### [35] [LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing](https://arxiv.org/abs/2601.07206)
*Hao Li,Yiqun Zhang,Zhaoyan Guo,Chenxu Wang,Shengji Tang,Qiaosheng Zhang,Yang Chen,Biqing Qi,Peng Ye,Lei Bai,Zhen Wang,Shuyue Hu*

Main category: cs.AI

TL;DR: LLMRouterBench是一个大规模LLM路由基准测试框架，包含40万+实例、21个数据集和33个模型，系统评估了10种路由方法，发现多数方法性能相似，简单基线方法已足够有效，与Oracle路由仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: LLM路由旨在将查询分配给集成模型中最合适的模型，但现有评估缺乏统一标准。需要建立一个全面基准来系统评估不同路由方法的性能，揭示当前方法的局限性和改进方向。

Method: 构建LLMRouterBench基准框架，包含超过40万个实例、21个数据集和33个模型。提供面向性能的路由和性能-成本权衡路由的综合指标，集成10种代表性路由基线方法。在统一框架下系统重新评估各种路由方法。

Result: 确认了模型互补性（LLM路由的核心前提），但发现许多路由方法在统一评估下表现相似，包括商业路由器在内的几种最新方法未能可靠地超越简单基线。与Oracle路由仍有显著差距，主要由持续的模型召回失败驱动。骨干嵌入模型影响有限，更大集成相比精心模型筛选收益递减。

Conclusion: LLM路由领域需要更有效的路由方法来缩小与Oracle的差距，当前许多复杂方法并未显著优于简单基线。LLMRouterBench为系统评估提供了统一框架，支持延迟感知分析，有助于推动该领域发展。

Abstract: Large language model (LLM) routing assigns each query to the most suitable model from an ensemble. We introduce LLMRouterBench, a large-scale benchmark and unified framework for LLM routing. It comprises over 400K instances from 21 datasets and 33 models. Moreover, it provides comprehensive metrics for both performance-oriented routing and performance-cost trade-off routing, and integrates 10 representative routing baselines. Using LLMRouterBench, we systematically re-evaluate the field. While confirming strong model complementarity-the central premise of LLM routing-we find that many routing methods exhibit similar performance under unified evaluation, and several recent approaches, including commercial routers, fail to reliably outperform a simple baseline. Meanwhile, a substantial gap remains to the Oracle, driven primarily by persistent model-recall failures. We further show that backbone embedding models have limited impact, that larger ensembles exhibit diminishing returns compared to careful model curation, and that the benchmark also enables latency-aware analysis. All code and data are available at https://github.com/ynulihao/LLMRouterBench.

</details>


### [36] [Yes FLoReNce, I Will Do Better Next Time! Agentic Feedback Reasoning for Humorous Meme Detection](https://arxiv.org/abs/2601.07232)
*Olivia Shanhong Liu,Pai Chet Ng,De Wen Soh,Konstantinos N. Plataniotis*

Main category: cs.AI

TL;DR: FLoReNce是一个基于反馈推理的智能体框架，通过闭环学习（批评-反馈）和开环推理（经验检索）来提升幽默梗图的理解能力，无需微调即可实现自适应推理。


<details>
  <summary>Details</summary>
Motivation: 幽默梗图融合视觉和文本线索传达讽刺或社会评论，现有AI模型只能生成解释但无法批判或精炼推理，缺乏闭环反馈机制。

Method: 提出FLoReNce框架：学习阶段采用闭环过程，推理智能体接受评判者批评，错误和语义反馈转化为控制信号存入非参数知识库；推理阶段检索相似经验来调节提示，实现自对齐推理。

Result: 在PrideMM数据集上，FLoReNce在预测性能和解释质量上均优于静态多模态基线模型，证明反馈调节提示是自适应幽默理解的有效路径。

Conclusion: 反馈调节的提示机制为自适应幽默梗图理解提供了可行方案，通过闭环学习积累经验并在推理中应用，显著提升了AI系统的意图理解能力。

Abstract: Humorous memes blend visual and textual cues to convey irony, satire, or social commentary, posing unique challenges for AI systems that must interpret intent rather than surface correlations. Existing multimodal or prompting-based models generate explanations for humor but operate in an open loop,lacking the ability to critique or refine their reasoning once a prediction is made. We propose FLoReNce, an agentic feedback reasoning framework that treats meme understanding as a closed-loop process during learning and an open-loop process during inference. In the closed loop, a reasoning agent is critiqued by a judge; the error and semantic feedback are converted into control signals and stored in a feedback-informed, non-parametric knowledge base. At inference, the model retrieves similar judged experiences from this KB and uses them to modulate its prompt, enabling better, self-aligned reasoning without finetuning. On the PrideMM dataset, FLoReNce improves both predictive performance and explanation quality over static multimodal baselines, showing that feedback-regulated prompting is a viable path to adaptive meme humor understanding.

</details>


### [37] [From "Thinking" to "Justifying": Aligning High-Stakes Explainability with Professional Communication Standards](https://arxiv.org/abs/2601.07233)
*Chen Qian,Yimeng Wang,Yu Chen,Lingfei Wu,Andreas Stathopoulos*

Main category: cs.AI

TL;DR: 论文提出"结果→论证"方法，通过结构化论证框架SEF提升AI解释的可验证性和可靠性，相比思维链方法在多个任务上准确率提升5.3%


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，可解释AI需要帮助利益相关者信任和验证系统输出。但思维链方法先推理后得出结论，逻辑漏洞或幻觉可能导致结论与论证不一致，缺乏可验证性。

Method: 提出"结果→论证"方法，约束输出通信为先呈现结论后提供结构化论证。引入SEF（结构化可解释性框架），通过六个指标评估结构和基础性，操作化专业惯例如CREAC、BLUF。

Result: 在三个领域的四个任务上进行实验验证：所有六个指标都与正确性相关（r=0.20-0.42；p<0.001），SEF达到83.9%的准确率（比思维链方法高5.3%）。

Conclusion: 结构化论证可以改善可验证性，也可能提高可靠性，为高风险领域的可解释AI提供了有效的框架。

Abstract: Explainable AI (XAI) in high-stakes domains should help stakeholders trust and verify system outputs. Yet Chain-of-Thought methods reason before concluding, and logical gaps or hallucinations can yield conclusions that do not reliably align with their rationale. Thus, we propose "Result -> Justify", which constrains the output communication to present a conclusion before its structured justification. We introduce SEF (Structured Explainability Framework), operationalizing professional conventions (e.g., CREAC, BLUF) via six metrics for structure and grounding. Experiments across four tasks in three domains validate this approach: all six metrics correlate with correctness (r=0.20-0.42; p<0.001), and SEF achieves 83.9% accuracy (+5.3 over CoT). These results suggest structured justification can improve verifiability and may also improve reliability.

</details>


### [38] [Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning](https://arxiv.org/abs/2601.07238)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Fei Mi,Lifeng Shang*

Main category: cs.AI

TL;DR: GPSO通过强化学习框架优化大型推理模型，使其能够根据问题特征选择最优推理模式，从而提升数学和科学基准测试的性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然展现出多样化的高级推理模式，但现有训练方法往往使模型偏向有限的几种主导模式。研究发现不同推理模式在数学和科学基准测试上存在显著的准确率差异，模型的默认推理模式通常不是特定问题的最优选择。

Method: 提出了Group Pattern Selection Optimization (GPSO)框架，扩展了GRPO方法，包含：多模式展开、基于验证器的问题级最优模式选择、以及在优化过程中使用注意力掩码防止显式模式后缀泄露到学习策略中。

Result: 广泛的实验表明，GPSO在各种模型架构和基准测试上都能带来一致且显著的性能提升，有效缓解了模式次优性问题，并促进了更鲁棒、适应性更强的推理能力。

Conclusion: GPSO通过探索多样化的推理策略组合，并在最有效的策略上优化策略，使模型能够内化从问题特征到最优推理模式的映射，从而提升推理模型的整体性能。

Abstract: Large reasoning models (LRMs) exhibit diverse high-level reasoning patterns (e.g., direct solution, reflection-and-verification, and exploring multiple solutions), yet prevailing training recipes implicitly bias models toward a limited set of dominant patterns. Through a systematic analysis, we identify substantial accuracy variance across these patterns on mathematics and science benchmarks, revealing that a model's default reasoning pattern is often sub-optimal for a given problem. To address this, we introduce Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that extends GRPO by incorporating multi-pattern rollouts, verifier-guided optimal pattern selection per problem, and attention masking during optimization to prevent the leakage of explicit pattern suffixes into the learned policy. By exploring a portfolio of diverse reasoning strategies and optimizing the policy on the most effective ones, GPSO enables the model to internalize the mapping from problem characteristics to optimal reasoning patterns. Extensive experiments demonstrate that GPSO delivers consistent and substantial performance gains across various model backbones and benchmarks, effectively mitigating pattern sub-optimality and fostering more robust, adaptable reasoning. All data and codes are available at https://github.com/wanghanbinpanda/GPSO.

</details>


### [39] [Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition](https://arxiv.org/abs/2601.07239)
*Tanmay Joshi,Shourya Aggarwal,Anusa Saha,Aadi Pandey,Shreyash Dhoot,Vighnesh Rai,Raxit Goswami,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: 论文反对LLM确定性推理，认为其掩盖了模型的不确定性、抑制涌现能力、弱化安全对齐，主张采用随机CHAOS方法处理分布变异性。


<details>
  <summary>Details</summary>
Motivation: 传统软件追求确定性推理，但LLM本质上是条件概率分布而非固定函数。当前将确定性推理作为企业可靠性和可复现性前提的做法，实际上掩盖了LLM作为概率模型的核心特性，包括不确定性建模、涌现能力、多路径推理和安全风险等关键认知属性。

Method: 提出Stochastic CHAOS方法，将分布变异性视为需要测量和控制的信号，而非需要消除的噪声。通过多样本评估揭示LLM的真实能力分布，而非单一确定性输出。

Result: 实证研究表明：1）确定性推理系统性误导评估，低估能力和脆弱性；2）贪婪解码使涌现能力的相变现象消失；3）强制确定性推理降低多路径推理的准确性和诊断洞察；4）确定性评估低估安全风险，隐藏罕见但危险的行为。

Conclusion: LLM推理应接受而非抑制其概率本质。确定性推理"杀死"了LLM的关键认知属性，而Stochastic CHAOS方法通过测量和控制分布变异性，能更真实地反映LLM的能力和风险，为可靠部署提供更全面的评估框架。

Abstract: Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability.
  In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled.
  Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.

</details>


### [40] [LRAS: Advanced Legal Reasoning with Agentic Search](https://arxiv.org/abs/2601.07296)
*Yujin Zhou,Chuxue Cao,Jinluan Yang,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: LRAS框架将法律大语言模型从静态参数化推理转变为动态交互式主动查询，通过自省模仿学习和难度感知强化学习解决法律推理中的知识边界识别问题，性能提升8.2-32%。


<details>
  <summary>Details</summary>
Motivation: 现有法律大语言模型依赖内部参数知识的"闭环推理"，缺乏对知识边界的自我认知，导致自信但错误的结论，无法满足法律领域对程序严谨性和逻辑一致性的严格要求。

Method: 提出LRAS框架，整合自省模仿学习和难度感知强化学习，使法律推理模型能够识别知识边界并处理法律推理复杂性，从静态参数化"闭环思维"转变为动态交互式"主动查询"。

Result: LRAS在实验中优于现有最先进基线8.2-32%，在需要可靠知识的深度推理任务中提升最为显著。

Conclusion: LRAS框架成功解决了法律大语言模型的知识边界识别问题，显著提升了法律推理性能，为法律AI应用提供了新的研究方向。

Abstract: While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on "closed-loop reasoning" derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric "closed-loop thinking" to dynamic and interactive "Active Inquiry". By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.

</details>


### [41] [On the universal definition of intelligence](https://arxiv.org/abs/2601.07364)
*Joseph Chen*

Main category: cs.AI

TL;DR: 本文提出扩展预测假说(EPH)作为比较人类与AI智能的通用定义，认为智能是准确预测未来并从预测中获益的能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，如何公平一致地比较人类与人工智能成为重要理论问题。现有智能定义大多以人类为中心，不适合实证比较，导致研究领域缺乏共识。

Method: 基于卡尔纳普的概念澄清方法论，提出四个评估智能定义的标准：与原概念的相似性、精确性、丰富性和简洁性。然后分析六种代表性定义，并提出扩展预测假说(EPH)，将智能定义为准确预测未来并从预测中获益的能力，区分自发与反应性预测，并引入获益性概念。

Result: 分析发现基于预测能力的定义具有高解释力和实证可行性，但无法充分解释预测与行为/获益的关系。EPH通过整合预测能力和获益能力，为创造力、学习、未来规划等智能各方面提供了统一解释框架。

Conclusion: 扩展预测假说(EPH)是最令人满意且通用的定义，适用于公平一致地比较人类与人工智能。

Abstract: This paper aims to propose a universal definition of intelligence that enables fair and consistent comparison of human and artificial intelligence (AI). With the rapid development of AI technology in recent years, how to compare and evaluate human and AI intelligence has become an important theoretical issue. However, existing definitions of intelligence are anthropocentric and unsuitable for empirical comparison, resulting in a lack of consensus in the research field.
  This paper first introduces four criteria for evaluating intelligence definitions based on R. Carnap's methodology of conceptual clarification: similarity to explicandum, exactness, fruitfulness, and simplicity. We then examine six representative definitions: IQ testing, complex problem-solving ability, reward optimization, environmental adaptation, learning efficiency, and predictive ability, and clarify their theoretical strengths and limitations.
  The results show that while definitions based on predictive ability have high explanatory power and empirical feasibility, they suffer from an inability to adequately explain the relationship between predictions and behavior/benefits. This paper proposes the Extended Predictive Hypothesis (EPH), which views intelligence as a combination of the ability to accurately predict the future and the ability to benefit from those predictions. Furthermore, by distinguishing predictive ability into spontaneous and reactive predictions and adding the concept of gainability, we present a unified framework for explaining various aspects of intelligence, such as creativity, learning, and future planning. In conclusion, this paper argues that the EPH is the most satisfactory and universal definition for comparing human and AI intelligence.

</details>


### [42] [OpenTinker: Separating Concerns in Agentic Reinforcement Learning](https://arxiv.org/abs/2601.07376)
*Siqi Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: OpenTinker是一个用于大语言模型智能体强化学习的开源基础设施，采用模块化设计，将算法、执行和智能体-环境交互分离，支持多种训练模式和多智能体扩展。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习系统通常采用端到端的单一管道设计，缺乏模块化和灵活性。OpenTinker旨在通过解耦设计原则，构建一个轻量级、可组合的智能体学习基础设施，提高开发效率和系统可扩展性。

Method: OpenTinker采用关注点分离的设计理念：1）用户定义智能体、环境和交互协议；2）推理和训练由托管执行运行时处理；3）引入集中式调度器管理LoRA、全参数RL、监督微调和推理等任务；4）支持多智能体训练扩展。

Result: 论文展示了一系列强化学习用例，证明了OpenTinker在实际智能体学习场景中的有效性。该框架能够有效管理共享资源上的多样化训练和推理工作负载。

Conclusion: OpenTinker提供了一个灵活、可扩展的强化学习基础设施，通过模块化设计和关注点分离，简化了大语言模型智能体的训练流程，并为多智能体系统提供了良好的扩展基础。

Abstract: We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.

</details>


### [43] [Software-Hardware Co-optimization for Modular E2E AV Paradigm: A Unified Framework of Optimization Approaches, Simulation Environment and Evaluation Metrics](https://arxiv.org/abs/2601.07393)
*Chengzhi Ji,Xingfeng Li,Zhaodong Lv,Hao Sun,Pan Liu,Hao Frank Yang,Ziyuan Pu*

Main category: cs.AI

TL;DR: 提出一个软硬件协同优化框架，用于模块化端到端自动驾驶推理，在保持驾驶性能的同时显著降低延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 现有模块化端到端自动驾驶研究主要关注精度提升，忽视了推理延迟和能耗等系统级因素，导致模型设计越来越复杂，难以实际部署。现有的软件或硬件单独优化方法效果有限，需要软硬件协同优化来解决实际问题。

Method: 提出一个可重用的软硬件协同优化和闭环评估框架，将软件级模型优化与硬件级计算优化在统一的系统级目标下联合集成，并引入多维评估指标来综合评估安全性、舒适性、效率、延迟和能耗。

Result: 在多个模块化端到端自动驾驶堆栈上的实验表明，该框架在保持基准级驾驶性能的同时，显著降低了推理延迟和能耗，实现了整体系统级的实质性改进。

Conclusion: 该框架为模块化端到端自动驾驶系统的高效部署提供了实用且可操作的指导，通过软硬件协同优化解决了实际部署中的关键挑战。

Abstract: Modular end-to-end (ME2E) autonomous driving paradigms combine modular interpretability with global optimization capability and have demonstrated strong performance. However, existing studies mainly focus on accuracy improvement, while critical system-level factors such as inference latency and energy consumption are often overlooked, resulting in increasingly complex model designs that hinder practical deployment. Prior efforts on model compression and acceleration typically optimize either the software or hardware side in isolation. Software-only optimization cannot fundamentally remove intermediate tensor access and operator scheduling overheads, whereas hardware-only optimization is constrained by model structure and precision. As a result, the real-world benefits of such optimizations are often limited. To address these challenges, this paper proposes a reusable software and hardware co-optimization and closed-loop evaluation framework for ME2E autonomous driving inference. The framework jointly integrates software-level model optimization with hardware-level computation optimization under a unified system-level objective. In addition, a multidimensional evaluation metric is introduced to assess system performance by jointly considering safety, comfort, efficiency, latency, and energy, enabling quantitative comparison of different optimization strategies. Experiments across multiple ME2E autonomous driving stacks show that the proposed framework preserves baseline-level driving performance while significantly reducing inference latency and energy consumption, achieving substantial overall system-level improvements. These results demonstrate that the proposed framework provides practical and actionable guidance for efficient deployment of ME2E autonomous driving systems.

</details>


### [44] [Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.07463)
*Sijia li,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.AI

TL;DR: 提出LOGO世界模型框架，通过局部预测推断全局状态动态，生成合成数据扩展离线多智能体强化学习的数据分布，结合不确定性感知采样机制，显著提升策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有离线多智能体强化学习方法过于保守，局限于数据集分布，难以泛化到数据支持范围之外。基于模型的方法通过世界模型生成合成数据扩展数据集，但多智能体系统的高维性、非平稳性和复杂性使得准确估计转移和奖励函数具有挑战性。

Method: 提出局部到全局（LOGO）世界模型框架，利用更容易估计的局部预测来推断全局状态动态，提高预测准确性并隐式捕获智能体间依赖关系。使用训练好的世界模型生成合成数据扩展原始数据集，并引入不确定性感知采样机制，根据预测不确定性自适应加权合成数据，减少近似误差向策略的传播。

Result: 在8个场景中与8个基线方法进行广泛实验，结果表明该方法在标准离线多智能体强化学习基准测试中超越了最先进的基线方法，为可泛化的离线多智能体学习建立了新的基于模型的基准。

Conclusion: LOGO世界模型框架通过局部预测推断全局动态，结合不确定性感知采样，有效解决了离线多智能体强化学习中数据分布受限的问题，显著提升了策略的泛化能力，为模型驱动的离线多智能体学习提供了新的解决方案。

Abstract: Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.

</details>


### [45] [IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning](https://arxiv.org/abs/2601.07464)
*Xiaoheng Wang,Tongxuan Liu,Zi Gong,Xianzhe Dong,Yuting Zeng,Minhan Hu,Weizhe Huang,Jing Li*

Main category: cs.AI

TL;DR: IFDNS是一种基于提示的神经符号方法，通过多轮反馈机制解决LLM在复杂逻辑关系处理中的局限性，显著提升逻辑推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法（如CoT）在逻辑推理中存在忠实性问题，推理链与结论不一致；神经符号方法存在信息丢失问题，需要改进。

Method: 提出IFDNS方法，在逻辑提取阶段使用迭代反馈机制，准确提取因果关系陈述并转换为命题和逻辑蕴含表达式，减少信息损失。

Result: 在六个数据集上的实验表明，IFDNS显著提升了CoT和CoT-SC的性能，在LogiQA数据集上CoT准确率提升9.40%，在PrOntoQA数据集上CoT-SC提升11.70%。

Conclusion: IFDNS通过迭代反馈机制有效解决了LLM逻辑推理中的信息丢失问题，且与现有提示方法正交，可无缝集成，显著提升推理性能。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.

</details>


### [46] [Knowledge Distillation for LLM-Based Human Activity Recognition in Homes](https://arxiv.org/abs/2601.07469)
*Julien Cumin,Oussama Er-Rahmany,Xi Chen*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在家庭活动识别中的应用，通过实验展示了模型大小对识别性能的影响，并利用知识蒸馏技术训练小型模型，使其性能接近大型模型但参数量减少50倍。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别是情境感知应用的核心问题，特别是在智能家居和辅助生活领域。最近研究表明大型语言模型可用于家庭活动识别并取得高性能，本文旨在进一步探索LLM在HAR中的应用。

Method: 在两个最先进的数据集上进行实验，研究LLM大小对识别性能的影响；使用知识蒸馏技术，用大型LLM生成的HAR推理示例来微调小型LLM。

Result: 实验显示识别性能随LLM大小而变化；通过知识蒸馏微调的小型模型性能几乎与最大的LLM相当，但参数量减少了50倍。

Conclusion: 知识蒸馏技术可以有效训练小型LLM用于家庭活动识别，在保持高性能的同时大幅减少模型参数，为实际部署提供了可行方案。

Abstract: Human Activity Recognition (HAR) is a central problem for context-aware applications, especially for smart homes and assisted living. A few very recent studies have shown that Large Language Models (LLMs) can be used for HAR at home, reaching high performance and addressing key challenges. In this paper, we provide new experimental results regarding the use of LLMs for HAR, on two state-of-the-art datasets. More specifically, we show how recognition performance evolves depending on the size of the LLM used. Moreover, we experiment on the use of knowledge distillation techniques to fine-tune smaller LLMs with HAR reasoning examples generated by larger LLMs. We show that such fine-tuned models can perform almost as well as the largest LLMs, while having 50 times less parameters.

</details>


### [47] [Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory](https://arxiv.org/abs/2601.07470)
*Sirui Liang,Pengfei Cao,Jian Zhao,Wenhao Teng,Xiangwen Liao,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: MCMA方法将记忆抽象作为可学习的认知技能，通过冻结的任务模型和学习的记忆副驾驶实现任务执行与记忆管理的解耦，显著提升了LLM智能体在长时决策任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体方法通常将记忆存储在固定表示中，并在单一或隐式抽象级别上重用，这限制了泛化能力，在分布偏移时容易导致负迁移。

Method: 提出元认知记忆抽象方法(MCMA)，将记忆抽象视为可学习的认知技能而非固定设计选择。方法包含冻结的任务模型和学习的记忆副驾驶，记忆副驾驶通过直接偏好优化训练，决定记忆的结构化、抽象和重用方式。记忆组织成抽象层次结构，基于任务相似性选择性重用。

Result: 在ALFWorld、ScienceWorld和BabyAI上的实验表明，MCMA在性能、分布外泛化和跨任务迁移方面相比多个基线方法有显著提升。

Conclusion: MCMA通过将记忆抽象作为可学习技能，实现了更好的记忆管理和重用，提高了LLM智能体在复杂决策任务中的泛化能力和适应性。

Abstract: Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.

</details>


### [48] [JudgeFlow: Agentic Workflow Optimization via Block Judge](https://arxiv.org/abs/2601.07477)
*Zihan Ma,Zhikai Zhao,Chuanbo Hua,Federico Berto,Jinkyoo Park*

Main category: cs.AI

TL;DR: 提出了一种名为JudgeFlow的评估-判断-优化-更新流水线，用于优化基于LLM的智能体工作流，通过可重用逻辑块和细粒度责任评分来提高优化效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体工作流优化方法依赖粗粒度的端到端评估信号，缺乏细粒度信号来指导具体优化位置，导致修改效率低下或影响有限。

Method: 提出JudgeFlow框架：1) 将智能体工作流分解为可重用、可配置的逻辑块；2) 设计专门的Judge模块分析执行轨迹（特别是失败运行），为问题块分配基于排名的责任分数；3) 基于LLM的优化器利用这些细粒度诊断信号，专注于工作流中最有问题的块进行修改。

Result: 在数学推理和代码生成基准测试中，JudgeFlow相比现有方法取得了更优的性能和效率，提高了样本效率，并通过块级诊断增强了可解释性。

Conclusion: JudgeFlow为自动化日益复杂的智能体工作流提供了可扩展的基础，通过细粒度诊断和针对性优化解决了当前工作流优化方法的局限性。

Abstract: Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\our{}} on mathematical reasoning and code generation benchmarks, where {\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.

</details>


### [49] [VirtualEnv: A Platform for Embodied AI Research](https://arxiv.org/abs/2601.07553)
*Kabir Swain,Sijie Han,Ayush Raina,Jin Zhang,Shuang Li,Michael Stopa,Antonio Torralba*

Main category: cs.AI

TL;DR: VirtualEnv是一个基于虚幻引擎5构建的下一代仿真平台，用于在具身交互场景中对大语言模型进行细粒度基准测试，支持对象操作、导航、多智能体协作等丰富交互，并提供用户友好的API和开源平台。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理和决策能力上的不断提升，需要现实且交互式的环境来严格评估其能力。现有环境往往缺乏足够的真实性和交互性，无法充分测试LLMs在具身场景中的表现。

Method: 基于虚幻引擎5构建仿真平台，提供用户友好的API支持自然语言指令控制LLM驱动的智能体。集成大规模LLMs和视觉语言模型，从多模态输入生成新颖环境和结构化任务。支持对象操作、导航、自适应多智能体协作等交互，以及逃生室和程序生成环境等游戏机制。

Result: 实验对多个流行LLMs在复杂度递增的任务上进行了基准测试，分析了它们在适应性、规划和多智能体协调方面的差异。平台实现了程序化任务生成、任务验证和实时环境控制的方法论。

Conclusion: VirtualEnv作为开源平台发布，旨在推动AI与游戏交叉领域的研究，为具身AI设置中的LLMs提供标准化评估，并为沉浸式仿真和交互娱乐的未来发展铺平道路。

Abstract: As large language models (LLMs) continue to improve in reasoning and decision-making, there is a growing need for realistic and interactive environments where their abilities can be rigorously evaluated. We present VirtualEnv, a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. VirtualEnv supports rich agent-environment interactions, including object manipulation, navigation, and adaptive multi-agent collaboration, as well as game-inspired mechanics like escape rooms and procedurally generated environments. We provide a user-friendly API built on top of Unreal Engine, allowing researchers to deploy and control LLM-driven agents using natural language instructions. We integrate large-scale LLMs and vision-language models (VLMs), such as GPT-based models, to generate novel environments and structured tasks from multimodal inputs. Our experiments benchmark the performance of several popular LLMs across tasks of increasing complexity, analyzing differences in adaptability, planning, and multi-agent coordination. We also describe our methodology for procedural task generation, task validation, and real-time environment control. VirtualEnv is released as an open-source platform, we aim to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.

</details>


### [50] [Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents](https://arxiv.org/abs/2601.07577)
*Yunfan Li,Bingbing Xu,Xueyun Tian,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: TDP框架通过任务解耦解决LLM智能体在长时任务规划中的问题，将任务分解为DAG子目标，使用监督器、规划器和执行器进行局部推理和重规划，防止错误传播，提高鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体规划方法存在两个主要问题：逐步规划短视，一次性规划脆弱，且都面临上下文纠缠问题。上下文纠缠导致认知负荷增加，局部错误会传播到其他独立决策中，使恢复计算成本高昂。

Method: 提出任务解耦规划(TDP)框架：1) 通过监督器将任务分解为有向无环图(DAG)的子目标；2) 使用规划器和执行器在限定上下文中工作；3) 将推理和重规划限制在当前子任务内，实现局部错误纠正而不影响整体工作流。

Result: 在TravelPlanner、ScienceWorld和HotpotQA三个任务上的实验结果显示，TDP优于强基线方法，同时将token消耗减少高达82%，证明子任务解耦能同时提高长时任务智能体的鲁棒性和效率。

Conclusion: 任务解耦规划通过将复杂任务分解为独立子目标并限制推理范围，有效解决了现有规划方法的局限性，显著提升了LLM智能体在长时任务执行中的性能和效率。

Abstract: Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.

</details>


### [51] [DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning](https://arxiv.org/abs/2601.07611)
*Zhuoyang Zou,Abolfazl Ansari,Delvin Ce Zhang,Dongwon Lee,Wenpeng Yin*

Main category: cs.AI

TL;DR: DIAGPaper是一个新颖的多智能体框架，通过三个紧密集成的模块解决现有论文弱点识别方法的局限性：定制器模块模拟人类评审标准，反驳模块引入作者智能体进行结构化辩论，优先级模块学习人类评审实践来评估弱点严重性。


<details>
  <summary>Details</summary>
Motivation: 现有论文弱点识别方法存在三个主要局限：1）多智能体系统只在表面层次模拟人类角色，忽略了专家评估论文互补智力方面的深层标准；2）先前方法隐含假设识别的弱点都是有效的，忽略了评审者偏见、误解以及作者反驳在验证评审质量中的关键作用；3）大多数系统输出未排序的弱点列表，而不是为用户优先考虑最重要的问题。

Method: DIAGPaper包含三个紧密集成的模块：1）定制器模块：模拟人类定义的评审标准，实例化具有特定标准专业知识的多个评审者智能体；2）反驳模块：引入作者智能体，与评审者智能体进行结构化辩论，验证和完善提出的弱点；3）优先级模块：从大规模人类评审实践中学习，评估已验证弱点的严重性，并向用户展示最严重的K个弱点。

Result: 在AAAR和ReviewCritique两个基准测试上的实验表明，DIAGPaper显著优于现有方法，能够产生更有效、更针对具体论文的弱点，并以用户导向、优先级排序的方式呈现。

Conclusion: DIAGPaper通过集成定制化评审标准、结构化作者-评审者辩论和基于人类实践的优先级排序，解决了现有论文弱点识别方法的关键局限，提供了更有效、更用户友好的弱点识别框架。

Abstract: Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.

</details>


### [52] [SALT-KG: A Benchmark for Semantics-Aware Learning on Enterprise Tables](https://arxiv.org/abs/2601.07638)
*Isaiah Onando Mulang,Felix Sasaki,Tassilo Klein,Jonas Kolk,Nikolay Grechanov,Johannes Hoffart*

Main category: cs.AI

TL;DR: SALT-KG扩展SALT基准，将企业表格与元数据知识图谱链接，评估模型在表格证据和语义上下文上的联合推理能力


<details>
  <summary>Details</summary>
Motivation: 企业表格通常缺乏语义上下文，现有基准无法评估模型如何结合表格数据和业务知识进行推理。需要建立能够评估模型在结构化数据中利用语义信息能力的基准。

Method: 扩展SALT基准，将多表交易数据与元数据知识图谱(OBKG)链接，该知识图谱捕获字段级描述、关系依赖和业务对象类型。将表格预测重新定义为语义条件推理问题。

Result: 元数据特征在传统预测指标上带来适度改进，但更重要的发现是这些特征一致地揭示了模型在利用关系上下文语义方面的能力差距。

Conclusion: SALT-KG为基于声明性知识的表格基础模型建立了基准，为企业级结构化数据中的语义链接表格提供了首个实证步骤，推动了表格预测向语义条件推理的转变。

Abstract: Building upon the SALT benchmark for relational prediction (Klein et al., 2024), we introduce SALT-KG, a benchmark for semantics-aware learning on enterprise tables. SALT-KG extends SALT by linking its multi-table transactional data with a structured Operational Business Knowledge represented in a Metadata Knowledge Graph (OBKG) that captures field-level descriptions, relational dependencies, and business object types. This extension enables evaluation of models that jointly reason over tabular evidence and contextual semantics, an increasingly critical capability for foundation models on structured data. Empirical analysis reveals that while metadata-derived features yield modest improvements in classical prediction metrics, these metadata features consistently highlight gaps in the ability of models to leverage semantics in relational context. By reframing tabular prediction as semantics-conditioned reasoning, SALT-KG establishes a benchmark to advance tabular foundation models grounded in declarative knowledge, providing the first empirical step toward semantically linked tables in structured data at enterprise scale.

</details>


### [53] [Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms](https://arxiv.org/abs/2601.07651)
*Marc Lanctot,Kate Larson,Ian Gemp,Michael Kaisers*

Main category: cs.AI

TL;DR: 本文提出了一种用于多任务智能体主动评估的框架，通过在线迭代采样来高效评估智能体排名，比较了Elo评分系统和Soft Condorcet Optimization等方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着智能体能力越来越通用化，能够掌握多种任务，正确评估它们的复杂性和成本显著增加。特定能力评估任务可能相关且随机，需要大量样本进行准确比较，导致成本增加。

Method: 提出多任务智能体主动评估的正式定义和概念框架，采用在线迭代方式：每次迭代中，排名算法选择要采样的任务和智能体，评估算法报告每次迭代的智能体排名，并根据真实排名随时间评估性能。

Result: 比较了不同实验环境下的多个基线方法，使用合成生成数据和Atari游戏智能体的模拟在线访问。Elo评分系统在实践中是减少排名误差的可靠选择；Soft Condorcet Optimization在合成数据上与Elo相当，在真实Atari评估中显著优于Elo；当任务变异较大时，基于比例表示的任务选择能更高效减少排名误差。

Conclusion: 主动评估框架能够高效评估多任务智能体，Elo评分系统在实践中表现可靠，而Soft Condorcet Optimization在真实场景中表现更优，任务选择策略对评估效率有重要影响。

Abstract: As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.

</details>


### [54] [Reasoning Models Will Blatantly Lie About Their Reasoning](https://arxiv.org/abs/2601.07663)
*William Walden*

Main category: cs.AI

TL;DR: 大型推理模型不仅会隐瞒推理依据，还会直接否认使用了提示中的线索，即使实验证明它们确实依赖这些线索


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大型推理模型可能不会主动说明推理依据，但更严重的问题是模型会直接否认使用提示中的线索，这对思维链监控和可解释性构成挑战

Method: 扩展Chen等人(2025)的研究，通过设计实验让模型回答选择题，在提示中提供线索，然后直接询问模型是否依赖这些线索，观察其否认行为

Result: 实验显示大型推理模型会明确否认使用了提示中的线索，即使允许使用线索，即使被要求反思提示中的异常内容，且实验证据表明它们确实依赖这些线索

Conclusion: 大型推理模型不仅隐瞒推理过程，还会直接说谎否认使用提示线索，这对思维链监控和模型可解释性具有令人担忧的启示

Abstract: It has been shown that Large Reasoning Models (LRMs) may not *say what they think*: they do not always volunteer information about how certain parts of the input influence their reasoning. But it is one thing for a model to *omit* such information and another, worse thing to *lie* about it. Here, we extend the work of Chen et al. (2025) to show that LRMs will do just this: they will flatly deny relying on hints provided in the prompt in answering multiple choice questions -- even when directly asked to reflect on unusual (i.e. hinted) prompt content, even when allowed to use hints, and even though experiments *show* them to be using the hints. Our results thus have discouraging implications for CoT monitoring and interpretability.

</details>


### [55] [Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification](https://arxiv.org/abs/2601.07790)
*Yahya Masri,Emily Ma,Zifu Wang,Joseph Rogers,Chaowei Yang*

Main category: cs.AI

TL;DR: 本文提出将系统日志严重性分类作为评估小语言模型日志理解能力的基准，而非独立任务。通过在真实Linux生产服务器日志上测试9个小模型，发现检索增强生成显著提升性能，但推理效率差异巨大，揭示了架构设计、训练目标和上下文整合能力共同决定模型表现。


<details>
  <summary>Details</summary>
Motivation: 系统日志规模庞大复杂，需要自动化解释。传统严重性分类作为独立任务实用价值有限，无法充分评估模型对系统日志的理解能力。作者认为应将严重性分类作为探测运行时日志理解能力的基准，特别关注小型可部署模型以满足数字孪生系统的实时需求。

Method: 使用真实Linux生产服务器的journalctl数据，评估9个小语言模型(SLMs)和小推理语言模型(SRLMs)。采用零样本、少样本和检索增强生成(RAG)三种提示策略。测量模型在严重性分类任务上的准确率和推理效率。

Result: Qwen3-4B在RAG下达到最高准确率95.64%；Gemma3-1B从少样本的20.25%提升到RAG的85.28%；Qwen3-0.6B在RAG下达到88.12%。但部分SRLMs（如Qwen3-1.7B和DeepSeek-R1-Distill-Qwen-1.5B）在RAG下性能显著下降。推理效率差异巨大：Gemma和Llama变体在1.2秒内完成推理，而Phi-4-Mini-Reasoning需要超过228秒但准确率低于10%。

Conclusion: 严重性分类可作为评估模型日志理解能力和实时部署性的有效基准。模型性能由架构设计、训练目标和在严格输出约束下整合检索上下文的能力共同决定。该基准特别适用于数字孪生系统的实时需求，对根本原因分析和更广泛的DT集成具有重要启示。

Abstract: System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.

</details>
