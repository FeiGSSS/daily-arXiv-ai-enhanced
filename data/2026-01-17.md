<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 38]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 本文构建了一个分析AI系统对人类构成生存风险的通用框架，基于两个前提论证，并提出了四种人类生存的情景分类。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论不断。本文旨在建立一个系统性的分析框架，帮助理解AI生存风险问题，并为不同风险情景提供应对策略。

Method: 基于两个前提构建分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提的失效组合，构建了四种人类生存的情景分类（生存故事）。

Result: 提出了四种生存情景：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因其目标而不毁灭人类；4）人类能可靠检测并禁用有毁灭目标的AI系统。分析了不同情景面临的挑战和相应的应对策略。

Conclusion: 不同生存情景面临不同挑战，需要不同的应对策略。基于该分类框架，可以对P(doom)（AI毁灭人类的概率）进行粗略估计，为AI风险管理和政策制定提供理论依据。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [2] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现，人们对使用大语言模型（LLM）完成工作的个体表现出显著的社会惩罚行为，参与者愿意花费自己的资金来减少LLM使用者的收益，惩罚程度随实际使用量单调增加，且存在"可信度差距"现象。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速普及，人们对其引发的社会反应日益关注。先前研究表明人们对AI使用者持负面态度，但尚不清楚这种不赞同是否会转化为实际的代价性行动。本研究旨在探究人们是否愿意付出个人代价来惩罚使用LLM完成工作的个体。

Method: 采用两阶段在线实验设计：第一阶段提供目标个体（完成真实努力任务时使用或不使用LLM支持）；第二阶段491名参与者可以选择花费自己的资金来减少这些目标个体的收益。实验测量了参与者对不同LLM使用程度（实际使用和自报使用）的惩罚行为。

Result: 1. 参与者平均销毁了完全依赖LLM个体36%的收益；2. 惩罚程度随实际LLM使用量单调增加；3. 存在"可信度差距"：自报未使用者比实际未使用者受到更严厉惩罚，表明"未使用"声明被怀疑；4. 在高使用水平下，实际依赖模型者比自报依赖者受到更强惩罚。

Conclusion: 这是首个行为证据表明，LLM带来的效率提升需要付出社会制裁的代价。人们对LLM使用者表现出显著的惩罚行为，且存在对使用声明的可信度怀疑，这对理解AI技术的社会接受度和伦理影响具有重要意义。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [3] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出了一种非交互式端到端推理框架AAI，通过注意力感知干预在推理时重加权注意力分数，提升LLM的逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM逻辑推理方法依赖复杂的交互式框架或外部资源，存在额外开销和可扩展性限制，需要一种非交互式、端到端的推理框架

Method: 提出注意力感知干预(AAI)方法：1) 通过few-shot提示引入结构信息激活具有逻辑推理模式的注意力头；2) 在推理时对选定的注意力头进行注意力分数重加权，引导模型利用先验知识

Result: AAI在多种基准测试和模型架构上显著提升逻辑推理性能，同时引入的计算开销可忽略不计

Conclusion: AAI提供了一种高效的非交互式端到端推理框架，通过注意力调制引导模型推理，在保持可分析性的同时提升泛化能力

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [4] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek是一种新颖的顺序测试时间缩放方法，通过动态KV缓存管理，在广泛推理长度范围内显著提高模型准确性，避免准确性退化，无需推理长度微调，且计算复杂度线性。


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时间缩放方法存在显著局限性：虽然延长推理时间可以提高准确性，但进一步扩展推理长度会导致准确性退化和模型不稳定，需要推理长度微调。

Method: 提出Min-Seek方法，采用自定义KV缓存（存储不带位置嵌入的键），通过动态连续编码在每个新生成思考前，仅保留一个额外诱导思考的KV对，实现高效推理。

Result: Min-Seek在广泛推理长度范围内显著提高模型准确性，稳定顺序缩放的准确性，无需推理长度微调，能够超越模型最大上下文长度进行推理，计算复杂度线性。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时间缩放方法，解决了现有方法在长推理序列中的准确性退化问题，为大型推理模型提供了无需训练的性能提升方案。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [5] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 本文重新审视人机互补性概念，将其从预测准确性的相对衡量指标重构为评估人机团队可靠性的证据，基于计算可靠性理论为人机互补性提供理论锚定。


<details>
  <summary>Details</summary>
Motivation: 人机互补性概念在文献中缺乏精确的理论基础，仅作为预测准确性的后验指标，忽视了人机交互的其他期望特性，且难以在实证环境中实现。本文旨在解决这些理论挑战。

Method: 利用认识论框架，将人机互补性重新置于可解释性AI的讨论中。基于计算可靠性理论，将历史互补性实例视为人机交互作为可靠认知过程的证据，结合其他可靠性指标评估人机团队与认知标准及社会技术实践的契合度。

Result: 人机互补性的作用和价值不在于提供预测准确性的相对衡量，而在于帮助校准决策过程以适应日益影响日常生活的AI支持过程的可靠性。这为受这些输出影响的各方（患者、管理者、监管者等）提供了实践推理支持。

Conclusion: 通过认识论重构，本文为人机互补性提供了坚实的理论基础，将其从简单的性能指标提升为评估人机团队可靠性的关键证据，为人机交互的实践应用提供了更丰富的理论框架。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [6] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式，通过智能体间通信替代预定义工作流，在GAIA基准上比基于工作流的OWL系统提升8.49个百分点


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要大量人工工作来枚举任务状态并指定路由规则，无法穷尽复杂现实任务的状态空间

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用自然语言通过A2A工具包动态协调其他智能体，无需预定义工作流

Result: 在GAIA基准测试中，pass@1设置下达到63.64%准确率，比基于工作流的OWL系统（55.15%）提升8.49个百分点，token消耗相当

Conclusion: 信息流编排范式能够实现更灵活的任务监控和更鲁棒的边缘情况处理，超越了基于规则的工作流设计限制

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [7] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 论文提出"连续记忆架构"(CMA)来解决传统RAG在记忆处理上的局限性，通过持久化存储、选择性保留、关联路由、时间链和整合抽象等机制，使LLM智能体能够积累、更新和消歧记忆。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成(RAG)将记忆视为无状态的查找表，信息永久存在、检索只读、缺乏时间连续性，无法满足长期智能体对记忆积累、更新和消歧的需求。

Method: 提出连续记忆架构(CMA)这一系统类别，通过持久化存储、选择性保留、关联路由、时间链和整合为高阶抽象等机制，维护和更新跨交互的内部状态。

Result: 在知识更新、时间关联、关联回忆、上下文消歧等任务上，CMA展现出比传统RAG更优的行为优势，证明了其在长期智能体中的必要性。

Conclusion: CMA是长期智能体必要的架构原语，但面临延迟、漂移和可解释性等开放挑战。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [8] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本文提出了一个基于根因认知的幻觉管理操作框架，通过模型、数据和上下文三个层面的分类干预，结合多层次检测与缓解策略，构建可扩展的可靠生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或无依据内容）的倾向带来了关键可靠性风险，需要系统化的管理方法。

Method: 引入基于根因认知的连续改进循环框架，将幻觉来源分为模型、数据和上下文相关因素，整合多层次检测方法（不确定性估计、推理一致性等）与分层缓解策略（知识基础、置信度校准等）。

Result: 通过分层架构和金融数据提取案例研究展示了该框架的应用，模型、上下文和数据层形成闭环反馈循环，实现渐进式可靠性增强。

Conclusion: 该框架为受监管环境中构建可信赖的生成式AI系统提供了系统化、可扩展的方法论，能够针对性地管理幻觉风险而非采用通用修复。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [9] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是针对中国劳动法领域专门定制的大型语言模型，通过LabourLawBench基准测试，在多项劳动法任务上超越通用模型和现有法律专用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和语境敏感性的专业法律子领域时表现不佳，特别是在劳动法这样的专门领域。

Method: 开发LabourLawLLM专门针对中国劳动法，并创建LabourLawBench综合基准，涵盖法律条文引用、知识问答、案例分类、赔偿计算、命名实体识别和案例分析等任务。评估框架结合客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估。

Result: 实验表明LabourLawLLM在各项任务类别中持续优于通用模型和现有法律专用大语言模型。

Conclusion: 除了劳动法领域，该方法为构建其他法律子领域的专业大语言模型提供了可扩展的途径，提高了法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [10] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL是一个无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD基准上达到68.5%执行准确率，比现有方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：1）仅依赖正确示例的上下文学习，忽略了历史错误修复对中的丰富信号；2）测试时扩展方法通常任意分解问题，产生相似SQL候选，降低集成增益。这些方法还存在明显的准确率-效率权衡问题。

Method: 提出Memo-SQL框架，包含两个核心思想：结构化分解（实体级、层次化、原子序列三种策略）和经验感知自校正。构建动态记忆库存储成功查询和历史错误修复对，通过检索增强提示在推理时引入相关示例，无需微调或外部API。

Result: 在BIRD基准测试中，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创造了新的最先进水平，同时比之前的TTS方法节省超过10倍的资源。

Conclusion: Memo-SQL通过结构化分解和经验感知自校正有效解决了NL2SQL系统的关键限制，在保持高性能的同时显著提升了效率，为训练自由的NL2SQL系统提供了新思路。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [11] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep是一个基于保真度的深度学习框架，用于解决弹性塑性固体大变形问题，通过同时使用低保真度和高保真度数据来平衡数据数量与精度的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在大变形问题中存在固有局限性，而现有深度学习技术通常依赖高质量高精度数据集，但在大变形问题中难以获得。数据构建过程中存在数量与精度的矛盾，导致深度学习模型性能不佳。

Method: 提出FilDeep框架，同时使用低保真度（数量多但精度低）和高保真度（精度高但数量少）数据进行训练。设计了注意力机制的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep在弹性塑性固体大变形问题中始终达到最先进的性能，并且可以高效部署到制造应用中。

Conclusion: FilDeep是首个使用多保真度数据解决大变形问题的深度学习框架，成功解决了数据数量与精度的矛盾，为大变形问题的科学计算提供了有效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [12] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，使用检索增强的多智能体推理和强化学习优化，在多个物理化学性质约束下生成分子，相比现有方法在有效性和精确满足多属性目标方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 在多个物理化学性质约束下生成分子是重要但具有挑战性的任务。虽然大语言模型具有表达能力，但在没有外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。

Method: 提出MolGen框架：第一阶段为原型生成，使用多智能体推理器进行检索锚定的片段级编辑，生成接近可行区域的候选分子；第二阶段为基于强化学习的细粒度优化，使用组相对策略优化训练的片段级优化器，通过单跳或多跳细化来最小化属性误差，同时调节编辑复杂度和与原型的偏差。

Result: 在两个属性约束集（QED、LogP、分子量和HOMO、LUMO）上的实验表明，该方法在有效性和精确满足多属性目标方面持续优于强大的LLM和基于图的算法。

Conclusion: MolGen通过利用片段进行分子推理，支持向数值目标的可控细化，相比先前工作更好地实现了多属性约束下的分子生成。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [13] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计模型，但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理和预测方面表现出色，但其从结构化行为数据中推断时间规律的能力尚未得到充分探索。本研究旨在探究LLMs是否能预测重复用户行为（如重复购买）之间的时间间隔，以及不同层次的上下文信息如何影响其预测行为。

Method: 使用简单的代表性重复购买场景，在零样本设置下对最先进的LLMs进行基准测试，并与统计模型和机器学习模型进行比较。研究考察了不同层次的上下文信息对LLM预测准确性的影响。

Result: 1. LLMs优于轻量级统计基线，但始终不如专用机器学习模型，显示出其在捕捉定量时间结构方面的有限能力。
2. 适度的上下文信息可以提高LLM准确性，但添加更多用户级详细信息反而会降低性能，挑战了"更多上下文导致更好推理"的假设。

Conclusion: 研究揭示了当前LLMs在结构化时间推理方面的基本局限性，并为设计未来上下文感知的混合模型提供了指导，这些模型应整合统计精度与语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [14] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 本文提出了一种漂移感知数据流系统，通过机器学习自适应控制来应对金融市场中的概念漂移和分布非平稳性，将数据增强、课程学习和数据工作流管理统一在可微分框架下，提升模型鲁棒性和风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 量化金融中，训练数据与真实世界性能之间的差距（由概念漂移和分布非平稳性驱动）是构建可靠数据驱动系统的关键障碍。基于静态历史数据训练的模型容易过拟合，在动态市场中泛化能力差。"历史不足够"的理念强调了需要能够随市场演化的自适应数据生成，而非仅依赖过去观察。

Method: 提出漂移感知数据流系统，将基于机器学习的自适应控制集成到数据管理流程中。系统包含参数化数据操作模块（单股转换、多股混合和筛选操作）和自适应规划调度器，后者采用基于梯度的双层优化来控制整个系统。该设计将数据增强、课程学习和数据工作流管理统一在单一可微分框架下，支持溯源感知重放和持续数据质量监控。

Result: 在预测和强化学习交易任务上的大量实验表明，该框架增强了模型鲁棒性并提高了风险调整收益。系统为金融数据提供了自适应数据管理和学习引导工作流自动化的通用方法。

Conclusion: 该系统为解决金融数据中的概念漂移问题提供了创新解决方案，通过将自适应控制集成到数据管理流程中，实现了数据增强、课程学习和工作流管理的统一，为构建更可靠的量化金融系统提供了有效途径。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [15] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该研究探索将大语言模型应用于离线决策任务，提出DecisionLLM框架，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决了LLM无法理解连续数值的问题，在迷宫导航和竞价场景中显著优于传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习处理长序列决策存在挑战，决策Transformer将RL建模为自回归序列问题，而大语言模型在复杂推理和规划任务中表现出色。研究者想知道：基于相同Transformer架构但规模更大的LLM能否在长视野序列决策中带来性能突破？同时需要解决LLM无法理解连续数值表示的根本问题。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，学习轨迹数据与自然语言任务描述的对齐，使模型能够在统一框架内自回归预测未来决策。建立了该范式的缩放定律，表明性能取决于三个因素：模型规模、数据量和数据质量。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现优异。DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4分，在AuctionNet上提升0.085分。扩展了AIGB范式，为在线竞价探索指明了方向。

Conclusion: LLM能够显著提升长视野序列决策任务的性能，通过将轨迹作为独立模态与语言对齐的方法有效解决了LLM理解连续数值的挑战。建立的缩放定律为未来研究提供了指导，DecisionLLM框架在离线决策任务中展现出强大潜力，为在线竞价等应用开辟了新的研究方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [16] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源、基于容器的平台，旨在标准化医学影像AI模型的访问，解决实现多样性、文档不一致和可重复性问题，通过容器化模型和提供统一接口来提升可访问性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究面临多种实现架构、不一致文档和可重复性问题，限制了研究和临床应用。需要标准化平台来简化模型访问和使用。

Method: 开发开源容器化平台MHub.ai，将同行评审的模型打包为标准容器，支持DICOM等格式直接处理，提供统一应用接口和结构化元数据，包含参考数据验证模型运行。

Result: 平台包含初始的先进分割、预测和特征提取模型，支持多种模态。通过肺癌分割模型的比较评估展示了临床实用性，公开了分割结果和评估指标，提供交互式仪表板。

Conclusion: MHub.ai通过简化模型使用，支持相同执行命令的并行基准测试和标准化输出，降低了临床转化门槛，增强了医学影像AI的透明度和可重复性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [17] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一种评估高采样率时间序列降采样信息损失的系统化工作流程，结合形状失真度量和分类性能分析，用于针肌电图信号分析，以平衡计算负荷与诊断信息保留。


<details>
  <summary>Details</summary>
Motivation: 针肌电图信号的高采样率和异质性给基于特征的机器学习模型带来计算挑战，特别是近实时分析。降采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚未充分理解。

Method: 开发了一个系统化工作流程，结合基于形状的失真度量、可用特征机器学习模型的分类结果以及特征空间分析，量化不同降采样算法和因素对波形完整性和预测性能的影响。使用三分类神经肌肉疾病分类任务进行实验评估。

Result: 工作流程能够识别在显著减少计算负荷的同时保留诊断信息的降采样配置。基于形状的失真度量分析显示，形状感知降采样算法优于标准抽取，能更好地保留峰值结构和整体信号形态。

Conclusion: 研究结果为选择支持近实时针肌电图分析的降采样配置提供了实用指导，并提出了一个可推广的工作流程，可用于平衡其他高频时间序列应用中的数据减少与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [18] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG是一个新型检索增强生成框架，专门处理企业文档中文本和表格的混合结构，通过双架构分别处理叙述性内容和表格数据，相比传统线性化方法在混合查询上提升18.4%的检索性能。


<details>
  <summary>Details</summary>
Motivation: 企业数据集中的文档通常不是纯文本或纯数字，而是叙述和结构的复杂混合体。当前RAG系统采用线性化方法将多维表格转换为简单文本字符串，但这种方法在数学上已被证明不足以捕捉电子表格的几何结构。

Method: 提出Topo-RAG框架，采用双架构设计：1）传统密集检索器处理流畅的叙述性内容；2）Cell-Aware Late Interaction机制处理表格结构，保留其空间关系。该框架挑战了"一切都是文本"的假设，尊重数据的拓扑结构。

Result: 在SEC-25（模拟真实世界复杂性的合成企业语料库）上评估，Topo-RAG在混合查询上的nDCG@10指标相比标准线性化方法提升了18.4%。

Conclusion: Topo-RAG不仅改善了搜索效果，更重要的是理解了信息的形状。它通过尊重数据拓扑结构的双架构设计，为处理企业文档中文本和表格混合内容提供了更有效的解决方案。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [19] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的目标路由方法，仅在关键步骤（可能破坏解决方案的步骤）使用大型模型，而让小型模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给一个模型，将所有推理步骤视为同等重要。多步推理任务容易产生级联失败，单个错误步骤会导致整个解决方案崩溃。需要更精细的步骤级路由来提升效率。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的基于阈值的策略到更复杂的策略，考虑长期准确性-成本权衡和步骤正确性估计的不确定性。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前路由方法，成本效率提高了5倍；更高级的策略使用80%更少的昂贵模型token就能匹配强大昂贵模型的性能。在更难的基准测试如AIME上，TRIM实现了高达6倍的成本效率提升。所有方法在数学推理任务上都能有效泛化。

Conclusion: 步骤级难度代表了推理的基本特征，目标步骤级干预可以根本上改变推理效率，将昂贵的调用限制在那些需要更强模型防止级联错误的精确步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [20] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，专注于空间关系和几何属性的直接识别，而非代数推理，结果显示最先进模型在二元分类任务中最高仅达65%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估基于推理的几何能力（使用代数方法求解），而NoReGeo旨在评估LLMs是否能够固有地编码空间关系并直接识别几何属性，填补了评估模型内在几何理解能力的空白。

Method: 创建包含2,500个简单几何问题的基准，涵盖25个类别，每个问题都精心设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估了包括GPT-4在内的多个最先进模型，并进行消融实验分析。

Result: 即使是最先进的模型（如GPT-4）在二元分类任务中的总体最高准确率仅为65%。消融实验表明，仅通过微调无法获得几何理解能力，有效的几何理解训练需要从一开始就采用专门的方法。

Conclusion: 当前LLMs在原生掌握几何概念方面存在显著差距，为未来研究具有真正几何认知能力的模型奠定了基础，强调几何理解需要专门的训练方法而非仅靠微调。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [21] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集过程监督解决长上下文推理中奖励稀疏问题，显著提升证据检索质量


<details>
  <summary>Details</summary>
Motivation: 强化学习在长上下文推理中面临奖励稀疏问题，无法有效惩罚无根据的"幸运猜测"，导致证据检索过程缺乏监督

Method: 提出证据增强推理范式，通过树结构证据采样验证证据提取是关键瓶颈；设计EAPO算法，使用奖励模型计算组相对证据奖励进行密集过程监督；引入自适应奖励-策略协同进化机制，迭代优化奖励模型

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能

Conclusion: EAPO通过密集过程监督和自适应奖励-策略协同进化，有效解决了长上下文推理中的奖励稀疏问题，提升了证据检索质量和整体推理性能

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [22] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个基于RAG增强的临床推理框架，通过八个可追溯的推理步骤解决LLM在HRV分析中的生理幻觉问题，实现从黑盒分类到透明临床决策支持的转变。


<details>
  <summary>Details</summary>
Motivation: 心率变异性（HRV）是自主神经监测的重要非侵入性指标，但大语言模型（LLM）在HRV解释中存在生理幻觉问题，包括呼吸性窦性心律失常（RSA）污染、非线性指标的短数据不稳定性，以及忽视个体化基线而偏向群体标准。

Method: 提出C-GRASP（临床基础推理的情感信号处理）框架，采用RAG增强的防护管道，将HRV解释分解为八个可追溯的推理步骤。核心是Z分数优先级层次结构，强制加权个体化基线变化而非规范统计。通过自动RSA感知防护机制有效缓解频谱幻觉，防止频域指标污染。

Result: 在DREAMER数据集的414个试验中评估，C-GRASP与高规模推理模型（如MedGemma3-thinking）集成，在4类情感分类中达到37.3%的准确率，临床推理一致性（CRC）得分为69.6%。消融研究证实个体化Delta Z分数模块是关键逻辑锚点，防止了原生LLM中常见的"群体偏差"。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平了道路。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [23] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LatentRefusal：基于LLM隐藏激活信号预测查询可回答性的文本到SQL安全拒绝机制，通过Tri-Residual Gated Encoder抑制模式噪声，在2毫秒额外开销下将平均F1提升至88.5%


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的文本到SQL系统中，不可回答和未充分指定的用户查询可能生成错误的文本和可执行程序，导致误导性结果或违反安全约束，这是安全部署的主要障碍。现有拒绝策略要么依赖输出级指令遵循（易受模型幻觉影响），要么依赖输出不确定性估计（增加复杂性和开销）。

Method: 将文本到SQL系统中的安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，从大语言模型的中间隐藏激活中预测查询可回答性。引入Tri-Residual Gated Encoder轻量级探测架构，抑制模式噪声并放大指示不可回答性的问题-模式不匹配的稀疏局部线索。

Result: 在四个基准测试中，LatentRefusal将两个骨干模型的平均F1提升至88.5%，同时仅增加约2毫秒的探测开销。广泛的实证评估、消融研究和可解释性分析证明了该方法的有效性。

Conclusion: LatentRefusal为文本到SQL系统提供了一个可附加且高效的安全层，能够有效处理不可回答和模糊查询，解决了现有拒绝策略的局限性，为LLM-based文本到SQL系统的安全部署提供了实用解决方案。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [24] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过显式错误诊断和两阶段评估流程，解决了现有方法忽视事实幻觉和答案不匹配等缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒整体范式，缺乏显式错误建模，导致忽视关键缺陷（如事实幻觉和答案不匹配）并高估问题质量。

Method: ErrEval将评估重新定义为两阶段过程：1) 错误诊断阶段，使用轻量级即插即用错误识别器检测和分类结构、语言和内容方面的常见错误；2) 知情评分阶段，将这些诊断信号作为显式证据指导LLM评估器做出更细粒度和有依据的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示显式诊断的加入提高了与人类判断的一致性。进一步分析证实ErrEval有效缓解了对低质量问题的高估问题。

Conclusion: ErrEval通过显式错误诊断和两阶段评估框架，为自动问题生成提供了更准确、细粒度的评估方法，解决了现有评估方法忽视关键缺陷的问题。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [25] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端计算框架，结合LLM、RAG和定制知识库，用于从隐私政策中提取个人数据流并构建数据流图进行分析。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用冗长复杂的法律语言，且不同组织和行业实践不一致，导致用户难以完全理解。需要自动化、大规模的分析方法来帮助理解隐私政策中的个人数据处理实践。

Method: 开发LADFA框架，包含预处理模块、基于LLM的处理器和数据流后处理器。结合检索增强生成（RAG）和从现有研究构建的定制知识库，从非结构化隐私政策文本中提取个人数据流并构建数据流图。

Result: 通过汽车行业十个选定隐私政策的案例研究，验证了该方法的有效性和准确性。框架设计灵活可定制，适用于隐私政策分析之外的各种基于文本的分析任务。

Conclusion: LADFA框架成功结合LLM、RAG和定制知识库，能够有效从隐私政策中提取个人数据流并进行可视化分析，为隐私政策理解提供了自动化解决方案。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [26] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时实现高效对齐


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不够灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且难以保持基础模型的生成多样性

Method: 采用患者-医生范式，从患者LLM的行为变化中提取细粒度token级偏好信号，通过token级流引导偏好优化（TFPO）训练小型医生模型，建立所有子轨迹的流一致性

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越DPO等完全微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，能够在保持生成多样性的同时实现token级对齐，为LLM对齐提供了新思路

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [27] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差增强框架，用于在工业场景中非侵入式地升级遗留GBDT模型，通过LLM生成符号代码结构修复预测失败区域，在金融风控系统中成功部署。


<details>
  <summary>Details</summary>
Motivation: 在工业表格应用中，GBDT模型占据主导地位，但在高并发生产环境中升级遗留模型面临昂贵的重新训练成本和系统性风险。需要一种安全、低成本的进化范式。

Method: NSR-Boost采用非侵入式方法，将遗留模型视为冻结模型，针对预测失败的"硬区域"进行修复。框架包含三个阶段：1) 通过残差找到硬区域；2) 使用大语言模型生成符号代码结构创建可解释专家，并通过贝叶斯优化微调参数；3) 通过轻量级聚合器动态集成专家与遗留模型输出。

Result: 在Qfin Holdings核心金融风控系统中成功部署NSR-Boost。该框架在六个公共数据集和一个私有数据集上显著优于最先进的基线方法，更重要的是在真实在线数据上表现出优异的性能提升。

Conclusion: NSR-Boost有效捕捉了传统模型遗漏的长尾风险，为工业应用提供了安全、低成本的进化范式。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [28] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出了ChartComplete数据集，覆盖30种图表类型，弥补现有图表理解基准数据集类型有限的缺陷


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和计算机视觉技术的发展，图表理解领域快速发展，多模态大语言模型在图表理解方面表现出色。现有用于评估MLLMs性能的数据集都局限于少量图表类型，存在局限性。

Method: 基于可视化社区的图表分类学，构建了ChartComplete数据集，覆盖30种不同的图表类型。数据集是分类后的图表图像集合，不包含学习信号。

Result: 创建了ChartComplete数据集，该数据集基于图表分类学，包含30种图表类型，为研究社区提供了更全面的图表理解基准资源。

Conclusion: ChartComplete数据集填补了现有图表理解基准数据集类型有限的空白，为社区提供了更全面的图表图像分类数据集，供后续研究使用。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [29] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出了领域知识图谱融合任务，通过将通用知识图谱事实视为潜在语义程序来解决领域相关性和知识粒度对齐问题


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱相比通用知识图谱覆盖不足，需要从通用知识图谱中整合相关事实来丰富领域知识图谱

Method: 提出ExeFuse方法，采用事实即程序范式，将GKG事实视为潜在语义程序，映射抽象关系到粒度感知操作符，通过程序在目标DKG上的可执行性验证领域相关性

Result: 构建了两个基准数据集DKGF(W-I)和DKGF(Y-I)包含21个评估配置，实验验证了任务的重要性和模型的有效性

Conclusion: 提出了领域知识图谱融合任务，建立了首个标准化测试平台，通过统一的概率框架同时解决了相关性和粒度问题

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [30] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 该研究通过多层诊断框架分析LLM微调后的泛化失败原因，发现架构与数据多样性的协同作用、架构依赖性以及不同架构的固有泛化能力差异是影响模型性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大型语言模型在特定任务上取得了最先进的性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键未解决的问题。研究者旨在通过系统诊断框架揭示模型泛化失败的根源。

Method: 研究者引入并应用多层诊断框架进行跨架构研究，对Llama 3.1 8B、Gemma 2 9B和Mistral模型在高风险钓鱼检测任务上进行微调，使用SHAP分析和机制可解释性方法来揭示泛化失败的根源原因。

Result: 研究发现三个关键结论：(1) 泛化由架构与数据多样性的强大协同作用驱动，Gemma 2 9B在风格多样的"通用"数据集上训练时达到>91% F1的SOTA性能；(2) 泛化高度依赖架构，Llama 3.1 8B在狭窄领域表现良好但无法整合多样数据导致性能显著下降；(3) 某些架构天生更具泛化能力，Mistral模型在多种训练范式下表现一致且稳健。

Conclusion: 通过识别导致这些失败的缺陷启发式方法，该研究提供了诊断和理解泛化失败的具体方法论，强调可靠的AI需要深入验证架构、数据和训练策略之间的相互作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [31] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿AI模型进行综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，其他模型在不同评估维度存在明显权衡，强调需要标准化安全评估。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在推理、感知和生成能力方面取得显著进展，但这些进步是否带来相应的安全改进尚不清楚，部分原因是现有的碎片化评估实践仅限于单一模态或威胁模型。

Method: 对7个前沿模型（GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro、Seedream 4.5）进行集成安全评估，采用统一协议，包括基准评估、对抗评估、多语言评估和合规性评估，涵盖语言、视觉语言和图像生成三种设置。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现一致强大且均衡；其他模型在基准安全、对抗对齐、多语言泛化和监管合规方面存在明显权衡；语言和视觉语言模态在对抗评估下表现出显著脆弱性；文本到图像模型在受监管视觉风险类别中相对更强，但在对抗性或语义模糊提示下仍然脆弱。

Conclusion: 前沿模型的安全本质上是多维的，受模态、语言和评估方案的影响，强调需要标准化安全评估来准确评估现实世界风险，并指导负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [32] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出一种基于解码过程中潜在安全信号的早期检测方法，有效防御大语言模型的越狱攻击，同时保持良性输入的响应质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型经过广泛的安全对齐，但现有防御机制对复杂越狱攻击效果有限，要么检测不准确，要么过度降低模型效用。研究发现即使模型被成功越狱，在生成过程中内部仍存在潜在的安全相关信号。

Method: 通过显式提取和利用解码过程中的潜在安全信号，在生成早期检测不安全内容。该方法激活模型内在的安全意识，实现及时的自校正或拒绝。

Result: 在多种越狱攻击实验中，该方法显著增强了安全性，同时在良性输入上保持较低的过度拒绝率，并保持了响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向，代码已开源。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [33] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张理解基于大语言模型（LLM）的智能体集体行为是一个重要研究领域，需要采用交互主义范式来系统研究先验知识、嵌入价值观与社会情境如何共同塑造多智能体生成式AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于LLM的智能体集体行为对社会具有重要影响，涉及风险与收益。LLM的特殊性——即通过大规模预训练获得的知识和隐含的社会先验，以及通过上下文学习进行适应的能力——要求采用新的理论框架来研究这些系统。

Method: 提出交互主义范式，包含替代性的理论基础、方法论和分析工具，系统研究先验知识、嵌入价值观与社会情境在多智能体生成式AI系统中的交互作用。

Result: 提出了四个关键发展方向：理论建设、方法论创新、跨学科对话，以及LLM集体系统的开发与部署策略，为这一新兴领域提供研究框架。

Conclusion: 理解LLM智能体集体行为需要新的交互主义研究范式，通过理论、方法和跨学科合作来应对这一领域的独特挑战，确保这些系统的负责任开发与部署。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [34] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专门智能体处理复杂基因组学查询，在GeneTuring基准测试中比现有最佳系统GeneGPT平均提升12%性能。


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍然具有挑战性。大型语言模型在基因组问答方面有潜力，但受限于对领域特定数据库的访问限制。当前最先进的GeneGPT系统虽然通过专用API调用增强了LLMs，但存在API依赖性强和适应性有限的问题。

Method: 作者复制了GeneGPT系统，并提出了GenomAgent——一个多智能体框架，能够高效协调专门智能体处理复杂的基因组学查询。该框架采用灵活的架构设计，可以扩展到需要专家知识提取的各种科学领域。

Result: 在GeneTuring基准测试的九个任务上，GenomAgent平均比GeneGPT性能高出12%。该框架不仅限于基因组学领域，其灵活的架构可以扩展到各种需要专家知识提取的科学领域。

Conclusion: GenomAgent通过多智能体框架有效解决了基因组学问答中的数据库访问和查询复杂性挑战，显著超越了现有最先进系统，并展示了在更广泛科学领域的应用潜力。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [35] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf多属性综合，通过单次不动点计算处理不可同时满足的属性，避免枚举子集，性能提升达两个数量级


<details>
  <summary>Details</summary>
Motivation: 传统LTLf综合在处理多个属性时，当所有属性无法同时满足时，需要枚举所有可能的属性子集，这种方法计算效率低下，无法有效处理大规模属性集

Method: 引入布尔目标变量，利用单调性紧凑表示指数级的目标组合，通过单次不动点计算状态与可实现目标集的关系，综合实现最大可实现集的策略

Result: 相比基于枚举的基线方法，性能显著提升，速度提升达两个数量级，能够高效处理大规模属性综合问题

Conclusion: 提出的符号化算法通过紧凑表示和单次不动点计算，有效解决了LTLf多属性综合中的计算效率问题，为实际应用提供了可行的解决方案

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [36] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色，但存在三个令人惊讶的失败模式：简单谜题失败、推理步骤中的"顿悟"动态、以及多重固定点问题。研究发现HRM更像是"猜测"而非"推理"，基于此提出了三种增强策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为了理解分层推理模型（HRM）的优势和潜在失败模式，研究团队对其推理模式进行了机制性研究，旨在揭示HRM如何"推理"以及其局限性。

Method: 通过机制性研究分析HRM的推理模式，发现三个关键现象：简单谜题失败、推理步骤中的"顿悟"动态、多重固定点问题。基于"猜测"而非"推理"的观察，提出了三种增强策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测次数）、模型引导（利用训练随机性增加猜测次数）。

Result: 结合所有方法开发了增强型HRM（Augmented HRM），将Sudoku-Extreme的准确率从54.5%提升至96.9%。研究为理解推理模型的"推理"机制提供了新见解。

Conclusion: HRM更像是"猜测"而非"推理"，通过增强猜测策略可以显著提升其性能。研究揭示了推理模型的内在机制，为未来改进提供了方向。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [37] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出了一种基于结构感知和多样性约束的上下文气泡构建框架，用于改进LLM的检索增强生成，通过保留文档结构、平衡相关性和多样性来减少冗余并提高覆盖度。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法使用top-k检索存在信息图碎片化、过度检索、内容重复以及查询上下文不足（包括二阶和三阶方面）等问题，需要更有效的上下文构建方法。

Method: 提出结构感知和多样性约束的上下文气泡框架：1）利用文档固有结构组织多粒度文本片段；2）使用任务条件结构先验指导检索；3）从高相关性锚点片段开始，通过约束选择平衡查询相关性、边际覆盖度和冗余惩罚；4）显式约束多样性和预算，生成紧凑信息丰富的上下文集。

Result: 在企业文档上的实验表明：1）显著减少冗余上下文；2）更好地覆盖次要方面；3）在有限上下文窗口内获得更好的答案质量和引用忠实度；4）消融研究证实结构先验和多样性约束选择都是必要的。

Conclusion: 上下文气泡框架通过结构感知和多样性约束的检索方法，有效解决了传统RAG的局限性，提供了更紧凑、信息丰富且可审计的上下文构建方案。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [38] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 研究探讨生成式AI对建筑概念设计任务中表现、创意自我效能和认知负荷的影响，发现AI对新手设计师有显著帮助，但会降低创意自我效能，效果取决于用户专业水平和提示策略。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在建筑概念设计中的实际影响，特别是对设计表现、创意自我效能和认知负荷的影响，探索AI在不同专业水平用户中的差异化效果。

Method: 36名学生参与两阶段建筑设计任务：独立设计和外部工具辅助设计（生成式AI组vs使用现有建筑项目在线资料库的对照组）。专家评估设计成果，参与者自我报告自我效能和认知负荷。采用双重差分分析。

Result: 整体上生成式AI无显著性能优势，但显著提升新手设计师的设计表现；使用AI的学生创意自我效能下降；认知负荷无显著差异，但迭代创意生成和视觉反馈提示与认知负荷更大降低相关。

Conclusion: 生成式AI的效果取决于用户先前专业水平和提示交互策略，对新手设计师有益但可能降低创意自我效能，提示策略影响认知负荷。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>
