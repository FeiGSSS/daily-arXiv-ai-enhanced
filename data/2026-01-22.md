<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 67]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: 研究人员创建了MIMIC-RD基准测试来评估LLM在罕见病鉴别诊断中的表现，发现当前最先进的LLM表现不佳，揭示了现有能力与临床需求之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响十分之一的美国人，但其鉴别诊断仍然具有挑战性。现有评估LLM在罕见病诊断中的方法存在两个关键局限性：依赖理想化的临床案例研究，或使用ICD代码作为疾病标签，这显著低估了罕见病数量，因为许多罕见病缺乏与Orphanet等综合罕见病数据库的直接映射。

Method: 开发了MIMIC-RD基准测试，通过将临床文本实体直接映射到Orphanet数据库来构建罕见病鉴别诊断基准。方法包括初始的LLM挖掘过程，然后由四位医学注释员验证，确认识别的实体是真正的罕见病。在145名患者的数据集上评估了各种模型。

Result: 当前最先进的大型语言模型在罕见病鉴别诊断方面表现不佳，突显了现有能力与临床需求之间的巨大差距。

Conclusion: 需要改进罕见病的鉴别诊断方法，论文概述了几个未来改进方向。

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [2] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 论文认为机器意识不仅取决于计算内容，还取决于计算时机。顺序计算系统无法实现意识所需的同步性，需要并发硬件支持。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统大多采用顺序或时分复用更新，而意识体验呈现统一性和同时性。这种时间结构差异对机器能否具有意识有重要影响。

Method: 扩展栈理论，引入代数定律将时间窗口内的约束满足与合取关联。定义精确的时间语义τ^{Δ,s}，证明存在性时间实现◇_Δ不保持合取。区分StrongSync（要求合取在窗口内客观共现）和WeakSync（允许时间"模糊"）两种假设。

Result: 系统可以在时间上实现所有体验成分，但从未实例化体验合取本身。神经生理学证据表明意识依赖相位同步和有效连接，失去意识常伴随其崩溃，这使WeakSync不太可能成立。

Conclusion: 在StrongSync假设下，严格顺序基板上的软件意识对于需要两个或更多同时贡献者的内容是不可能的。所需同时贡献部分越多，需要的并发容量越大。硬件很重要，意识归因需要架构检查而不仅仅是功能性能。

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [3] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: 该研究将神经科学中的时间整合和亚稳态概念应用于Transformer模型，提出了一种复合动力学指标来量化LLM在文本生成过程中的内部动态组织，发现结构化推理相比重复、噪声和扰动条件表现出显著更高的动力学复杂度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过高维内部动态进行文本生成，但这些动态的时间组织机制仍不明确。现有可解释性方法主要关注静态表示或因果干预，忽视了时间结构。受神经科学中时间整合和亚稳态作为神经组织核心标志的启发，研究旨在将这些概念应用于Transformer模型，探索LLM内部动态的时间组织特征。

Method: 研究将神经科学中的时间整合和亚稳态概念适配到Transformer模型，提出了一种基于自回归生成过程中激活时间序列计算的复合动力学指标。在GPT-2-medium模型上评估了五种条件：结构化推理、强制重复、高温噪声采样、注意力头剪枝和权重噪声注入。使用单因素方差分析和效应大小进行统计检验，并对层选择、通道子采样和随机种子进行了鲁棒性验证。

Result: 结构化推理条件相比重复、噪声和扰动条件表现出显著更高的动力学指标值。统计检验显示组间存在显著差异（单因素方差分析），关键比较中效应大小较大。结果对层选择、通道子采样和随机种子具有鲁棒性。这表明神经科学启发的动力学指标能够可靠地表征大型语言模型在不同功能机制下的计算组织差异。

Conclusion: 神经科学启发的动力学指标能够有效捕捉大型语言模型内部动态的时间组织特征，结构化推理过程展现出更复杂的动力学模式。该指标捕获的是形式化的动力学特性，而非主观体验。这一方法为理解LLM的计算组织提供了新的视角，将时间维度纳入模型可解释性分析框架。

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [4] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文提出了一种训练时解释性方法，通过跟踪微调过程中token级归因的变化来监控模型决策证据的演变，并引入了"推理稳定点"概念。


<details>
  <summary>Details</summary>
Motivation: 微调预训练语言模型可能会微妙地改变模型所依赖的证据，需要一种方法来监控决策证据在微调过程中的演变。

Method: 提出训练时解释性视图，跟踪微调各epoch中token级归因的变化，定义"解释漂移"为固定探测集上归一化token归因的epoch间变化，并引入"推理稳定点"作为漂移首次持续保持低水平的最早epoch。

Result: 在多个轻量级transformer分类器和基准分类任务中，漂移通常在训练早期就进入低稳定状态，而验证准确率仅发生微小变化。在受控的捷径设置中，归因动态揭示了模型对捷径的依赖增加，即使验证准确率保持竞争力。

Conclusion: 解释漂移提供了一种简单、低成本的诊断工具，用于监控微调过程中决策证据的演变，并选择处于稳定证据状态的检查点。

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [5] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: POLARIS是一个面向企业后台工作流的治理型LLM智能体编排框架，通过类型化计划合成和验证执行，确保自动化流程的可审计性、策略对齐和操作可预测性。


<details>
  <summary>Details</summary>
Motivation: 企业后台工作流需要可审计、策略对齐且操作可预测的智能体系统，而通用的多智能体设置往往无法满足这些要求，因此需要开发专门的治理型编排框架。

Method: POLARIS框架包含三个核心组件：1）规划器生成类型检查的有向无环图；2）基于评分标准的推理模块选择合规计划；3）执行阶段通过验证器门控检查、有限修复循环和编译策略护栏来防止副作用。

Result: 在文档中心化财务任务中，POLARIS能够生成决策级工件和完整执行轨迹，减少人工干预。在SROIE数据集上达到0.81的微F1分数，在受控合成套件中实现0.95-1.00的异常路由精度，同时保持审计轨迹。

Conclusion: POLARIS为策略对齐的智能体AI提供了方法论和基准参考，建立了治理型智能体AI的初步基准，展示了在企业自动化中实现可审计、策略对齐工作流的可行性。

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [6] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: 开发了一个基于PICOS框架的AI协同科学家平台，用于可扩展、透明的知识合成，通过自动化PICOS合规性检测、研究设计分类和检索增强生成来减少生物医学研究浪费


<details>
  <summary>Details</summary>
Motivation: 生物医学研究中存在研究浪费问题，包括冗余研究、不完整报告和传统证据合成工作流程的可扩展性有限。需要开发能够提高证据合成可扩展性、透明度和效率的AI工具

Method: 开发了基于PICOS框架的AI协同科学家平台，整合关系存储、向量语义检索和Neo4j知识图谱。使用Bi-LSTM和基于PubMedBERT的transformer多任务分类器进行PICOS合规性和研究设计分类。采用检索增强生成进行全文合成，结合向量和图检索，使用BERTopic进行主题建模

Result: transformer模型在研究设计分类上达到95.7%准确率，Bi-LSTM在PICOS合规性检测上达到87%准确率。检索增强生成在需要结构化约束、跨研究整合和图推理的查询上优于非检索生成，而非检索方法在高级摘要上仍有竞争力。主题建模揭示了大量主题冗余和未充分探索的研究领域

Conclusion: PICOS感知和可解释的自然语言处理能够提高证据合成的可扩展性、透明度和效率。该架构是领域无关的，为减少跨生物医学学科的研究浪费提供了实用框架

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [7] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究者开发了ITA-GPT工具支持归纳主题分析，通过人机协作框架研究AI如何影响质性研究的分析过程和解释权威。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在质性研究中的应用增加，需要探讨AI工具如何影响分析实践和解释权威，理解人机协作在归纳主题分析中的实际作用。

Method: 基于HACITA人机协作框架，三位经验丰富的质性研究者使用ITA-GPT工具分析加纳教师教育访谈转录本，收集交互日志、AI生成表格、研究者修订记录等数据。

Result: ITA-GPT作为程序性支架结构化分析流程并增强透明度，但解释权威仍由人类研究者掌握，通过修改、删除、拒绝、插入和评论等分析行动行使判断力。

Conclusion: 研究展示了如何通过负责任的人机协作实施归纳主题分析，AI工具提供程序支持而人类保持解释权威，为质性研究中的AI应用提供了实践框架。

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [8] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: MyGram是一个用于多模态实体对齐的模态感知图变换器，通过模态扩散学习模块捕获模态内的深层结构上下文信息，并引入Gram损失实现跨模态的全局分布一致性，在多个数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体对齐方法可能忽视模态内的结构上下文信息，容易受到浅层特征的干扰，需要更好的方法来整合多模态数据并丰富实体语义表示。

Method: 提出MyGram框架：1）模态扩散学习模块捕获模态内深层结构上下文信息并实现细粒度多模态融合；2）引入Gram损失作为正则化约束，通过最小化多模态特征形成的4维平行多面体体积来实现跨模态全局分布一致性。

Result: 在五个公共数据集上的实验表明，MyGram优于基线模型，在FBDB15K上Hits@1最大提升4.8%，在FBYG15K上提升9.9%，在DBP15K上提升4.3%。

Conclusion: MyGram通过模态扩散学习和Gram损失有效解决了多模态实体对齐中模态内结构信息捕获不足和跨模态分布不一致的问题，显著提升了对齐性能。

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [9] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA是一个用于评估基于大语言模型的多智能体系统的框架，通过多步骤、可审计的评估过程，相比单一LLM评估方法，提供更稳定、可追溯且支持人类监督的自动化评估方案。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常局限于单一响应评分或狭窄基准测试，缺乏稳定性、可扩展性和自动化能力，特别是在企业级多智能体系统部署中，需要可靠协调、透明决策和可验证性能的评估框架。

Method: AEMA是一个过程感知、可审计的框架，能够在人类监督下规划、执行和聚合异构智能体工作流的多步骤评估，支持可追溯的记录和负责任的自动化。

Result: 在模拟真实业务场景的企业风格智能体工作流测试中，AEMA相比单一LLM评估方法，实现了更高的稳定性、更好的人类对齐性，并提供了可追溯的记录，支持负责任的自动化评估。

Conclusion: AEMA为基于大语言模型的多智能体系统提供了一个透明、可复现的负责任评估路径，解决了现有评估方法在稳定性、可扩展性和自动化方面的不足。

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [10] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: 该论文提出了一个整合算法追索、上下文赌博机和大型语言模型的统一框架，用于高风险环境中的序列决策，如个性化医疗。提出了追索赌博机问题和GLRB算法，以及结合LLM的LIBRA算法，具有三个关键保证。


<details>
  <summary>Details</summary>
Motivation: 在高风险个性化决策（如个性化医疗）中，需要同时考虑治疗行动和患者特征的可修改性。传统方法缺乏将算法追索、上下文赌博机和LLM知识有效结合的框架，难以平衡统计严谨性和领域知识。

Method: 1. 提出追索赌博机问题，决策者需同时选择治疗行动和患者特征的最小可行修改。2. 开发广义线性追索赌博机（GLRB）算法。3. 提出LIBRA算法，战略性地结合LLM的领域知识和赌博机学习的统计严谨性。

Result: LIBRA提供三个关键保证：热启动保证（LLM推荐接近最优时显著减少初始遗憾）、LLM努力保证（仅需O(log²T)次咨询LLM）、鲁棒性保证（即使LLM不可靠也不差于纯赌博机算法）。建立了匹配下界，实验证明GLRB和LIBRA在遗憾、治疗质量和样本效率上优于标准方法。

Conclusion: 该研究展示了追索感知、LLM辅助的赌博机算法在高风险个性化决策中的潜力，为LLM与赌博机的可信协作提供了有前景的框架，平衡了领域知识和统计严谨性。

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [11] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: TAAR框架通过检测思维陷阱并自适应重启解码，提升大模型推理能力，无需微调基础模型参数


<details>
  <summary>Details</summary>
Motivation: 长思维链虽然能增强推理能力，但模型一旦早期做出错误承诺，就会陷入思维陷阱，后续反思和验证都无法修正根错误

Method: 提出TAAR框架：训练诊断策略预测两个信号（陷阱索引和逃脱概率），推理时在预测的陷阱段前截断轨迹并自适应重启解码，严重情况下应用更高温度重采样和结构化重启后缀

Result: 在DAPO-MATH子集上89%的失败案例存在思维陷阱；在AIME24、AIME25、GPQA-Diamond、HMMT25、BRUMO25等数学和科学推理基准上，TAAR提升了推理性能

Conclusion: TAAR通过检测和逃离思维陷阱，有效解决了长思维链推理中的错误累积问题，为测试时计算控制提供了新框架

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [12] [Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement](https://arxiv.org/abs/2601.11974)
*Xinmeng Hou,Peiliang Gong,Bohao Qu,Wuqi Wang,Qing Guo,Yang Liu*

Main category: cs.AI

TL;DR: MARS框架通过单次循环实现高效自我进化，结合原则性反思和程序性反思来优化推理逻辑，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体受限于静态的人工设计提示，缺乏适应性。现有的自我改进框架通常依赖低效的多轮递归循环，计算成本高昂。

Method: 提出MARS框架，模仿人类学习过程，整合原则性反思（抽象规范规则避免错误）和程序性反思（推导逐步成功策略），在单次递归循环中合成优化指令。

Result: 在六个基准测试上的广泛实验表明，MARS优于最先进的自我进化系统，同时显著降低了计算开销。

Conclusion: MARS框架通过高效的自我进化机制，使智能体能够系统性地改进推理逻辑，无需持续在线反馈，为自主智能体发展提供了新方向。

Abstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

</details>


### [13] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: PICL是一种动态演示集成框架，通过实时识别推理过程中的困惑点并插入相关演示来提升数学推理能力，相比静态演示方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法在数学推理等需要逐步逻辑推导的任务中存在局限性，主要问题是使用静态演示，无法适应推理过程中出现的动态困惑点，导致级联错误。

Method: 提出过程上下文学习（PICL）框架，包含两个阶段：1）通过分析推理过程的语义和熵来识别潜在困惑点并总结核心特征；2）在遇到这些困惑点时，从演示池中检索匹配困惑上下文的演示，并将其直接插入到正在进行的推理过程中以指导后续步骤。

Result: 实验表明PICL优于基线方法，通过缓解推理过程中的困惑点，提高了复杂数学推理的准确性。

Conclusion: 自适应演示插入在复杂数学推理中具有重要价值，动态演示集成框架能够有效提升上下文学习在需要逐步逻辑推导任务中的表现。

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [14] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体框架，将大规模用户评论转化为可执行的商业建议，通过聚类、生成、评估和可行性排序等组件提升建议质量。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法（如情感分析、方面提取）主要停留在描述性任务，而大语言模型生成的建议往往缺乏准确性和深度推理，需要一种能将用户评论转化为可执行商业建议的系统性方法。

Method: 提出多智能体LLM框架，包含四个组件：1) 聚类选择代表性评论；2) 建议生成；3) 迭代评估；4) 基于可行性的排序。该设计将语料库提炼与反馈驱动的建议优化相结合。

Result: 在三个服务领域和多个模型系列的实验中，该框架在可操作性、特异性和非冗余性方面持续优于单模型基线，中等规模模型的表现接近大型模型框架。

Conclusion: 该多智能体LLM框架能够有效将大规模用户评论转化为具体、可执行且实用的商业建议，为决策支持提供了新的技术路径。

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [15] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: 该论文提出在Dung抽象论辩框架中引入明确的子论点关系，作为与攻击关系并列的基本关系，以更好地捕捉结构化论辩中的依赖关系。


<details>
  <summary>Details</summary>
Motivation: Dung的抽象论辩框架仅通过攻击关系来表征论点可接受性，虽然这种抽象层次产生了丰富的研究成果，但限制了表示结构化论辩形式中核心的结构依赖关系（特别是子论点关系）的能力。现有扩展（包括双极论辩框架）引入了支持关系，但这些无法捕捉子论点的非对称性和构成性本质，也无法处理子论点与攻击之间的相互作用。

Method: 研究在抽象论辩框架中丰富明确的子论点关系，将其与攻击关系一起作为基本关系处理。分析子论点关系如何与攻击相互作用，并考察它们对基本语义属性的影响。

Result: 该框架为结构信息提供了原则性的抽象，并澄清了子论点在抽象可接受性推理中的作用。

Conclusion: 通过引入子论点关系作为基本关系，可以更好地抽象结构化论辩信息，为理解子论点在抽象论辩推理中的作用提供了更清晰的框架。

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [16] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: UniMo是一个统一框架，通过监督微调和强化学习优化，结合运动-语言信息和可解释的思维链推理，显著提升3D人体运动生成与理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动生成与理解方法可解释性有限，限制了这两个相关任务之间的相互增强。基于大语言模型的统一框架存在语义对齐和任务连贯性挑战，且下一词预测范式不适合运动序列，会导致累积预测误差。

Method: 提出UniMo框架：1) 通过监督微调将运动-语言信息和可解释思维链推理集成到大语言模型中；2) 引入基于组相对策略优化的强化学习作为后训练策略，通过优化令牌组来强制结构正确性和语义对齐，减轻运动令牌预测中的累积误差。

Result: 大量实验表明，UniMo显著优于现有的统一和任务特定模型，在运动生成和理解任务上都达到了最先进的性能。

Conclusion: UniMo通过结合监督微调和强化学习优化，有效解决了现有方法在语义对齐、任务连贯性和累积误差方面的问题，为3D人体运动生成与理解提供了一个高效统一的解决方案。

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [17] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: DriveSafe：针对LLM驾驶助手的四级风险分类法，包含129个细粒度风险类别，评估显示现有模型在驾驶场景中安全对齐不足


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地集成到车载数字助手中，但现有安全分类和评估框架多为通用型，无法捕捉真实驾驶场景的领域特定风险。不安全、模糊或法律错误的响应可能导致严重的安全、伦理和监管后果。

Method: 提出了DriveSafe，一个分层的四级风险分类法，包含129个细粒度原子风险类别，涵盖技术、法律、社会和伦理维度。该分类法基于真实驾驶法规和安全原则，并由领域专家评审。通过评估六个广泛部署的LLM在构建提示上的拒绝行为来验证安全相关性和真实性。

Result: 评估的模型经常无法适当拒绝不安全或不合规的驾驶相关查询，突显了通用安全对齐在驾驶场景中的局限性。

Conclusion: 需要针对驾驶场景的领域特定安全评估框架，现有LLM在驾驶安全对齐方面存在不足，DriveSafe分类法为系统评估LLM驾驶助手的安全风险提供了基础。

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [18] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: TIDE是一种用于处理时序扩展目标规划的新方法，通过将时序问题分解为可管理的子问题，利用成本驱动启发式引导搜索，并采用自适应回溯机制确保完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf任务规划方法通常将时序规划问题转化为经典规划问题，但缺乏针对时序目标的启发式引导搜索，导致效率受限。

Method: TIDE将时序问题分解为一系列可管理的reach-avoid子问题，利用成本驱动启发式在域图中识别和优先处理有希望的自动机轨迹，并采用自适应回溯机制从失败计划中恢复。

Result: 实验结果表明TIDE实现了有前景的性能表现，是时序扩展目标规划方法组合中的一个有价值的补充。

Conclusion: TIDE通过创新的分解策略、启发式引导和自适应回溯机制，有效解决了传统LTLf规划方法缺乏启发式引导的问题，为时序扩展目标规划提供了高效解决方案。

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [19] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: CoLLaMo是一个基于大语言模型的分子助手，通过多级分子模态协作投影器整合1D序列、2D分子图和3D构象信息，解决了现有大分子语言模型的幻觉和鲁棒性限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大分子语言模型（LMLMs）通常存在幻觉问题和有限的鲁棒性，这主要是由于未能充分整合多种分子模态（如1D序列、2D分子图和3D构象）造成的。需要开发能够更好整合这些模态的模型来提升分子理解和生成能力。

Method: 提出了CoLLaMo模型，包含一个多级分子模态协作投影器，采用关系感知的模态协作注意力机制，通过整合2D结构关系和3D空间关系，促进原子间的细粒度、关系引导的信息交换。同时提出了新的分子中心自动评估方法，包括幻觉评估指标和基于GPT的标题质量评估。

Result: 实验表明CoLLaMo增强了LMLMs的分子模态泛化能力，在多个任务上取得了最佳性能，包括分子标题生成、计算性质问答、描述性质问答、基序计数和IUPAC名称预测。

Conclusion: CoLLaMo通过有效整合多种分子模态，解决了现有大分子语言模型的幻觉和鲁棒性问题，为分子理解和生成任务提供了更强大的工具，并通过新的评估指标更好地评估了模型的分子理解能力。

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [20] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX-Pro扩展了FutureX的通用未来预测基准，针对金融、零售、公共卫生和自然灾害四个高价值垂直领域建立了专门的预测框架，评估当前最先进的智能体LLM在这些关键领域的预测能力。


<details>
  <summary>Details</summary>
Motivation: 虽然通用智能体在开放领域搜索中表现出色，但它们在资本密集型和安全关键领域的可靠性尚未得到充分探索。需要评估智能体LLM是否具备工业部署所需的领域基础能力。

Method: 基于FutureX的无污染实时评估流程，针对四个垂直领域（金融、零售、公共卫生、自然灾害）建立专门的预测框架，包括市场指标预测、供应链需求预测、流行病趋势跟踪和自然灾害追踪等基础预测任务。

Result: 研究发现当前最先进的智能体LLM在通用推理能力与高价值垂直应用所需精度之间存在性能差距，表明这些模型尚未具备工业部署所需的领域基础能力。

Conclusion: FutureX-Pro揭示了智能体LLM在关键垂直领域预测中的局限性，强调了开发具备领域专业知识的专门化预测系统的重要性，为未来研究和工业应用提供了基准框架。

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [21] [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](https://arxiv.org/abs/2601.12310)
*Jennifer Dodgson,Alfath Daryl Alhajir,Michael Joedhitya,Akira Rafhael Janson Pattirane,Surender Suresh Kumar,Joseph Lim,C. H. Peh,Adith Ramdas,Steven Zhang Zhexu*

Main category: cs.AI

TL;DR: 提出了一种基于环境生存性而非奖励的自我训练架构，通过行为在真实资源约束下的持久性和可重复性进行选择，避免了奖励黑客攻击和语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统自我训练系统由于缺乏判断数据质量的外部标准，容易导致奖励黑客攻击和语义漂移问题。需要一种在稀疏外部反馈和有限内存条件下实现稳定自我训练的系统架构。

Method: 引入基于环境生存性的自我训练架构，学习完全由环境生存性调节而非奖励或目标函数。候选行为在真实资源约束下执行，只有那些环境效应持久且保持未来交互可能性的行为才会被传播。环境不提供语义反馈、密集奖励或任务特定监督，选择仅通过行为作为世界改变事件的差异生存来实现。

Result: 分析语义动态表明，改进主要通过有效和可重复策略在整合和剪枝机制下的持久性实现（负空间学习范式）。模型在没有明确指导的情况下发展出元学习策略（如故意实验失败以获取信息性错误消息）。

Conclusion: 环境基础的选择机制能够实现可持续的开放式自我改进，为构建更鲁棒和可泛化的自主系统提供了可行路径，无需依赖人工标注数据或复杂的奖励塑造。

Abstract: Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.

</details>


### [22] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: 本文首次建立了文档智能数据生成的综合技术图谱，重新定义数据生成为监督信号生产，基于"数据和标签可用性"提出新分类法，将方法分为四个资源中心范式，并建立多级评估框架。


<details>
  <summary>Details</summary>
Motivation: 文档智能发展需要大规模高质量训练数据，但人工标注成为关键瓶颈。现有综述局限于单一模态或特定任务，缺乏与现实工作流程统一视角，需要填补这一空白。

Method: 重新定义数据生成为监督信号生产，基于"数据和标签可用性"引入新分类法，将方法分为四个资源中心范式：数据增强、从零开始数据生成、自动数据标注和自监督信号构建。建立多级评估框架整合内在质量和外在效用。

Result: 建立了首个文档智能数据生成的综合技术图谱，系统化整理了碎片化领域，揭示了保真度差距等关键挑战和协同进化生态系统等前沿方向，编译了跨多样文档智能基准的性能增益。

Conclusion: 通过系统化这一碎片化领域，将数据生成定位为下一代文档智能的核心引擎，为未来研究提供统一框架和方向指导。

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [23] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: MARO是一种通过多智能体社交环境训练大语言模型的方法，通过分解学习信号、平衡角色权重和直接评估行为效用，显著提升了模型的社交推理能力，并能将学习到的能力迁移到数学推理等任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型训练方法主要让模型从现有文本内容学习或解决预定问题，缺乏在真实社交场景中与他人互动、协商和竞争的经验，这限制了模型在需要复杂社交推理的场景中的表现。

Method: 提出多智能体奖励优化(MARO)方法：1) 将最终成功或失败结果分解为交互过程中的具体行为，解决稀疏学习信号问题；2) 平衡不同角色的训练样本权重，解决角色分布不均问题；3) 直接评估每个行为的效用，解决环境不稳定问题。

Result: 实验结果表明，MARO不仅在社交推理能力上取得显著提升，而且通过社交模拟学习获得的能力能够有效迁移到数学推理和指令跟随等其他任务中。

Conclusion: 多智能体社交学习在增强大语言模型通用推理能力方面具有巨大潜力，MARO方法为解决模型在复杂社交场景中的学习问题提供了有效途径。

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [24] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出一个两阶段LLM框架，将客户评论转化为可执行建议：首先提取问题并分类，然后基于问题表示生成针对性操作建议，使用LoRA专家混合策略实现专业化。


<details>
  <summary>Details</summary>
Motivation: 客户评论包含丰富的服务失败和用户期望信号，但将这些非结构化反馈转化为可执行的商业决策仍然困难。需要将评论转化为具体、可实施的操作建议。

Method: 提出模块化两阶段LLM框架：1) 问题模型提取关键问题并分配粗粒度主题；2) 建议模型基于提取的问题表示生成针对性操作建议。采用LoRA专家混合策略，训练多个低秩适配器，通过轻量级门控机制在推理时进行token级专家混合。

Result: 在Yelp评论（航空和餐厅）构建的合成数据集上评估，使用包含8个维度的操作评估标准（可操作性、特异性、可行性、预期影响、新颖性、非冗余性、偏见、清晰度）。在两个领域均优于仅提示和单适配器基线，获得更高的可操作性和特异性，同时保持有利的效率-质量权衡。

Conclusion: 提出的两阶段LLM框架结合LoRA专家混合策略，能够有效将客户评论转化为具体、可实施的操作建议，在多个评估维度上优于基线方法，为商业决策提供有价值的支持。

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [25] [PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling](https://arxiv.org/abs/2601.12392)
*Zhentao Xia,Yongqi Fan,Yuxiang Chu,Yichao Yin,Liangliang Chen,Tong Ruan,Weiyan Zhang*

Main category: cs.AI

TL;DR: PsychēChat是一个用于心理咨询的LLM系统，通过显式建模来访者情绪变化和安全风险分析来提升咨询效果


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询模型通常不显式建模来访者在咨询会话中的情绪变化，这是经典心理学流派的核心关注点。同时，如何使咨询师模型的回应与这些情绪变化对齐，并主动缓解安全风险，这些问题尚未得到充分探索。

Method: 提出PsychēChat系统，通过交互式角色扮演合成咨询师-来访者对话，包含两个模块：情绪管理模块（捕捉当前情绪和情绪变化）和风险控制模块（预测后续反应和识别潜在风险）。引入两种建模范式：Agent模式（多智能体协作管道）和LLM模式（统一思维链端到端推理）。

Result: 通过交互式评分、对话级评估和人工评估等广泛实验表明，PsychēChat在情感洞察和安全控制方面优于现有方法。

Conclusion: PsychēChat通过显式整合情绪变化跟踪和安全风险分析，为心理咨询提供了更有效的解决方案，在情感洞察和安全控制方面表现出色。

Abstract: Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.

</details>


### [26] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: LLMs在知识状态追踪和估计任务上表现接近随机水平，显著低于人类，未来研究应更重视知识估计和意图理解能力


<details>
  <summary>Details</summary>
Motivation: 认知人类学认为人类智能的关键在于推断他人知识状态和理解意图的能力，而黑猩猩等近亲动物缺乏这种能力。本研究旨在评估LLM在知识状态追踪和估计方面的表现。

Method: 设计两个任务：1) 检测故事角色是否通过行动表现出本不应拥有的知识；2) 基于角色自身知识（而非客观真相）预测其下一步行动。测试当前最先进的LLM在这些任务上的表现。

Result: 大多数当前最先进的LLM在两个任务上都表现出接近随机的性能水平，显著低于人类的表现。

Conclusion: LLM在知识状态追踪和意图理解方面存在明显不足，未来LLM研究应更加重视知识估计和意图理解能力的提升。

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [27] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: 该研究探索大语言模型在OWL本体论中的证明生成能力，开发了自动化数据集构建和评估框架，发现逻辑复杂性是影响性能的主要因素，而非表示格式。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在推理任务（如演绎）方面的能力已被广泛研究，但它们在生成证明——即忠实、人类可读的结论推导解释——方面的能力仍未被充分探索。本研究旨在探索LLMs在OWL本体论背景下的证明生成能力。

Method: 开发了一个自动化数据集构建和评估框架，在OWL本体论背景下评估LLMs的证明生成能力。评估包括三个顺序任务：提取、简化和解释，以及一个额外的逻辑完整性评估任务。在广泛使用的大语言模型上进行了大量实验。

Result: 研究发现：(1) 某些模型在整体上表现良好，但在复杂案例上仍有局限；(2) 逻辑复杂性（而非表示格式，如形式逻辑语言与自然语言）是影响LLM性能的主要因素；(3) 输入数据中的噪声和不完整性会显著降低LLMs的性能。

Conclusion: 这些结果强调了LLMs在提供严谨逻辑解释方面的潜力，同时也揭示了在复杂或不完美条件下支持弹性推理的差距。研究为LLMs在逻辑证明生成方面的能力提供了重要见解。

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [28] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: 大型语言模型在多跳推理中存在位置偏见问题，导致性能下降。研究发现多跳推理性能取决于最不可见证据的性能水平，且失败由绝对位置而非事实间线性距离决定。注意力引导可改善识别瓶颈，而"思考"模型能有效定位和整合信息。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已扩展到大规模上下文窗口，但在多跳推理中仍存在位置偏见问题，导致模型忽略某些位置的信息。需要厘清这些失败是由于无法定位证据（识别失败）还是无法整合证据（合成失败）。

Method: 引入多焦点注意力指令（MFAI）作为语义探针，通过显式引导注意力到选定位置来分离识别和合成机制。在5个LLM上对两个多跳QA任务（MuSiQue和NeoQA）进行实验，分析位置偏见对推理性能的影响。

Result: 建立了"最弱链接定律"：多跳推理性能崩溃到最不可见证据的性能水平。失败由绝对位置而非事实间线性距离决定（性能方差<3%）。匹配的MFAI可解决识别瓶颈，在低可见性位置提高准确率达11.5%。"思考"模型能有效定位和整合所需信息，即使在嘈杂的长上下文设置中也能匹配仅使用黄金信息的基线。

Conclusion: 多跳推理失败主要由识别失败而非合成失败驱动，且受绝对位置偏见影响。注意力引导可缓解识别瓶颈，而系统2推理模型能有效处理长上下文中的多跳推理任务。这些发现为改进LLM的多跳推理能力提供了重要见解。

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [29] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: 该综述论文系统性地组织了智能体推理的研究框架，将其分为三个互补维度：基础智能体推理、自我进化智能体推理和集体多智能体推理，并区分了上下文推理与后训练推理两种方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在封闭环境中表现出强大的推理能力，但在开放动态环境中表现不佳。智能体推理通过将LLMs重构为能够规划、行动和持续学习的自主智能体，实现了范式转变，旨在解决LLMs在开放环境中的局限性。

Method: 论文采用三维框架组织智能体推理研究：1）基础智能体推理（单智能体在稳定环境中的规划、工具使用和搜索）；2）自我进化智能体推理（通过反馈、记忆和适应优化能力）；3）集体多智能体推理（协作环境中的协调、知识共享和共同目标）。同时区分了上下文推理（通过结构化编排扩展测试时交互）和后训练推理（通过强化学习和监督微调优化行为）。

Result: 该综述系统回顾了智能体推理在科学、机器人、医疗、自主研究和数学等实际应用领域的代表性框架和基准测试，将各种方法整合成一个统一的路线图，连接了思维与行动。

Conclusion: 智能体推理为LLMs在开放动态环境中的应用提供了系统性框架。未来研究方向包括个性化、长时程交互、世界建模、可扩展的多智能体训练以及实际部署的治理机制。

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [30] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: Deep Research是一个多智能体系统，能够在几分钟内完成交互式科学研究，相比传统批处理模式（需要数小时）显著提升了效率，并在BixBench计算生物学基准测试中取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学发现系统大多是专有的，且采用批处理模式，每个研究周期需要数小时，无法实现实时研究人员指导。需要开发能够支持交互式科学调查的系统。

Method: 采用多智能体架构，包括规划、数据分析、文献搜索和新颖性检测等专门智能体，通过持久世界状态统一管理跨迭代研究周期的上下文。支持两种操作模式：带选择性人工检查点的半自主模式和用于扩展研究的全自主模式。

Result: 在BixBench计算生物学基准测试中表现出色：开放回答准确率达到48.8%，多项选择评估准确率达到64.5%，比现有基线提高了14到26个百分点。

Conclusion: Deep Research系统实现了交互式科学调查，显著缩短了研究周期时间。同时分析了架构约束（如开放获取文献限制和自动新颖性评估挑战），为AI辅助科学工作流的实际部署提供了实用考虑。

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [31] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 论文主张临床AI应从预测引擎转向支持临床决策的序数、非补偿性推理框架，采用稳健的序数规则而非期望效用优化，以更好地模拟临床医生的实际推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前临床AI系统主要作为预测引擎（产生标签或风险评分），但真实的临床推理是时间受限、顺序控制的不确定性问题。临床医生在信息收集与不可逆行动之间交替，受后悔、约束和患者价值观指导。需要开发与临床医生推理方式更一致的AI框架。

Method: 提出临床推理的主导计算基础不是基数优化而是序数、非补偿性决策：临床医生常依赖快速节俭的词典式启发式（如快速节俭树），仅检查少量固定线索后即停止。为这种算法提供规范性理由，并构建临床医生对齐的AI蓝图。

Result: 论证了序数决策在医学中的认识论优势：1）临床权衡主要通过人类判断构建，在绝对尺度上可测量性弱；2）偏好和信号获取结构粗糙，存在持久的不确定性下限。当这种"粗糙性"超过决策边界时，期望效用优化变得脆弱，而稳健的支配/过滤规则能稳定决策。

Conclusion: 提出临床对齐AI蓝图：使用丰富模型进行信念和轨迹建模，但通过稳健序数规则选择行动；将启发式视为低维特例；将AI部署为"选择性复杂性"——主要在决策脆弱且信息具有正期望影响时用于打破平局。

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [32] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: 论文提出了一个统一的智能体AI分类框架，将智能体分解为感知、大脑、规划、行动、工具使用和协作六个核心组件，用于分析从简单单循环智能体到分层多智能体系统的各种架构设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI从单纯生成文本转向智能体AI，系统能够作为自主实体感知、推理、规划和行动。LLMs不再只是被动知识引擎，而是作为认知控制器结合记忆、工具使用和环境反馈来追求长期目标。然而，从简单单循环智能体到分层多智能体系统的各种新兴设计使得这一领域难以导航，需要统一的分类框架来理解这一快速发展的领域。

Method: 论文提出了一个统一的分类法，将智能体分解为六个核心组件：感知、大脑、规划、行动、工具使用和协作。使用这个框架分析从线性推理过程到原生推理时间推理模型的转变，以及从固定API调用到开放标准（如模型上下文协议和原生计算机使用）的过渡。同时分类了智能体运行的环境，包括数字操作系统、具身机器人和其他专业领域，并回顾了当前的评估实践。

Result: 提出了一个系统化的智能体AI架构分类框架，能够帮助研究人员和从业者理解各种智能体设计。该框架涵盖了智能体的核心功能组件、推理方法演进、工具使用标准、运行环境分类以及当前评估方法，为智能体AI领域提供了结构化的分析工具。

Conclusion: 智能体AI正在快速发展，从简单的文本生成模型演变为能够自主感知、推理、规划和行动的复杂系统。论文提出的统一分类框架有助于理解这一领域的多样化设计，同时指出了幻觉行为、无限循环和提示注入等开放挑战，为未来构建更鲁棒可靠的自主系统指明了研究方向。

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [33] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: STEP-LLM：通过大语言模型从自然语言生成STEP格式CAD模型，解决传统文本到CAD方法对内核依赖和制造兼容性问题，通过DFS重序列化、检索增强生成和强化学习提升几何保真度。


<details>
  <summary>Details</summary>
Motivation: 传统CAD模型创建需要专业知识和大量人工，现有基于大语言模型的文本到CAD方法主要关注命令序列或脚本格式（如CadQuery），但这些格式依赖于特定内核且缺乏制造通用性。STEP文件作为广泛采用的中性边界表示格式直接兼容制造，但其图结构、交叉引用的特性给自回归大语言模型带来独特挑战。

Method: 1. 构建约40K STEP-描述对数据集；2. 针对STEP图结构格式设计新颖预处理：基于深度优先搜索的重序列化线性化交叉引用同时保持局部性，以及思维链式结构注释指导全局一致性；3. 集成检索增强生成在监督微调中基于相关示例进行预测；4. 通过强化学习使用基于Chamfer距离的几何奖励优化生成质量。

Result: 实验表明STEP-LLM在几何保真度上相比Text2CAD基线持续提升：检索增强生成模块显著增强完整性和可渲染性，DFS重序列化提升整体准确性，强化学习进一步减少几何差异。指标和视觉比较都确认STEP-LLM生成形状具有更高保真度。

Conclusion: 研究证明了通过大语言模型从自然语言生成STEP模型的可行性，展示了其在为制造领域民主化CAD设计方面的潜力。该方法解决了传统方法的制造兼容性问题，为非专家用户将直观设计意图转化为可制造工件提供了有效途径。

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [34] [Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction](https://arxiv.org/abs/2601.12688)
*Xu Zhang,Qinghua Wang,Mengyang Zhao,Fang Wang,Cunquan Qu*

Main category: cs.AI

TL;DR: 该论文提出了一种用于多被告人案件的掩码多阶段推理框架，通过整合量刑逻辑到Transformer编码器中，提高AI在司法案件中的智能辅助能力，特别关注主犯与从犯的责任区分。


<details>
  <summary>Details</summary>
Motivation: 在多被告人刑事案件中，司法文书表述往往模糊各被告人的具体角色，这阻碍了AI系统进行有效的责任分析。现有方法难以精确区分主犯与从犯的责任程度，需要结合司法逻辑来提升智能司法系统的准确性和可解释性。

Method: 提出掩码多阶段推理框架：1）使用定向掩码机制澄清被告人角色；2）采用对比数据构建策略增强模型对主犯与从犯责任差异的敏感性；3）通过广播机制将预测的罪名标签整合到回归模型中，结合犯罪描述和法庭观点。

Result: 在自定义的故意伤害案件数据集IMLJP上评估，该框架在基于角色的责任区分方面取得显著准确率提升，优于基线方法，为智能司法系统提供了有效解决方案。

Conclusion: 该研究通过将量刑逻辑整合到预训练Transformer框架中，有效解决了多被告人案件中角色模糊的问题，提高了AI司法辅助系统的准确性和法律可解释性，代码已公开。

Abstract: Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.

</details>


### [35] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: 该论文提出SCFT和RLERR两种方法来解决大型推理模型中的表面反思问题，通过自我批判微调和强化学习提升反思质量与推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中常进行自我反思，但许多反思是表面的，对原始答案改进有限且增加计算开销，需要解决表面反思问题。

Method: 提出SCFT训练框架，让模型批判自身输出，通过拒绝采样筛选高质量批判，使用批判目标微调模型；进一步提出RLERR，利用SCFT初始化的高质量反思构建奖励信号，通过强化学习内化自我修正过程。

Result: 在AIME2024和AIME2025两个挑战性基准测试中，SCFT和RLERR显著提高了推理准确性和反思质量，优于最先进的基线方法。

Conclusion: SCFT和RLERR有效解决了大型推理模型中的表面反思问题，通过自我批判微调和强化学习相结合的方法，显著提升了模型的反思质量和推理性能。

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [36] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: VIRO框架通过嵌入轻量级操作级验证器来解决神经符号REC方法中的级联错误问题，在目标存在和不存在场景下达到61.1%的平衡准确率


<details>
  <summary>Details</summary>
Motivation: 现有神经符号REC方法假设中间推理步骤准确，导致级联错误：错误检测和无效关系在推理链中传播，即使图像中没有目标也会产生高置信度的假阳性结果

Method: 引入验证集成推理操作符(VIRO)框架，在推理步骤中嵌入轻量级操作级验证器，每个操作符执行并验证其输出（如对象存在性或空间关系），从而在验证条件不满足时鲁棒地处理无目标情况

Result: 在目标存在和无目标设置下达到61.1%的平衡准确率，在真实世界第一人称数据上展示泛化能力，具有高计算效率（吞吐量）、高可靠性（程序失败率<0.3%）和通过解耦程序生成与执行实现的可扩展性

Conclusion: VIRO框架通过集成操作级验证器有效解决了神经符号REC中的级联错误问题，在准确性、可靠性、效率和可扩展性方面均表现出色，为处理无目标情况提供了鲁棒解决方案

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [37] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: SL-CBM通过引入语义局部性约束，改进概念瓶颈模型的空间对齐能力，生成与模型内部推理一致的概念和类别级显著图，提升解释的局部忠实性和可干预性。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型（CBMs）存在局部忠实性不足的问题，无法将概念与有意义的图像区域进行空间对齐，这限制了模型的解释性和可靠性。需要一种能够提供空间连贯显著图的方法，使概念解释与图像区域更好地对应。

Method: 提出SL-CBM（具有语义局部性的CBM），通过集成1x1卷积层和交叉注意力机制来增强概念、图像区域和最终预测之间的对齐。该方法生成空间连贯的概念级和类别级显著图，并使用对比性和基于熵的正则化来平衡准确性、稀疏性和忠实性。

Result: 在图像数据集上的广泛实验表明，SL-CBM显著提高了局部忠实性、解释质量和干预效果，同时保持了具有竞争力的分类准确性。消融研究强调了对比性和基于熵的正则化对于平衡准确性、稀疏性和忠实性的重要性。

Conclusion: SL-CBM弥合了基于概念的推理和空间可解释性之间的差距，为可解释和可信赖的概念模型设定了新标准。该方法通过生成与模型内部推理紧密相关的忠实显著图，促进了更有效的调试和干预。

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [38] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: 本文介绍了基于答案集编程和转移系统的C-MT动作语言，用于形式化人类心理状态（如情绪）随可观察动作的演化过程，并引入新的因果规则来限制动作的心理副作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究需要更可控的智能体行为建模方法，特别是要限制动作对心理状态的不良副作用。人类心理状态（如情绪）的演化需要基于心理学理论进行形式化表示，以支持对心理状态动态变化的受控推理。

Method: 基于答案集编程和转移系统构建C-MT动作语言，利用评价情绪理论等心理学理论将心理状态形式化为多维配置。引入新的因果规则"forbids to cause"和专门用于心理状态动态的表达式，将心理变化原则转化为转移约束和不变性属性，通过轨迹分析在转移系统中进行严格评估。

Result: 开发了一个能够建模心理状态动态演化的框架，支持通过轨迹分析比较不同的心理变化动态。该框架已应用于情绪验证模型的设计，能够对心理状态的动态演化进行受控推理。

Conclusion: C-MT动作语言为形式化人类心理状态演化提供了有效框架，通过引入新的因果规则和转移约束，实现了对心理状态动态变化的受控推理，支持基于不同心理学原则的比较分析，在情绪验证等应用场景中具有实用价值。

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [39] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: 论文认为当前AI可解释性研究存在根本性问题，因为现有定义缺乏可操作性，无法为具体建模和推理规则提供形式化原则。作者提出基于对称性的可操作定义，并假设四种对称性足以解决可解释性的核心问题。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能可解释性研究存在根本性缺陷，因为现有的可解释性定义缺乏可操作性，无法为具体的建模和推理规则提供形式化原则。这导致可解释性研究停留在概念层面，难以转化为实际可用的方法和工具。

Method: 作者提出基于对称性的可操作定义方法，假设四种对称性足以：(1) 激发核心可解释性属性，(2) 刻画可解释模型的类别，(3) 推导出统一的可解释推理形式化框架（如对齐、干预和反事实推理），将其视为贝叶斯逆问题。

Result: 论文提出了一个基于对称性的可解释性理论框架，该框架能够为可解释性提供可操作的定义，并统一处理对齐、干预和反事实推理等可解释性任务，将其形式化为贝叶斯逆问题。

Conclusion: 通过引入对称性作为可解释性的基础，可以解决当前可解释性定义缺乏可操作性的问题，为人工智能可解释性研究提供坚实的理论基础和实用的方法论框架。

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [40] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: 论文分析了通用AI系统的风险与脆弱性，对比传统任务专用AI，提出C2V2框架（控制、一致性、价值、真实性）作为负责任AI的设计原则。


<details>
  <summary>Details</summary>
Motivation: 现代通用AI系统虽然功能强大，但在幻觉、毒性、刻板印象等方面存在风险，使其不可信。需要重新思考如何为通用AI系统设计负责任的AI原则。

Method: 从八个广泛接受的负责任AI原则（公平性、隐私性、可解释性、鲁棒性、安全性、真实性、治理、可持续性）出发，分析通用AI的风险和脆弱性，并与传统任务专用AI对比。提出输出自由度（DoFo）概念来解释差异，并推导出C2V2设计原则。

Result: 通用AI系统由于输出自由度（DoFo）非确定性地高，导致风险更严重且难以缓解。C2V2框架（控制、一致性、价值、真实性）可作为满足负责任AI要求的设计原则，AI对齐、检索增强生成、推理增强等技术在不同程度上满足这些要求。

Conclusion: 通过将应用或领域相关的负责任AI需求沿C2V2维度进行形式化建模，并采用系统设计方法结合各种技术，可以实现开发负责任通用AI的目标。

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [41] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 本文扩展了TIVS评估框架，增加了语义相似性缓存和可观测性评分比(OSR)，提出TIVS-O系统，在多智能体架构中同时优化安全性和透明度，实现了零高风险漏洞、41.6%的LLM调用减少以及性能-成本-环境的综合优化。


<details>
  <summary>Details</summary>
Motivation: 提示注入仍然是大型语言模型安全部署的主要障碍，特别是在多智能体环境中，中间输出可能传播或放大恶意指令。需要开发既能有效防御又能保持透明度的评估框架。

Method: 扩展了四指标TIVS框架，增加了语义相似性缓存和第五个指标(可观测性评分比)，形成TIVS-O系统。采用HOPE启发的嵌套学习架构，结合智能体管道和连续记忆系统，使用301个合成生成的注入提示进行测试，第四个智能体使用五个关键性能指标进行安全分析。

Result: 系统实现了零高风险漏洞的安全响应，语义缓存显著减少计算成本，LLM调用减少41.6%，延迟、能耗和碳排放相应降低。五种TIVS-O配置揭示了缓解严格性和取证透明度之间的最佳权衡。

Conclusion: 可观测性感知评估能揭示多智能体管道中的非单调效应，内存增强智能体可以联合最大化安全鲁棒性、实时性能、运营成本节约和环境可持续性，无需修改底层模型权重，为安全和绿色的LLM部署提供了生产就绪的途径。

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [42] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: CURE-MED框架通过课程式强化学习提升LLMs在多语言医疗推理中的表现，在13种语言上显著提高了逻辑正确性和语言一致性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单语言数学和常识推理上表现良好，但在多语言医疗推理应用中仍不可靠，阻碍了其在多语言医疗环境中的部署

Method: 提出CURE-MED框架：1) 引入CUREMED-BENCH多语言医疗推理数据集；2) 采用课程式强化学习，整合代码切换感知的监督微调和组相对策略优化，共同提升逻辑正确性和语言稳定性

Result: 在13种语言上持续超越强基线，7B参数模型达到85.21%语言一致性和54.35%逻辑正确性，32B参数模型达到94.96%语言一致性和70.04%逻辑正确性

Conclusion: 该研究支持LLMs实现可靠且公平的多语言医疗推理，代码和数据集已开源

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [43] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体精炼框架，通过结构化迭代对齐来提升医疗大语言模型的安全性和可靠性，结合生成模型和评估智能体，在900个临床查询上实现了89%的伦理违规减少和92%的风险降级率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域的应用日益增多，但确保其伦理完整性和安全合规性仍然是临床部署的主要障碍。需要一种系统化的方法来提升医疗AI的安全性和可靠性。

Method: 采用多智能体精炼框架，结合两个生成模型（DeepSeek R1和Med-PaLM）和两个评估智能体（LLaMA 3.1和Phi-4）。评估智能体使用美国医学会医学伦理原则和五级安全风险评估协议来评估响应。系统通过结构化迭代对齐过程进行优化。

Result: 在涵盖9个伦理领域的900个临床多样化查询上评估性能：DeepSeek R1实现更快的收敛（平均2.34 vs 2.67次迭代），Med-PaLM在处理隐私敏感场景方面表现更优。迭代多智能体循环实现了89%的伦理违规减少和92%的风险降级率。

Conclusion: 该研究提出了一种可扩展、符合监管要求且成本效益高的医疗AI安全治理范式，通过多智能体迭代精炼框架显著提升了医疗大语言模型的伦理合规性和安全性。

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [44] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: PepEDiff是一种新型的肽结合剂生成器，可直接在预训练蛋白质嵌入模型的连续潜在空间中生成结合序列，无需依赖结构预测，提高了序列多样性。


<details>
  <summary>Details</summary>
Motivation: 肽结合剂生成在治疗和生化应用中至关重要，但现有方法严重依赖中间结构预测，增加了复杂性并限制了序列多样性。

Method: 通过预训练蛋白质嵌入模型获得连续潜在空间表示，在潜在空间中进行探索和基于扩散的采样，直接生成结合序列而不依赖结构预测。

Result: 在TIGIT（一个具有大而平坦蛋白质-蛋白质相互作用界面的挑战性靶点）的案例研究中，该方法超越了现有最先进方法，展示了其作为零样本肽结合剂设计的通用、无结构框架的潜力。

Conclusion: PepEDiff提供了一种简单而有效的零样本肽结合剂设计方法，不依赖结构预测，能够生成超越已知结合剂分布的新颖肽序列，在治疗应用中具有重要潜力。

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [45] [The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models](https://arxiv.org/abs/2601.13358)
*Samuel Cyrenius Anderson*

Main category: cs.AI

TL;DR: 研究发现模型规模增长不会均匀提升推理能力，而是重构推理过程。通过分析25,000+思维链轨迹发现，神经缩放定律触发特定领域的相变而非均匀能力提升，推理成本由流形几何而非任务难度决定。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为模型规模增长会均匀提升推理能力，但本研究旨在探究规模增长如何实际改变推理过程的结构和几何特性，揭示推理能力的本质变化机制。

Method: 分析25,000+思维链轨迹，覆盖法律、科学、代码、数学四个领域和8B、70B两个参数规模。引入神经推理算子作为从初始到最终隐藏状态的映射，使用几何分析方法（如表示维度、轨迹对齐、流形解缠）量化推理结构变化。

Result: 发现三种不同的推理相变模式：法律推理呈现"结晶化"（维度下降45%，轨迹对齐增加31%，流形解缠10倍）；科学和数学推理保持"液态"（几何不变）；代码推理形成"晶格"结构（轮廓系数从0.13增至0.42）。神经推理算子在法律推理上通过探针解码达到63.6%准确率，无需遍历中间状态。发现跨域和跨规模的通用振荡特征（相干性约-0.4）。

Conclusion: 推理成本由流形几何决定而非任务难度，这为在拓扑允许的情况下加速推理提供了蓝图。规模增长触发领域特定的相变而非均匀能力提升，揭示了推理过程的结构化转变机制。

Abstract: Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.

</details>


### [46] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: AgentForge是一个轻量级开源Python框架，通过模块化架构简化LLM驱动的自主智能体开发，提供可组合技能抽象、统一LLM后端接口和声明式配置系统，显著降低开发时间。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架存在架构僵化、供应商锁定和复杂度过高等问题，阻碍了快速原型设计和部署。需要一种能够民主化LLM驱动自主智能体构建的解决方案。

Method: 提出AgentForge框架，包含三个关键创新：1) 可组合技能抽象，支持细粒度任务分解和形式化输入输出契约；2) 统一LLM后端接口，支持云端API和本地推理引擎无缝切换；3) 声明式YAML配置系统，分离智能体逻辑与实现细节。将技能组合机制形式化为有向无环图(DAG)。

Result: 在四个基准场景的评估中，AgentForge实现了竞争性的任务完成率，相比LangChain减少62%开发时间，相比直接API集成减少78%开发时间。编排延迟低于100ms，适合实时应用。框架集成了六个内置技能，支持自定义技能开发。

Conclusion: AgentForge填补了LLM智能体生态系统的关键空白，为研究人员和从业者提供了生产就绪的基础设施，用于构建、评估和部署自主智能体，同时不牺牲灵活性或性能。

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [47] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: SpatialBench-UC是一个用于评估文本到图像模型空间关系理解能力的小型可复现基准测试，包含200个提示和100个反事实对，提供完整的评估包和人类审核校准。


<details>
  <summary>Details</summary>
Motivation: 评估文本到图像模型是否遵循明确的空间指令难以自动化，因为对象检测器可能漏检目标或返回多个可能检测，简单的几何测试在边界情况下会变得模糊。空间评估本质上是一个选择性预测问题。

Method: 引入SpatialBench-UC基准测试，包含200个提示（50个对象对×4种关系）分组为100个反事实对。发布基准测试包、版本化提示、固定配置、每样本检查器输出和报告表格。包含轻量级人类审核来校准检查器的弃权边界和置信度阈值。

Result: 评估了三个基线模型：Stable Diffusion 1.5、SD 1.5 BoxDiff和SD 1.4 GLIGEN。结果显示，接地方法显著提高了通过率和覆盖率，但弃权仍然是主要因素，主要原因是缺失检测。

Conclusion: SpatialBench-UC提供了一个可复现、可审计的框架来评估文本到图像模型的空间关系理解能力，强调了选择性预测的重要性，并展示了接地方法的改进效果。

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [48] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: 提出基于上下文和文本的音频深度伪造检测器CADD，通过引入上下文信息和转录文本显著提升检测性能，并对抗攻击具有更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类在判断信息真伪时会利用上下文，但现有音频深度伪造检测器仅分析音频文件本身，忽略了上下文和转录文本的重要信息。

Method: 创建了记者提供的深度伪造数据集JDD和合成音频数据集SYN，提出了基于上下文的音频深度伪造检测器CADD架构，并在多个大规模数据集上评估性能。

Result: 上下文和/或转录文本能显著提升音频深度伪造检测器性能：F1分数提升5%-37.58%，AUC提升3.77%-42.79%，EER提升6.17%-47.83%。CADD对5种对抗攻击策略具有更强鲁棒性，性能下降平均仅为-0.71%。

Conclusion: 上下文信息和转录文本对于音频深度伪造检测至关重要，基于上下文的检测方法不仅能显著提升性能，还能增强对抗攻击的鲁棒性。

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [49] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: APOLO是一个自动化提示优化框架，通过多智能体协作机制系统探索更广更细粒度的提示空间，提升LLM在精神健康领域情感诊断的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在临床情感诊断中面临两大挑战：情感共病（多种交织情感状态使预测复杂化）和临床相关线索的低效探索。虽然LLM在情感分析任务中表现出色，但在高风险、上下文密集的医疗环境中，其诊断可靠性对提示设计高度敏感。

Method: APOLO将指令优化建模为部分可观察马尔可夫决策过程，采用多智能体协作机制，包括规划者、教师、批评者、学生和目标角色。规划者定义优化轨迹，教师-批评者-学生智能体迭代优化提示以增强推理稳定性和有效性，目标智能体根据性能评估决定是否继续优化。

Result: 实验结果表明，APOLO在领域特定和分层基准测试中持续提升诊断准确性和鲁棒性，展示了在精神健康领域可信赖LLM应用的可扩展和可泛化范式。

Conclusion: APOLO为解决临床情感诊断中的情感共病和线索探索效率问题提供了一个系统化的提示优化框架，为精神健康领域可信赖的LLM应用提供了可扩展的解决方案。

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [50] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: EGLR模型通过熵引导的潜在推理机制，在生成式重排序中实现"边推理边推荐"，解决了现有方法难以适应列表生成过程中动态熵变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式重排序方法大多无法适应列表生成过程中模型难度的动态熵变化，难以准确捕捉复杂偏好。受语言模型整合推理能力的启发，需要引入潜在推理机制来降低决策过程中的熵。

Method: 提出熵引导潜在推理（EGLR）推荐模型：1）采用"边推理边推荐"范式而非"先推理后推荐"；2）使用上下文感知推理令牌和动态温度调整实现熵引导的变长推理；3）轻量级集成设计，无需复杂独立模块或后处理。

Result: 在两个真实世界数据集上的实验验证了模型有效性，显著优势在于能与现有生成式重排序模型兼容以提升其性能。进一步分析展示了实际部署价值和研究潜力。

Conclusion: EGLR模型通过熵引导的潜在推理机制，在生成式重排序中实现了更精确的探索-利用权衡，为高难度的列表生成任务提供了有效的解决方案。

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [51] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: TruthTensor是一个创新的语言模型评估框架，通过实时预测市场任务评估LLMs在真实世界不确定性下的表现，超越传统静态基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估存在根本性挑战：静态基准测试无法捕捉真实世界的不确定性、分布偏移，以及孤立任务准确性与人类对齐决策之间的差距。需要一种能评估模型在社会化、高熵环境中作为人类模仿系统表现的评估范式。

Method: 提出TruthTensor评估框架，基于前瞻性、无污染的任务，将评估锚定在实时预测市场上，结合概率评分提供模型行为的整体视图。框架包含漂移中心诊断、显式鲁棒性检查、人类与自动评估角色划分、标注协议和统计测试程序。

Result: 在500多个真实市场（政治、经济、文化、技术）实验中，TruthTensor显示具有相似预测准确性的模型在校准、漂移和风险敏感性方面存在显著差异，表明需要从多个维度（准确性、校准、叙事稳定性、成本和资源效率）评估模型。

Conclusion: TruthTensor操作化了现代评估最佳实践，包括清晰的假设框架、谨慎的指标选择、透明的计算/成本报告、人类在环验证和开放的版本化评估合同，为LLMs在真实世界决策环境中的评估提供了可辩护的评估方法。

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


### [52] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: 提出TSEvol多智能体时间序列演化算法、TSEData-20K数据集、ChatAD系列模型、TKTO优化方法和LLADBench基准，显著提升时间序列异常检测的推理能力和泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的异常检测方法存在推理能力不足、多轮对话能力欠缺、泛化能力有限等问题，需要提升时间序列异常行为的理解和解释能力

Method: 1) 提出TSEvol多智能体时间序列演化算法；2) 构建TSEData-20K数据集和ChatAD系列模型；3) 提出TKTO优化方法增强跨任务泛化；4) 建立LLADBench基准评估框架

Result: ChatAD模型在准确率提升34.50%、F1分数提升34.71%、假阳性降低37.42%；TKTO优化后的ChatAD在分类、预测和填补任务上具有竞争力的推理和跨任务泛化性能

Conclusion: 提出的综合框架显著提升了LLM驱动的时间序列异常检测的推理能力、对话能力和泛化性能，为时间序列分析提供了有效的解决方案

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [53] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: AgentGC是首个基于智能体的进化式基因组数据压缩器，通过三层多智能体架构（Leader和Worker）实现用户友好界面、认知优化和自动压缩，在压缩比和吞吐量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的基因组数据压缩方法存在非进化性、低级压缩建模、适应性有限和用户界面不友好等问题，需要一种更智能、自适应的压缩解决方案。

Method: 提出AgentGC三层架构：1）用户层通过Leader结合LLM提供友好界面；2）认知层由Leader驱动，整合LLM考虑算法-数据集-系统的联合优化；3）压缩层由Worker负责，通过自动多知识学习框架执行压缩解压。设计了CP（压缩比优先）、TP（吞吐量优先）和BM（平衡模式）三种模式。

Result: 在9个数据集上与14个基线方法比较，平均压缩比增益分别为16.66%、16.11%和16.33%，吞吐量增益分别为4.73倍、9.23倍和9.15倍。

Conclusion: AgentGC作为首个基于智能体的进化式基因组数据压缩器，通过多智能体架构和LLM集成，有效解决了现有方法的局限性，在压缩性能和吞吐量方面都取得了显著提升。

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [54] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: 本文提出了一种角色分离的Transformer架构，在视觉推理任务ARC上超越了人类平均表现，验证了推理应作为独立通道存在的假设。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统（如LLMs和ViTs）主要作为行为序列预测机器运行，通过建模token统计来匹配可观察行为，缺乏持久、可读的思维状态。这与人类行为存在差距：人类可以通过解码内部状态来解释行为，而AI系统只能产生缺乏内部状态基础的流畅事后合理化解释。作者假设推理是一种模态：推理应该作为一个独立的通道存在，与规则应用的低级工作空间分离。

Method: 为解决ARC视觉推理任务，设计了新颖的角色分离Transformer块，将全局控制器token与网格工作空间token分离，实现迭代规则执行。该方法在VARC视觉中心协议中进行训练和评估。

Result: 在ARC-1任务上达到62.6%的准确率，超越了人类平均表现（60.2%），并显著优于先前方法。定性分析显示，与密集ViT基线相比，模型展现出更一致的规则应用结构，从概率斑块向控制器驱动的推理转变。

Conclusion: 研究验证了推理作为独立通道存在的假设，角色分离的Transformer架构在抽象推理任务上实现了超越人类的表现，为构建具有可解释内部状态的AI系统提供了新方向。

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [55] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: ScriptMind是一个基于大语言模型的诈骗检测框架，通过犯罪脚本推理任务、数据集构建和认知模拟评估，显著提升诈骗检测性能并增强用户认知警觉性。


<details>
  <summary>Details</summary>
Motivation: 传统诈骗检测方法难以应对个性化、多轮对话的社会工程诈骗，而现有大语言模型在诈骗检测方面的认知辅助潜力尚未充分挖掘。

Method: 提出ScriptMind框架，包含三个组件：犯罪脚本推理任务（CSIT）用于诈骗推理，犯罪脚本感知推理数据集（CSID）用于微调小型LLM，以及基于认知模拟的社会工程防御评估（CSED）用于评估实时认知影响。使用571个韩国电话诈骗案例构建了22,712个结构化诈骗序列训练实例。

Result: 经过ScriptMind微调的11B小型LLM在检测准确率上比GPT-4o高出13%，在误报减少、诈骗者话语预测和推理质量方面均优于商业模型。在电话诈骗模拟实验中，显著提升并维持了用户的怀疑水平，增强了他们对诈骗的认知意识。

Conclusion: ScriptMind代表了向以人为本、认知自适应的大语言模型在诈骗防御领域迈出的重要一步，为构建更有效的社会工程防御系统提供了新方向。

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [56] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: 提出基于音频情感信号的多智能体AI系统，实时生成响应式媒体内容，强调将情感识别转化为安全、可控的响应内容，而非仅关注分类精度。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别研究主要关注分类精度，但缺乏将识别到的情感状态转化为安全、可控的响应内容的能力。需要一种能够实时生成年龄适宜、安全合规的媒体响应系统。

Method: 采用四智能体协作架构：1)基于CNN的情感识别智能体提取声学特征；2)响应策略决策智能体将情感映射到响应模式；3)内容参数生成智能体产生媒体控制参数；4)安全验证智能体强制执行年龄适宜性和刺激约束，包含显式安全验证循环。

Result: 在公开数据集上，系统达到73.2%的情感识别准确率、89.4%的响应模式一致性、100%的安全合规性，同时保持低于100ms的推理延迟，适合设备端部署。

Conclusion: 该模块化架构实现了可解释性和可扩展性，适用于儿童相关媒体、治疗应用和情感响应智能设备，为情感AI系统提供了安全可控的响应生成框架。

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [57] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的全局一致性验证方法，通过自适应分治算法检测最小不一致子集，具有多项式查询复杂度


<details>
  <summary>Details</summary>
Motivation: 自然语言事实集合的全局一致性对于事实核查、摘要生成和知识库构建等任务至关重要。虽然LLM可以评估小规模事实子集的一致性，但其判断存在噪声，且成对检查无法保证全局一致性。

Method: 提出自适应分治算法，识别事实的最小不一致子集，可选地通过命中集计算最小修复。该方法具有低阶多项式查询复杂度。

Result: 在合成和真实LLM评估器上的实验表明，该方法能有效检测和定位不一致性，为基于LLM的语言一致性验证提供了可扩展框架。

Conclusion: 该方法解决了LLM评估器在全局一致性验证中的噪声和可扩展性问题，为实际应用提供了有效的解决方案。

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [58] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: SocialMindChange是一个新的动态心理理论基准测试，要求语言模型在社交互动中主动改变他人心理状态，而不仅仅是追踪心理状态变化。


<details>
  <summary>Details</summary>
Motivation: 现有动态心理理论基准测试大多让语言模型处于被动角色，只读取场景并报告心理状态变化。但在真实社交互动中，心理理论也用于行动：说话者计划说什么来改变他人的心理状态轨迹以达到目标。

Method: 引入SocialMindChange基准测试，每个实例定义包含4个角色的社交情境和5个连接场景。模型扮演一个角色，在5个场景中生成对话以达到目标，同时保持与所有参与者不断变化的状态一致。采用结构化四步框架构建了1200个社交情境，覆盖6000个场景和超过90000个问题。

Result: 对10个最先进的大型语言模型的评估显示，它们的平均性能比人类表现低54.2%。这表明当前LLM在长期连接互动中维持和改变心理状态表征方面仍存在困难。

Conclusion: 从追踪心理状态到改变心理状态的转变揭示了当前语言模型在动态社交互动中的局限性，为未来改进提供了重要基准。

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [59] [Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection](https://arxiv.org/abs/2601.13735)
*Hojin Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 研究发现当前基于概率的置信度指标无法有效捕捉推理步骤间的因果依赖关系，主要反映的是表面流畅度而非逻辑结构，并提出了一种对比因果指标来改进输出选择。


<details>
  <summary>Details</summary>
Motivation: 当前研究普遍假设概率置信度指标能够反映推理质量，但本文质疑这一假设，探究这些指标是否真正捕捉到了推理步骤间必要的因果依赖关系。

Method: 引入三类推理步骤间因果扰动，系统性地破坏推理步骤间的依赖关系但保持局部流畅性；在不同模型家族和推理基准上进行实验；提出对比因果指标来显式隔离步骤间的因果依赖。

Result: 即使严重干扰推理步骤间的因果依赖（如应用硬注意力掩码阻止模型关注先前推理步骤），选择准确率仅轻微下降，表明当前概率指标对逻辑结构不敏感，主要捕捉表面流畅度或分布内先验。

Conclusion: 当前概率置信度指标无法有效评估推理质量，主要反映表面特征而非逻辑结构；提出的对比因果指标能够更忠实地进行输出选择，为改进推理评估提供了新方向。

Abstract: Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.

</details>


### [60] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: RELIEF框架通过调整大推理模型的自我认知信念来塑造其行为，无需监督推理轨迹，降低了训练成本


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在计算冗余和推理不忠实的问题，现有方法依赖强化学习或黄金标准推理轨迹监督，计算成本高且难以扩展

Method: 提出RELIEF框架，通过简单的logit探测捕获模型的潜在推理信念，然后通过微调使模型的自我概念与目标信念蓝图对齐，使用合成的自我反思问答对来内化期望特质

Result: 在效率和忠实性任务上的实验表明，RELIEF匹配或优于基于行为监督和偏好的基线方法，同时训练成本更低；分析验证了改变模型的推理信念能有效塑造其实际行为

Conclusion: RELIEF提供了一种简单有效的框架，通过调整模型的自我认知信念来塑造大推理模型的行为，无需昂贵的推理轨迹监督，具有更好的可扩展性

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [61] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: DARC是一个两阶段自进化框架，通过解耦问题生成和解答训练，解决了自博弈中优化不稳定问题，在多个推理基准上平均提升10.9分。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈框架存在优化不稳定问题：1）问题者依赖解答者反馈的非平稳目标；2）解答者使用自生成伪标签导致的引导误差。

Method: 两阶段框架：第一阶段训练问题者基于明确难度级别和外部语料合成难度校准的问题；第二阶段通过非对称自蒸馏机制训练解答者，使用文档增强的教师模型生成高质量伪标签监督无文档访问的学生解答者。

Result: DARC具有模型无关性，在9个推理基准和3个骨干模型上平均提升10.9分，持续优于所有基线方法，接近完全监督模型的性能且无需人工标注。

Conclusion: DARC通过解耦问题生成和解答训练，稳定了自进化过程，为自改进AI提供了一种有效的自博弈框架。

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations. The code is available at https://github.com/RUCBM/DARC.

</details>


### [62] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: Look-Ahead-Bench是一个标准化基准，用于评估金融工作流中PiT大语言模型的前瞻性偏差，通过实际场景测试而非简单问答，发现标准LLMs存在显著前瞻性偏差，而PiT模型随规模扩大展现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过问答测试LLMs的内部前瞻知识，缺乏对实际金融工作流中模型行为的评估。需要区分真正的预测能力与基于记忆的性能，建立标准化评估框架来识别适合实际部署的金融LLMs。

Method: 创建Look-Ahead-Bench基准，评估不同时间市场制度下的性能衰减，引入多个量化基线建立性能阈值。评估开源LLMs（Llama 3.1 8B/70B和DeepSeek 3.2）与PiT-Inference的PiT LLMs（Pitinf-Small、Medium、Large），使用alpha衰减衡量前瞻性偏差。

Result: 标准LLMs显示出显著的前瞻性偏差，而PiT模型随规模扩大展现出改进的泛化和推理能力。Pitinf模型在不同时间市场制度下表现更稳定，表明更好的时间泛化能力。

Conclusion: 该工作为金融LLMs的时间偏差标准化评估奠定了基础，提供了识别适合实际部署模型的实用框架。PiT模型在减少前瞻性偏差方面表现优异，为金融领域LLMs的实际应用提供了重要参考。

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [63] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: 本文提出了虚拟都市主义(VU)框架，通过生成式AI创建城市合成复制品来量化城市身份认同，并在东京九个区域进行试点研究验证可行性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可计算的城市身份认同度量方法，需要开发一个能够量化城市核心特征的多模态AI分析框架。

Method: 整合Stable Diffusion和LoRA模型生成东京九个区域的合成城市序列，排除现有地标以提取核心身份元素，通过人类评估实验验证复制品感知合法性、量化区域级身份、提取核心身份形成元素。

Result: 合成复制品的平均识别准确率达到约81%，验证了复制品的有效性；城市身份水平(UIL)指标能够评估不同区域的身份水平；语义分析揭示了文化嵌入的类型学作为核心身份形成元素。

Conclusion: VU框架为AI增强的城市分析提供了可行路径，展示了实现自动化、多参数身份度量的潜力。

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [64] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: 提出人类模拟计算（HSC）框架，将智能建模为包含思考、行动、学习、反思和活动调度的连续闭环过程，强调通过行动自动改进内部推理机制，无需外部干预。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型仅依赖文本数据，限制了其在开放动态现实环境中的适应能力、推理结果验证和有效操作。需要一种更接近人类智能的计算框架。

Method: 提出人类模拟计算（HSC）框架，将智能建模为包含思考、行动、学习、反思和活动调度的连续闭环内部推理过程。强调主动参与环境交互，使用行动不仅实现目标，还自动改进内部推理机制。融入人类常用思维策略，如主特征导向推理、通过行动扩展范围、环境反馈驱动的及时学习。

Result: 通过理论分析表明，人类模拟策略无法仅从语言材料中完全学习，人类式推理过程和基于行动的推理方法对于在现实环境中实现稳健适应和有效交互至关重要。

Conclusion: HSC框架为解决大语言模型在现实环境中的局限性提供了新思路，强调主动参与、闭环学习和基于行动的推理改进，为实现更强大的适应性智能系统奠定了基础。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [65] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: PREFAB是一种低成本回顾式自我标注方法，针对情感变化区域而非完整标注，通过偏好学习模型检测相对情感变化，让标注者只标注选定片段，其余部分通过插值完成。


<details>
  <summary>Details</summary>
Motivation: 现有情感计算中的自我标注方法通常需要完整标注，这过程耗时、认知负担重、容易疲劳且易出错。需要一种更高效、负担更轻的标注方法。

Method: 基于峰值-终点规则和情感序数表示，使用偏好学习模型检测相对情感变化，指导标注者只标注选定片段，其余部分通过插值完成。引入预览机制提供上下文线索辅助标注。

Result: PREFAB在建模情感变化方面优于基线方法，同时减轻了工作负担（有条件地减轻时间负担）。重要的是，PREFAB提高了标注者信心且未降低标注质量。

Conclusion: PREFAB是一种有效的低成本情感标注方法，能够准确捕捉情感变化区域，同时减轻标注者负担，提高标注效率和信心。

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [66] [Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics](https://arxiv.org/abs/2601.14027)
*Junqi Liu,Zihao Zhou,Zekai Zhu,Marco Dos Santos,Weikun He,Jiawei Liu,Ran Wang,Yunzhou Xie,Junqiao Zhao,Qiufeng Wang,Lihong Zhi,Jia Li,Wenda Li*

Main category: cs.AI

TL;DR: 提出使用通用编码智能体作为形式数学推理器的新范式，通过Numina-Lean-Agent系统在Putnam 2025竞赛中取得满分成绩，并成功形式化Brascamp-Lieb定理。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统依赖特定任务流水线和训练过的形式证明器，限制了灵活性和可复现性。本文提出使用通用编码智能体作为形式数学推理器，因为：(1)通用编码智能体为超越证明的多样化推理任务提供自然接口；(2)仅通过替换底层基础模型即可提升性能，无需训练；(3)MCP支持灵活扩展和自主调用专业工具，避免复杂设计。

Method: 提出Numina-Lean-Agent系统，结合Claude Code与Numina-Lean-MCP，实现与Lean的自主交互、相关定理检索、非形式化证明和辅助推理工具调用。使用Claude Opus 4.5作为基础模型。

Result: 使用Claude Opus 4.5作为基础模型的Numina-Lean-Agent在Putnam 2025竞赛中解决了所有问题（12/12），与最佳闭源系统性能相当。此外，通过与数学家交互成功形式化了Brascamp-Lieb定理，展示了系统的通用性。

Conclusion: 通用编码智能体作为形式数学推理器的新范式具有显著优势，Numina-Lean-Agent在竞赛和实际数学定理形式化中都表现出色。系统代码和解决方案已开源发布。

Abstract: Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.

</details>


### [67] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent是一个多智能体框架，将反驳信生成重构为以证据为中心的规划任务，通过分解审稿意见、构建混合上下文、集成外部搜索，生成可检查的响应计划，确保每个论点都有明确证据支撑。


<details>
  <summary>Details</summary>
Motivation: 当前的反驳信生成方法通常将其视为直接文本生成问题，存在幻觉、遗漏批评、缺乏可验证基础等问题。需要一种能够精确对齐审稿意图和稿件细节的解决方案。

Method: 提出RebuttalAgent多智能体框架：1）将复杂反馈分解为原子化关切点；2）动态构建混合上下文，合成压缩摘要与高保真文本；3）集成自主按需外部搜索模块解决需要外部文献的关切点；4）在起草前生成可检查的响应计划。

Result: 在提出的RebuttalBench基准测试中，该流水线在覆盖率、忠实度和策略连贯性方面优于强基线模型，为同行评审过程提供了透明可控的助手。

Conclusion: RebuttalAgent通过将反驳信生成重构为证据中心的规划任务，解决了现有方法的局限性，提供了透明、可控且基于证据的反驳信生成框架，代码将开源。

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>
