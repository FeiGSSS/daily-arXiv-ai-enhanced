<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 47]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 该论文研究了随机噪声对四旋翼无人机系统的影响，使用扩展卡尔曼滤波器进行状态估计，并应用线性二次高斯控制器和期望最大化算法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 无人机在多个领域应用广泛，但地震等灾害会破坏基础设施，使救援人员难以到达某些区域。无人机可以克服这些障碍，但系统易受随机噪声影响，需要研究噪声对无人机系统的影响并开发有效的控制方法。

Method: 1. 在四旋翼无人机系统中添加随机噪声；2. 使用扩展卡尔曼滤波器基于传感器噪声观测进行状态估计；3. 基于随机微分方程系统实现线性二次高斯控制器；4. 应用期望最大化算法进行无人机参数估计；5. 比较离线参数估计和在线参数估计的结果。

Result: 在线参数估计的收敛值范围略大于离线参数估计。这表明在线参数估计方法在应对系统变化时具有更好的适应性，但同时也表现出更大的估计不确定性。

Conclusion: 随机噪声对无人机系统有显著影响，但通过扩展卡尔曼滤波器、线性二次高斯控制器和期望最大化算法的组合可以有效处理噪声问题。在线参数估计方法虽然收敛范围较大，但更适合实时应用场景，为无人机在复杂环境下的稳定控制提供了有效解决方案。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [2] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 该论文评估了现有可解释性方法在智能体系统中的应用局限，提出了专门针对智能体系统设计的新可解释性技术方向，以确保智能体AI系统的安全可靠部署。


<details>
  <summary>Details</summary>
Motivation: 智能体系统与传统机器学习模型在架构和部署上存在根本差异，引入了独特的安全挑战（如目标错位、决策错误累积、多智能体协调风险等），需要嵌入可解释性和可追溯性设计，但现有主要针对静态模型的可解释性技术在智能体系统中存在局限性。

Method: 评估现有可解释性方法在智能体系统中的适用性和局限性，识别其在提供智能体决策洞察方面的能力差距，提出专门针对智能体系统设计的新可解释性技术发展方向。

Result: 现有可解释性技术在应用于智能体系统时表现出局限性，无法有效处理智能体系统的时间动态性、决策累积性和上下文依赖行为，需要新的分析方法。

Conclusion: 需要开发专门针对智能体系统设计的可解释性技术，在智能体生命周期的各个阶段（目标形成、环境交互、结果评估）嵌入监督机制，这对于确保智能体AI系统的安全和负责任部署至关重要。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [3] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实世界临床记录构建生成式模拟器，能够基于患者历史生成高保真的未来临床轨迹


<details>
  <summary>Details</summary>
Motivation: 模拟在临床医学中具有变革潜力，可用于个性化治疗规划和虚拟临床试验，但模拟患者轨迹因复杂的生物和社会文化影响而具有挑战性

Method: 开发生成式模拟器模型，以患者历史为输入，合成细粒度的现实未来轨迹；在超过2亿条临床记录上进行预训练

Result: 模型生成高保真未来时间线，与真实患者未来数据中的事件发生率、实验室测试结果和时间动态紧密匹配；准确估计未来事件概率，观察与预期比率在不同结果和时间范围内始终接近1.0

Conclusion: 揭示了电子健康记录中真实世界数据的未开发价值，并引入了临床护理计算机模拟的可扩展框架

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [4] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 论文提出了一个最小化可校准理论，用于预测多智能体系统在固定推理预算下的三种行为模式：提升、饱和和崩溃，该理论基于三个关键约束：有限上下文窗口、有损通信和相似智能体间的共享故障。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统理论上能提高可靠性，但在固定推理预算下，它们常常只能提供有限帮助、达到饱和甚至崩溃。现有研究缺乏一个统一的理论框架来解释这些现象，特别是考虑到现代智能体堆栈的三个关键约束：有限上下文窗口、有损的智能体间通信，以及相似智能体间的共享故障。

Method: 开发了一个最小化可校准理论，将每个叶智能体用计算-性能缩放指数β表示，通信用消息长度保真度曲线γ(m)表示，相关性用有效共享误差相关性ρ表示，上下文窗口W施加了硬性扇入限制。对于具有多数聚合的二元成功/失败任务，证明了深度b叉树在相关输入和有损通信下的尖锐相变，单个标量αρ（结合γ(m)、ρ和扇入b）决定了弱信号是被放大到非平凡固定点还是被洗牌为随机结果。

Result: 理论推导出组织指数s，并证明当s>β时会出现预算协同效应（即相同总预算下优于最佳单个智能体），给出了封闭形式的计算分配规则和明确的预算阈值。进一步通过混合深度表征饱和现象，提供了在增长和饱和阶段都保持准确的保守裁剪预测器。连续性能预热给出了星形、链式和树形组织的封闭形式风险，使相关性和通信引起的地板效应变得明确。

Conclusion: 该理论框架成功预测了受控合成模拟中的相变边界，并解释了最近大规模匹配预算研究中报告的LLM智能体系统缩放的主要瓶颈。该理论为多智能体系统设计提供了核心权衡的明确分析，帮助理解在有限预算下如何优化智能体组织结构和通信策略。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [5] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式化数学数据合成管道，通过分解形式化过程为五个子任务并采用解耦提取策略，有效利用失败轨迹中的训练信号，显著降低数据合成成本。


<details>
  <summary>Details</summary>
Motivation: 形式化数学中智能体工作流的高成本阻碍了大规模数据合成，加剧了开源语料库的稀缺性问题。需要开发成本效益高的解决方案来构建训练未来专家模型所需的数据飞轮。

Method: 将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图。采用解耦提取策略，从全局失败的轨迹中恢复有效的训练信号，充分利用浪费的计算资源。

Result: 在2000个问题的基准测试中，TheoremForge实现了12.6%的验证率，超过8.6%的基线，每个成功轨迹的平均成本仅为0.481美元（使用Gemini-3-Flash）。解耦策略使证明生成的数据产量相比标准过滤提高了1.6倍。

Conclusion: TheoremForge作为一个可扩展的框架，能够有效构建数据飞轮来训练未来的专家模型，为解决形式化数学数据稀缺问题提供了成本效益高的解决方案。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [6] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该研究证明AGI（通用人工智能）无法获得独立于任务分布的统一定义，缺乏普遍鲁棒性，存在有限迁移能力，且无法通过可计算程序（包括自我验证）进行完备认证。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨AGI是否具有支持存在性、鲁棒性或自我验证绝对主张的连贯理论定义。当前对AGI的讨论常基于模糊概念，缺乏严格的数学框架来评估其理论可能性。

Method: 采用公理化方法，将AGI形式化为一个分布性、资源受限的语义谓词，索引包括任务族、任务分布、性能函数和显式资源预算。在此框架下，运用数学证明方法（包括Rice风格和哥德尔-塔斯基论证）推导理论结果。

Result: 1. 通用性是关系性的，不存在独立于分布的统一AGI概念；2. 任务分布的微小扰动可通过"悬崖集"使AGI属性失效，排除普遍鲁棒性；3. 有限资源下无法实现跨任务族的无界泛化；4. AGI作为非平凡语义属性，无法通过任何可计算程序（包括自我验证）进行完备认证。

Conclusion: 强分布独立的AGI主张在没有显式形式化索引的情况下是未定义的，AI的实证进展并不暗示自我认证通用智能的可实现性，依赖内部自我认证的递归自我改进方案存在根本缺陷。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [7] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 现有模型编辑特异性评估方法存在缺陷，作者提出了新的评估协议来解决这些问题


<details>
  <summary>Details</summary>
Motivation: 模型编辑需要平衡编辑效果和特异性（保留非目标知识），但现有特异性评估方法存在根本性问题，无法有效评估不同方法的性能差异

Method: 提出新的评估协议，消除开放LLM与确定性答案假设的冲突，避免查询无关的流畅性偏差，并能在接近连续的空间中平滑调整评估严格度

Result: 实验表明，新协议生成的指标对特异性正则化强度的变化更敏感，与正则化强度强相关，能更精细地区分不同方法的知识保留能力

Conclusion: 现有特异性评估方法存在概念和实证问题，提出的新评估协议能更准确、敏感地评估模型编辑的特异性性能

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [8] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体协作的MALPP框架，利用LLM驱动的智能体为高等教育提供透明、可解释的个性化学习路径规划。


<details>
  <summary>Details</summary>
Motivation: 当前智能导学系统中的学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需要解决这些挑战以充分发挥LLM在教育中的潜力。

Method: 提出MALPP框架，包含三个基于角色和规则的LLM智能体：学习者分析智能体、路径规划智能体和反思智能体，通过结构化提示和预定义规则协作分析学习档案、生成定制学习路径并进行迭代优化。

Result: 在MOOCCubeX数据集上使用7种LLM进行实验，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融研究验证了协作机制和理论约束的有效性。

Conclusion: 该研究为可信、可解释的教育AI发展做出贡献，展示了基于LLM的以学习者为中心的自适应教学的可扩展方法。

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [9] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 研究视觉语言模型在残疾人描述中的解释偏移问题，发现引入残疾上下文会降低解释保真度，导致推测性推断、叙事阐述、情感降级和缺陷导向框架等问题，这些效应在种族和性别维度上进一步放大。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型越来越多地应用于社会敏感领域，但其在残疾方面的行为尚未得到充分探索。模型经常从基于证据的事实描述转向解释偏移，引入超出可观察视觉证据的不支持推断。

Method: 基于中性提示(NP)和残疾情境化提示(DP)的配对基准，在零样本设置下评估15个最先进的开源和闭源视觉语言模型，涵盖9个残疾类别。评估框架将解释保真度作为核心目标，结合标准文本指标和LLM作为评判协议，由有残疾生活经验的标注者验证。

Result: 引入残疾上下文会持续降低解释保真度，导致解释偏移，表现为推测性推断、叙事阐述、情感降级和缺陷导向框架。这些效应在种族和性别维度上进一步放大。

Conclusion: 有针对性的提示和偏好微调能有效提高解释保真度，并大幅减少解释偏移。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [10] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: 研究探索大语言模型是否像人类逻辑一样从直觉推理转向形式系统，使用存在导入作为探针评估三段论推理，发现模型规模、思维链和基础模型是影响这种转变的关键因素。


<details>
  <summary>Details</summary>
Motivation: 受人类逻辑从直觉推理向严格形式系统演变的启发，探索大语言模型是否表现出类似的逻辑框架演变。利用存在导入作为探针，评估模型在传统逻辑和现代逻辑下的三段论推理能力。

Method: 使用存在导入作为探针评估三段论推理，在新建的三段论数据集上测试SOTA大语言模型。通过大量实验分析模型规模缩放、思维链推理和基础模型对逻辑框架转变的影响。

Result: 发现三个关键结果：(1) 模型规模缩放促进向现代逻辑的转变；(2) 思维链推理是超越参数缩放的高效加速器；(3) 基础模型决定这种转变的容易程度和稳定性。此外还进行了深入分析当前LLM在三段论推理上的特性。

Conclusion: 大语言模型确实表现出类似人类逻辑的演变模式，模型规模、思维链推理和基础模型是影响逻辑框架转变的关键因素，为理解LLM的逻辑推理能力提供了新视角。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [11] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice是一个用于构建和持续改进对话AI护栏的自适应框架，通过两阶段方法实现：初始构建和持续优化，在ProsocialDialog数据集上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI护栏系统使用静态规则，无法适应新威胁或部署环境的变化，需要能够自我构建和持续改进的自适应护栏框架。

Method: Lattice采用两阶段框架：1) 构建阶段通过迭代模拟和优化从标注示例构建初始护栏；2) 持续改进阶段通过风险评估、对抗测试和整合自主适应已部署的护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1分数，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点。持续改进阶段通过闭环优化在跨域数据上实现了7个百分点的F1提升。

Conclusion: Lattice框架表明，通过迭代优化可以自我构建有效的护栏，为对话AI系统提供自适应安全防护能力。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [12] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 论文提出认知平台工程新范式，通过四层参考架构整合数据收集、智能推理、策略驱动编排和人工交互，实现云原生系统的自主运维和自调整能力。


<details>
  <summary>Details</summary>
Motivation: 传统DevOps自动化方法难以应对云原生系统的规模和动态性，导致运维反应滞后、修复延迟和过度依赖人工经验，需要更智能的平台工程解决方案。

Method: 提出认知平台工程范式，设计四层参考架构：数据收集层、智能推理层、策略驱动编排层和人工体验层，构建持续反馈循环。原型基于Kubernetes、Terraform、Open Policy Agent和ML异常检测实现。

Result: 原型实现显示在平均解决时间、资源效率和合规性方面均有改善，证明将智能嵌入平台运维能够创建弹性、自调整且意图对齐的云环境。

Conclusion: 认知平台工程使云环境具备弹性、自调整和意图对齐能力，未来研究方向包括强化学习、可解释治理和可持续自管理云生态系统。

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [13] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX实现的高性能强化学习环境，用于ARC推理任务，相比Gymnasium实现了38-5439倍的加速，支持大规模并行计算。


<details>
  <summary>Details</summary>
Motivation: 现有的Gymnasium-based RL环境在ARC任务上存在计算瓶颈，限制了实验规模，需要高性能环境来支持大规模强化学习研究。

Method: 采用JAX实现功能化、无状态架构，支持大规模并行化，提供多种ARC数据集、灵活的动作空间、可组合的包装器和配置驱动的可重复性。

Result: 在匹配的批量大小下，相比Gymnasium实现了38-5439倍的加速，峰值吞吐量达到7.9亿步/秒，使之前计算上不可行的大规模RL研究成为可能。

Conclusion: JaxARC是一个开源的高性能RL环境，解决了ARC任务中的计算瓶颈问题，为大规模强化学习研究提供了可行的计算平台。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [14] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 论文提出了Health-ORSC-Bench基准，用于系统评估医疗大语言模型在安全对齐中的过度拒绝和安全完成能力，发现当前模型在安全性和实用性之间存在显著权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大语言模型的安全对齐主要依赖二元拒绝边界，导致对良性查询的过度拒绝或对有害查询的不安全遵从。现有基准只能测量极端情况，无法评估模型在双用途或边界查询中提供安全高层指导而不跨越可操作危害的能力。

Method: 开发了Health-ORSC-Bench基准，包含31,920个良性边界提示，涵盖七个健康类别（如自残、医疗错误信息）。采用自动化流水线结合人工验证，在不同意图模糊度水平下测试模型。评估了30个最先进的大语言模型，包括GPT-5和Claude-4。

Result: 安全优化的模型经常拒绝高达80%的"困难"良性提示，而领域特定模型往往为了实用性牺牲安全性。模型家族和规模显著影响校准：大型前沿模型表现出"安全悲观主义"和更高的过度拒绝，而较小或MoE架构模型表现不同。当前模型难以平衡拒绝和遵从。

Conclusion: Health-ORSC-Bench为校准下一代医疗AI助手提供了严格标准，使其能够实现细致、安全和有帮助的完成。该基准揭示了当前模型在安全对齐方面的局限性，为未来医疗AI的安全优化提供了重要参考。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [15] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 该论文提出了LLM数据审计框架，系统评估大语言模型生成的多模态合成数据的质量和可信度，发现现有评估方法存在缺陷并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM已成为生成多模态数据的有力工具，能将稀缺数据转化为可控资产，但确保合成数据的高质量仍面临挑战。现有研究主要关注生成方法，对数据质量本身关注有限，且多为单模态研究，缺乏跨模态的统一视角。

Method: 提出LLM数据审计框架：1)描述LLM如何生成六种不同模态的数据；2)从质量和可信度两个维度系统分类合成数据的内在评估指标；3)将评估焦点从依赖下游任务性能的外在评估转向数据本身固有属性；4)分析各模态代表性生成方法的实验评估；5)基于发现提出改进建议；6)概述合成数据在不同模态中的实际应用方法。

Result: 通过该评估系统分析发现，当前各模态代表性生成方法的实验评估存在显著缺陷，识别出评估实践中的实质性不足。

Conclusion: 基于研究发现，为社区提供了改进数据生成评估的具体建议，并概述了合成数据在不同模态中的实际应用方法，为LLM生成数据的质量评估提供了系统框架。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [16] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld是一个大规模企业级基准测试，包含1,756个任务，覆盖CRM、ITIL、ERP等六个企业领域，用于评估多模态大语言模型在企业环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对消费级场景（如电商、旅行预订），无法捕捉企业工作流程的复杂性和严谨性。企业系统具有高密度用户界面、严格业务逻辑约束和精确状态一致性要求等特点，当前通用智能体在这些环境中表现不佳。

Method: 采用基于模式的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，合成真实的长周期工作流程。提出基于SQL的确定性验证机制，用严格的状态转换验证替代模糊的视觉匹配。

Result: 实验结果显示，最先进的模型（如GPT-4.1）在EntWorld上的成功率仅为47.61%，远低于人类表现，突显了当前智能体在企业环境中的显著能力差距。

Conclusion: EntWorld揭示了当前通用智能体在企业环境中的局限性，强调了开发领域特定智能体的必要性。该基准测试为下一代企业级数字智能体的开发和评估提供了严格的测试平台。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [17] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: 提出混合集成框架，结合CNN、LSTM深度学习与KNN、XGB传统机器学习，通过投票机制提升心血管疾病预测性能


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，传统预测模型难以泛化到异构数据集和复杂生理模式，需要智能数据驱动的诊断工具

Method: 集成CNN、LSTM深度学习架构与KNN、XGB传统机器学习算法，采用投票机制构建混合集成框架

Result: 在两个Kaggle数据集上分别达到82.30%和97.10%的准确率，在精确率、召回率和F1分数上均有稳定提升

Conclusion: 混合AI框架在心血管疾病预测中表现出鲁棒性和临床潜力，支持联合国可持续发展目标3，促进早期诊断和干预

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [18] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: MMR-Bench是一个多模态路由基准测试，用于解决多模态大语言模型在实际部署中的异构性和效率问题，通过智能路由在不同任务间选择最优模型，在保持准确性的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在架构、对齐策略和效率方面存在异质性，没有单一模型在所有任务上都表现最优。实际部署中，工作负载从轻量级OCR到复杂多模态推理不等，使用单一模型要么在简单实例上过度配置计算资源，要么在困难实例上牺牲准确性。需要解决从纯文本LLM路由扩展到多模态MLLM的挑战。

Method: 提出MMR-Bench统一基准测试：1）提供具有模态感知输入和可变计算预算的受控环境；2）包含OCR、通用VQA和多模态数学推理等广泛的视觉语言任务套件；3）提供强单模型参考、理论上限和代表性路由策略。通过该基准测试研究多模态信号对路由质量的影响。

Result: 实验表明，融入多模态信号能提升路由质量，改善成本-准确性边界。路由系统能以最强单模型约33%的成本超越其准确性。在模型和任务子集上训练的策略能够零样本泛化到新数据集和纯文本基准测试，无需重新调优。

Conclusion: MMR-Bench为研究自适应多模态模型选择和高效MLLM部署提供了基础框架，证明了智能路由在多模态场景中的有效性，能够在显著降低计算成本的同时保持甚至提升系统性能。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [19] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard是一个工业级AI助手，用于自动化解读异构监管文本并将其与公司内部政策对齐，通过HiSACC和ReLACE组件提升检索和生成质量，在企业环境中显著改善回答质量并降低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 监管更新日益频繁和复杂，给跨国制药公司带来沉重负担。合规团队需要跨司法管辖区、格式和机构手动解读不断变化的规则，成本高且容易出错。

Method: 系统通过安全管道摄取异构文档源，采用两个创新组件：HiSACC（分层语义聚合上下文分块）将长文档语义分割为连贯单元，保持非连续部分的一致性；ReLACE（监管列表自适应交叉编码器重排序）基于开源模型的领域适应交叉编码器，联合建模用户查询和检索候选以改进排名相关性。

Result: 企业环境评估显示，RegGuard在相关性、基础性和上下文聚焦方面显著改善回答质量，同时大幅降低幻觉风险。系统架构具备可审计性和可追溯性，包括来源跟踪、访问控制和增量索引。

Conclusion: RegGuard系统能够高效响应不断变化的文档源，适用于任何有严格合规需求的领域，为监管合规提供自动化解决方案。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [20] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: IGFT是一种无需人类对话数据、通过信息增益奖励训练医疗对话AI的新方法，能够在模拟患者对话中学习有效的病史采集策略。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI训练依赖昂贵的人工标注对话数据或静态数据集，需要开发一种无需预收集人类对话、能够通过自主探索学习有效问诊策略的方法。

Method: 结合在线组相对策略优化（GRPO）与信息论奖励，使用信息增益奖励函数追踪临床实体（症状、时间模式、病史）的揭示情况，结合GPT-4o-mini的质量评估计算问题奖励，通过LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B模型。

Result: DeepSeek-R1-Distill-Qwen-7B（IGFT）在Avey数据上F1分数达到0.408（比基础模型提升10.9%），在MIMIC数据上达到0.289（提升12.9%）；Llama-3.1-8B-Instruct（IGFT）分别达到0.384和0.336，均优于OpenAI模型和医疗领域基线模型。

Conclusion: IGFT框架能够有效训练医疗对话AI进行患者访谈和病史采集，无需依赖预收集的人类对话数据，在信息增益奖励的驱动下，模型能够学习提出有针对性的临床问题，显著优于现有方法。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [21] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog框架通过潜在思维空间分析大语言模型的认知过程，发现LLM认知遵循帕累托原则，并提出基于潜在激活的候选优先级策略提升推理性能


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中认知能力如何被调用方面存在局限，需要新的框架来分析LLM的认知过程

Method: 提出UniCog统一框架，作为潜在变量模型，将密集模型激活编码为稀疏、解耦的潜在维度，分析六个先进LLM的认知过程

Result: 发现LLM认知遵循帕累托原则：共享推理核心辅以能力特定特征；推理失败常表现为潜在激活异常强度；潜在信息候选优先级策略可将推理性能提升达7.5%

Conclusion: UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视角，通过潜在激活分析可提升模型推理性能

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [22] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 本文综述了自主实验室（SDL）中的人工智能问题，将SDL自主性构建为智能体-环境交互问题，回顾了闭环实验的主要方法家族，提出了基于能力的分类法，并总结了实际部署中的经验教训和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 自主实验室（SDL）通过连接实验设计、自动化执行和数据驱动决策，为在昂贵操作、噪声延迟反馈、严格可行性安全约束和非平稳性条件下的智能体AI提供了严格测试平台。本文以软物质为代表场景，重点关注实际实验室中出现的人工智能问题。

Method: 将SDL自主性构建为具有明确观察、行动、成本和约束的智能体-环境交互问题；回顾了闭环实验的主要方法家族，包括用于样本高效实验选择的贝叶斯优化和主动学习、用于长视野协议优化的规划和强化学习，以及协调异构仪器和软件的工具使用智能体；提出了基于能力的分类法，按决策视野、不确定性建模、行动参数化、约束处理、故障恢复和人类参与组织系统；合成了基准任务模板和评估指标。

Result: 建立了SDL与AI原则的连接框架；提出了系统化的分类法和评估体系；强调了可验证和溯源感知的策略以支持调试、可重复性和安全操作；总结了实际部署SDL的经验教训。

Conclusion: 自主实验室为AI提供了具有挑战性的测试平台，需要进一步发展多模态表示、校准不确定性、安全探索和共享基准基础设施等开放挑战。本文为SDL研究提供了系统化的框架和评估方法，促进了该领域的标准化和可比性研究。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [23] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 论文提出了一种基于技能图的分层课程学习方法，让智能体在复杂实时控制环境中（黑暗之魂III）能够持续学习而不需要从头训练或覆盖已学行为。


<details>
  <summary>Details</summary>
Motivation: 终身学习智能体应该能够随时间扩展能力，而不需要从头开始重新训练或覆盖先前学习的行为。在复杂实时控制环境中实现这一目标具有挑战性。

Method: 将战斗表示为有向技能图，采用分层课程训练其组件。将控制分解为五个可重用技能：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责进行优化。

Result: 技能分解提高了样本效率，减少了单个策略的负担，并支持选择性后训练。当环境从第一阶段切换到第二阶段时，只需调整部分技能，而上游技能保持可迁移性。仅对两个技能进行针对性微调就能在有限交互预算下快速恢复性能。

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中进化、持续学习的智能体提供了一条实用路径。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [24] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: Sentipolis是一个情感状态化智能体框架，通过PAD情感表示、双速情感动态和情感-记忆耦合解决LLM智能体在社交模拟中的情感遗忘和长期连续性不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在社交模拟中常将情感视为短暂线索，导致情感遗忘和长期情感连续性薄弱，需要更系统化的情感状态管理框架。

Method: 提出Sentipolis框架，包含：1) 连续Pleasure-Arousal-Dominance情感表示；2) 双速情感动态机制；3) 情感-记忆耦合系统。

Result: 在数千次交互中，Sentipolis提升了情感基础行为、沟通能力和情感连续性。效果模型依赖：高容量模型可信度提升，小模型可能下降；情感意识可能轻微降低社会规范遵守度。

Conclusion: Sentipolis通过情感状态化智能体改善了社交模拟的情感连续性，揭示了情感驱动行为与社会规范遵守之间的人类化张力，支持研究累积社交动态如联盟形成和关系渐变。

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [25] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 研究发现心理健康领域专家对AI生成回复的评估存在系统性分歧，尤其是在自杀自伤等安全关键问题上，专家间一致性远低于可接受水平，这种分歧源于不同的临床框架而非测量误差。


<details>
  <summary>Details</summary>
Motivation: 验证"人类反馈学习"的基本假设：专家判断经过适当聚合后能产生有效的训练和评估AI系统的真实标签。在心理健康这一安全风险高的领域，专家共识尤为重要。

Method: 三位认证精神科医生使用校准的评分标准独立评估LLM生成的回复，计算组内相关系数(ICC)和Krippendorff's α等可靠性指标，并进行定性访谈了解分歧原因。

Result: 专家间可靠性极低(ICC 0.087-0.295)，低于可接受阈值；自杀自伤类回复分歧最大；一个因素甚至出现负可靠性(α=-0.203)；分歧源于三种不同的临床框架：安全优先、参与为中心和文化敏感。

Conclusion: 专家分歧是原则性的社会技术现象，而非测量误差；聚合标签只是算术妥协，抹杀了专业哲学；建议从基于共识的聚合转向能够保留和学习专家分歧的对齐方法。

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [26] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE框架通过多种进化策略（MCTS和IGR）和结构化测试平台生成，在Verilog硬件设计任务中实现了自动化，在功能正确性和优化方面均达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: Verilog硬件设计周期劳动密集且需要大量领域专业知识，现有大型语言模型由于训练数据有限和顺序推理特性，难以捕捉硬件系统的严格形式逻辑和并发特性。

Method: 提出EvolVE框架，分析多种进化策略：蒙特卡洛树搜索（MCTS）用于最大化功能正确性，想法引导精炼（IGR）用于优化；采用结构化测试平台生成（STG）加速进化过程；引入IC-RTL基准测试套件，针对行业规模问题。

Result: 在VerilogEval v2上达到98.1%，在RTLLM v2上达到92%；在行业规模的IC-RTL套件上，超越竞赛参与者参考实现，在哈夫曼编码中将PPA（功耗、性能、面积）乘积降低66%，所有问题的几何平均值降低17%。

Conclusion: EvolVE框架通过结合多种进化策略和结构化测试平台生成，成功解决了硬件设计自动化的挑战，在功能正确性和优化方面均实现了最先进的性能。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [27] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL推理与调试基准，包含469个语法错误查询和516个语义错误查询，通过自动注入真实错误构建，评估显示现有LLM在复杂SQL调试上表现不佳（最佳模型准确率仅36%）。


<details>
  <summary>Details</summary>
Motivation: 企业数据工程中SQL至关重要，但即使经验丰富的开发者和先进LLM也难以一次性生成完全正确的SQL代码，通常需要多次调试迭代，缺乏专门的企业级SQL调试基准。

Method: 采用自动化构建流程：1) 使用逆向工程在大规模SQL代码中系统注入真实错误，实现可扩展的多样化基准生成；2) 设计面向企业环境的免执行评估框架，提供快速、准确、资源高效的评估。

Result: OurBench包含469个带明确错误信息的语法错误查询和516个语义错误查询，查询平均超过140行且具有深广的抽象语法树。评估近30个LLM显示显著性能差距：最佳模型Claude-4-Sonnet在语法错误上仅36.46%准确率，语义错误上32.17%，大多数模型低于20%。

Conclusion: 企业级SQL调试对现有LLM仍具挑战性，研究探索了四种解决方案策略，识别了关键挑战，并为LLM在企业SQL调试中的应用指明了有前景的方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [28] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: 研究提出截止时间感知控制方法，通过强化学习优化家用浸入式热水器能耗，在指定时间达到目标温度的同时最小化能量消耗。


<details>
  <summary>Details</summary>
Motivation: 传统家用浸入式热水器在冬季通常连续运行，只追求快速加热而忽视效率，忽略了可预测的需求窗口和环境热损失。需要一种能在指定截止时间达到目标温度的同时最小化能耗的智能控制方法。

Method: 开发了Gymnasium仿真环境，模拟具有一阶热损失的浸入式热水器，每120秒执行0W或6000W的离散开关动作。比较了三种方法：时间最优的bang-bang基线控制、零样本蒙特卡洛树搜索规划器、以及近端策略优化强化学习策略。

Result: 在2小时（60步）时间范围内，PPO策略仅消耗3.23千瓦时能量，相比bang-bang控制的4.37-10.45千瓦时和MCTS的4.18-6.46千瓦时表现最优。在典型场景中（50kg水质量，20°C环境温度，60°C目标温度），PPO比bang-bang节能54%，比MCTS节能33%。

Conclusion: 学习型截止时间感知控制能在相同物理条件下显著降低能耗，规划器无需训练即可提供部分节能效果，而训练后的学习策略在推理时成本几乎为零，为智能热水器控制提供了有效解决方案。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [29] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA是一个高效的混合智能体框架，通过动态路由机制解决传统MoA方法的成本和延迟问题，使用轻量级评分器预筛选模型，结合混合评估器进行后验修正，实现成本降低89.8%和延迟减少63.6%。


<details>
  <summary>Details</summary>
Motivation: 传统混合智能体(MoA)方法通过分层协作提升LLM性能，但其密集拓扑结构导致成本高、延迟大。现有方法虽然使用LLM评判器过滤响应，但仍需所有模型先进行推理再评判，无法有效降低成本。同时缺乏模型选择标准，在大规模模型池中面临成本过高和上下文限制问题。

Method: RouteMoA采用动态路由框架：1) 轻量级评分器根据查询预测粗略性能，预筛选高潜力候选模型子集，避免完整推理；2) 混合评估器通过轻量级自评估和交叉评估对现有模型输出进行后验修正；3) 模型排名机制综合考虑性能、成本和延迟进行模型选择。

Result: RouteMoA在不同任务和模型池规模下均优于传统MoA方法，在大规模模型池中实现成本降低89.8%，延迟减少63.6%，同时保持性能优势。

Conclusion: RouteMoA通过动态路由机制有效解决了传统混合智能体方法的成本和延迟问题，实现了高效的大规模模型协作，为实际部署提供了可行的解决方案。

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [30] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert是一个基于多LLM推理校准的罕见病早期筛查系统，通过整合10个LLM的推理信号，训练出可本地部署的单一模型，在158,666例真实世界数据上实现了0.917的AUC，优于现有最佳机器学习模型和所有评估的LLM。


<details>
  <summary>Details</summary>
Motivation: 罕见病的漏诊和延迟诊断是医疗领域的重大挑战。现有初级保健分诊流程在初次临床就诊时无法可靠识别罕见病患者，需要一种通用筛查系统来减少诊断延迟。

Method: 开发了RareAlert系统，整合10个LLM生成的推理信号，使用机器学习进行校准和加权，并将对齐的推理蒸馏到单个可本地部署的模型中。使用RareBench数据集（158,666例病例，覆盖33个Orphanet疾病类别和7,000多种罕见病）进行开发和评估。

Result: RareAlert在独立测试集上实现了0.917的AUC，优于最佳机器学习集成模型和所有评估的LLM（包括GPT-5、DeepSeek-R1、Claude-3.7-Sonnet等）。证明了LLM医学推理的多样性以及在高不确定性临床任务中对齐这种推理的有效性。

Conclusion: 罕见病识别可以重新概念化为应用于普通患者群体的通用不确定性解决过程。通过将校准推理整合到单一模型中，RareAlert实现了准确、保护隐私且可扩展的罕见病风险筛查，适合大规模本地部署。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [31] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: DeepPlanning是一个针对实际长时程智能体规划的挑战性基准测试，包含多日旅行规划和多产品购物任务，要求主动信息获取、局部约束推理和全局约束优化


<details>
  <summary>Details</summary>
Motivation: 现有智能体评估虽然转向长时程任务，但大多数基准测试仍强调局部、步骤级推理，而非需要真正规划能力的全局约束优化（如时间和财务预算）。同时，现有LLM规划基准未能充分体现现实世界中典型的主动信息收集和细粒度局部约束

Method: 引入DeepPlanning基准，包含多日旅行规划和多产品购物两类任务，这些任务要求智能体进行主动信息获取、局部约束推理和全局约束优化

Result: 评估显示即使是前沿的智能体LLM在这些问题上也表现不佳，突显了可靠的显式推理模式和并行工具使用对于实现更好的效果-效率权衡的重要性

Conclusion: DeepPlanning基准揭示了当前智能体LLM在长规划时程上的局限性，错误分析指出了改进方向，开源代码和数据以支持未来研究

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [32] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 成功条件化（success conditioning）是一种通过收集轨迹、识别成功轨迹并模仿其行动来改进策略的广泛使用技术。本文证明该方法实际上精确解决了一个信任域优化问题，在数据自动确定的χ²散度约束下最大化策略改进。


<details>
  <summary>Details</summary>
Motivation: 成功条件化技术以多种名称出现（如拒绝采样+SFT、目标条件RL、决策变换器等），但其解决的优化问题本质一直不明确。本文旨在揭示该方法的数学基础，阐明其作为保守改进算子的理论性质。

Method: 通过理论分析证明成功条件化精确解决了一个信任域优化问题：在χ²散度约束下最大化策略改进，约束半径由数据自动确定。建立了相对策略改进、策略变化幅度和动作影响三者相等的恒等式。

Result: 成功条件化被证明是一个保守改进算子，不会降低性能或引发危险的分布偏移。当失败时，它会通过几乎不改变策略来可观察地失败。理论还应用于常见的回报阈值化实践，显示其能放大改进但可能偏离真实目标。

Conclusion: 成功条件化具有坚实的数学基础，解决了特定的信任域优化问题。它提供了一种安全、保守的策略改进方法，其失败模式可观察，为实际应用提供了理论指导。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [33] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究LLM智能体在未知测试领域中的泛化能力，发现环境状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，而非领域真实性或文本相似性。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在狭窄环境集上进行后训练，但部署到更广泛的未知领域。本研究旨在探索当最终测试领域未知时，智能体后训练面临的挑战，分析哪些RL环境属性和建模选择对跨域性能影响最大。

Method: 首先识别与跨域泛化强相关的两个环境轴：状态信息丰富度（agent需要处理的信息量）和规划复杂度（通过基础策略下的目标可达性和轨迹长度估计）。提出随机化技术：在状态中添加少量分散注意力的目标无关特征来增加丰富度而不改变任务。同时研究建模选择：SFT预热或中期训练的影响，以及RL期间开启逐步思考的作用。

Result: 发现状态信息丰富度和规划复杂度是跨域泛化的主要决定因素，而领域真实性和文本相似性不是关键因素。增加状态信息丰富度能有效提高跨域鲁棒性。SFT预热或中期训练虽能防止灾难性遗忘，但会削弱对未包含在中期训练数据中的领域的泛化能力。RL期间的逐步思考虽不总是提高域内性能，但对保持泛化能力至关重要。

Conclusion: 跨域泛化主要受环境状态信息丰富度和规划复杂度影响，而非领域真实性。通过添加目标无关特征增加状态丰富度是提高泛化能力的有效方法。在训练策略上，需要在防止灾难性遗忘和保持泛化能力之间取得平衡，逐步思考是保持泛化能力的重要机制。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [34] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式，通过Yunjue Agent系统在开放环境中自我演化工具集，利用任务交互反馈持续扩展能力边界，无需真实标签监督。


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在开放环境中面临挑战：任务分布持续漂移、外部监督稀缺，依赖静态工具集或离线训练导致能力边界僵化且未知。

Method: 提出In-Situ Self-Evolving范式，将序列任务交互视为连续经验流，将短期执行反馈提炼为长期可重用能力。具体实现Yunjue Agent系统，通过工具演化作为能力扩展关键路径，采用Parallel Batch Evolution策略优化演化效率。

Result: 在五个不同基准测试的零起点设置下，相比专有基线获得显著性能提升。补充的暖启动评估证实积累的通用知识可无缝迁移到新领域。提出了监测演化收敛的新指标。

Conclusion: In-Situ Self-Evolving范式使智能体能够在开放环境中持续自我演化，工具演化作为可验证的二元反馈信号为能力扩展提供了有效路径。开源代码库、系统轨迹和演化工具以促进弹性自演化智能研究。

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [35] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: 提出Think-Augmented Function Calling (TAFC)框架，通过函数和参数级别的显式推理增强LLM函数调用准确性，无需修改模型架构


<details>
  <summary>Details</summary>
Motivation: 当前LLM在函数调用中缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数。现有方法如思维链提示在智能体级别操作，无法为单个函数参数提供细粒度的推理指导。

Method: 提出TAFC框架：1) 引入通用的"think"参数增强，让模型阐述决策过程；2) 动态优化参数描述以提高推理质量；3) 对复杂参数基于复杂度评分自动触发细粒度推理；4) 提出推理引导优化以对齐人类期望。该方法无需修改现有LLM架构，保持完全API兼容性。

Result: 在ToolBench上对专有和开源模型的评估显示，TAFC在多参数函数的参数生成准确性和推理连贯性方面有显著改进，同时为调试AI智能体行为提供了增强的可解释性。

Conclusion: TAFC通过显式推理机制有效解决了LLM函数调用中参数生成的透明度问题，提高了复杂函数调用的准确性和可解释性，同时保持与现有系统的兼容性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [36] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的灾害预警系统，通过整合多源数据和个性化行动建议，将传统预警转化为可执行行动，提高防护措施执行率和响应效率。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统虽然能快速传播警报，但往往无法触发及时的防护行动，导致可预防的损失和不公平现象。随着气候相关灾害加剧，需要更有效的预警机制。

Method: 开发Climate RADAR系统，整合气象、水文、脆弱性和社会数据形成综合风险指数，使用带有护栏的大型语言模型为公民、志愿者和市政部门提供个性化行动建议。

Result: 通过模拟、用户研究和市政试点评估显示，系统提高了防护行动执行率、减少了响应延迟、增强了可用性和信任度。

Conclusion: Climate RADAR通过结合预测分析、行为科学和负责任AI，推进了以人为本、透明和公平的早期预警系统，为符合要求的灾害韧性基础设施提供了实用路径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [37] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 生成式AI在模仿作家风格方面已超越人类专家，特别是在经过微调后，专家评委也更倾向于AI作品，这引发了创意写作领域的身份危机


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念——创意写作长期以来被认为是人类独有的能力，需要机器无法复制的独特声音和风格。研究旨在探索生成式AI是否能够真正模仿著名作家的写作风格，以及这对创意写作领域意味着什么

Method: 行为实验设计：28位MFA创意写作专家与3个大型语言模型竞争模仿50位备受好评的作家风格。采用盲审成对比较，由28位专家评委和131位普通评委进行评估。测试两种条件：上下文提示和基于作者完整作品微调

Result: 在上下文提示条件下，专家评委82.7%更倾向于人类写作；但经过微调后，这一偏好逆转为62%倾向于AI写作。普通评委则始终更倾向于AI作品。专家作家访谈显示，他们对AI写作的偏好引发了身份危机，削弱了审美自信，质疑了"好写作"的定义

Conclusion: 研究结果挑战了关于AI创意局限性的传统论述，提出了关于创意劳动未来的根本性问题。AI在模仿写作风格方面的能力已经达到甚至超越人类专家水平，这对创意写作领域的专业身份和审美标准构成了深刻挑战

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [38] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: 论文提出DynTS方法，通过识别推理过程中的关键决策token并仅保留其KV缓存，来优化大型推理模型的效率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成推理轨迹时会产生大量内存占用和计算开销，成为效率瓶颈。研究发现只有部分决策关键token真正影响最终答案，其他token贡献很小

Method: 提出动态思维token选择(DynTS)方法，使用注意力图分析推理轨迹的影响，识别决策关键token，在推理时仅保留这些关键token的KV缓存状态，剔除冗余条目

Result: 通过仅保留关键决策token的KV缓存，显著减少了内存占用和计算开销，优化了推理效率

Conclusion: DynTS方法通过选择性保留关键token的KV缓存，有效解决了大型推理模型推理轨迹带来的效率瓶颈问题

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [39] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: DeepMed提出了一种针对医学领域的深度研究模型，通过解决任务特性和工具使用扩展两个关键差距，在医学基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学推理模型受限于参数化知识，容易遗忘和产生幻觉。通用深度研究模型虽然能在一般领域基于工具验证证据，但直接应用于医学领域效果有限，主要存在两个差距：任务特性差距（医学问题需要在知识密集的临床环境中解释证据）和工具使用扩展差距（盲目扩展工具调用会引入噪声，干扰敏感的医学推理）。

Method: 提出DeepMed模型：1）数据方面，采用多跳医学搜索QA合成方法，支持模型在医学环境中应用深度研究范式；2）训练方面，引入难度感知的回合惩罚机制，抑制过度的工具调用增长；3）推理方面，加入监控器帮助在可控步骤内验证假设，避免上下文污染。

Result: 在七个医学基准测试中，DeepMed相比基础模型平均提升9.79%，并且超越了更大的医学推理和深度研究模型。

Conclusion: 通过针对医学领域特点设计的深度研究方法，DeepMed有效解决了通用深度研究模型在医学应用中的局限性，显著提升了医学推理性能。

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [40] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: MOSAIC框架通过动态生成包含多达20个应用导向约束的数据集，对LLM指令遵循能力进行模块化评估，发现合规性受约束类型、数量和位置影响显著。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以反映真实世界使用情况，且无法将合规性与任务成功分离，需要新的评估框架来可靠地确保LLM遵循复杂指令。

Method: 提出MOSAIC框架，使用动态生成的数据集，包含多达20个应用导向的生成约束，对五种不同家族的LLM进行模块化评估。

Result: 合规性不是单一能力，而是随约束类型、数量和位置显著变化；揭示了模型特定弱点、指令间的协同与冲突交互，以及首因效应和近因效应等位置偏差。

Conclusion: 这些细粒度洞察对于诊断模型失败和开发需要严格遵循复杂指令的系统中的更可靠LLM至关重要。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [41] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 研究发现训练稳定性与生成质量并不一致：稳定的参数轨迹会导致模型集中在有限的模式上，降低生成熵，产生重复性输出，尽管损失函数平滑收敛。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为训练稳定性是大型语言模型可靠优化的前提，但本文旨在分析训练稳定性如何影响生成分布，探讨稳定性是否真正保证生成质量。

Method: 理论分析标准最大似然训练下稳定参数轨迹的数学特性，并设计基于反馈的训练框架来稳定内部生成统计量，在不同架构和随机种子下进行实证验证。

Result: 稳定训练导致模型近似最小化前向KL散度，同时隐式降低生成熵，使模型集中在有限的模式上，产生低熵输出和重复性行为，尽管损失函数平滑收敛。

Conclusion: 优化稳定性与生成表达能力并不内在一致，稳定性本身不足以作为生成质量的指标，需要更全面的评估标准来确保模型的生成多样性。

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [42] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM和逻辑求解器的新方法，通过迭代反馈机制补充缺失的常识关系，提升复杂推理问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在形式推理方面表现出色，但在需要复杂证明规划的问题上常常失效。传统逻辑求解器虽然推理效率更高，但假设所有相关事实都已提供，无法处理缺失的常识关系。因此需要一种能平衡神经和符号元素的方法。

Method: 提出了一种新颖的迭代方法：使用逻辑求解器的反馈来增强逻辑问题，通过LLM提供常识关系。采用搜索程序遍历潜在的常识假设，最大化找到有用事实的机会，同时保持成本可控。

Result: 在一组纯逻辑推理数据集上（其中部分常识信息已被移除），该方法相比现有技术始终取得显著改进，证明了在人类语境中平衡神经和符号元素的价值。

Conclusion: 通过结合LLM的常识推理能力和逻辑求解器的形式推理效率，提出的迭代反馈方法能有效解决需要复杂证明规划和常识补充的推理问题，展示了神经符号混合方法的优势。

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [43] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 该研究提出了一种基于人类评估的方法来评估大型语言模型在心理健康对话中的表现，发现LLMs在认知支持方面表现可靠，但在情感共鸣方面存在不稳定，揭示了认知-情感差距。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机日益严重，存在治疗缺口和合格治疗师短缺的问题，大型语言模型作为可扩展的支持途径具有潜力。然而，LLMs的可靠性、治疗相关性和与人类标准的对齐仍面临挑战，需要建立评估框架来确保其在心理健康应用中的质量。

Method: 研究提出了一种基于人类评估的方法：1) 从真实世界场景数据集中整理500个心理健康对话；2) 评估9个不同LLMs（包括闭源和开源模型）生成的响应；3) 由两位经过精神病学培训的专家独立使用5点李克特量表对每个响应进行评分；4) 使用包含6个属性的综合评估框架，重点关注认知支持和情感共鸣两个维度。

Result: 分析显示：1) LLMs在认知可靠性方面表现强劲，能够提供安全、连贯且临床适当的信息；2) 但在情感对齐方面表现不稳定；3) 闭源模型（如GPT-4o）提供更平衡的治疗响应；4) 开源模型表现出更大的变异性和情感平淡；5) 揭示了持续的认知-情感差距。

Conclusion: 研究强调了需要建立具有失败意识、基于临床的评估框架，在心理健康导向的LLMs中优先考虑关系敏感性而不仅仅是信息准确性。提倡采用以人为中心的平衡评估协议，重点关注治疗敏感性，并为心理健康导向的对话AI提供负责任设计和临床监督的框架。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [44] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner是一个多模态模型家族，通过学习工具使用作为通用推理技能而非特定工具或显式监督行为，实现了强大的工具自适应和泛化能力，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 人类在面对超出自身能力的问题时会依赖工具，这为提高多模态大语言模型的视觉推理能力提供了一个有前景的范式。有效的推理需要知道使用哪些工具、何时调用它们以及如何在多步骤中组合它们，即使面对新工具或新任务时也是如此。

Method: AdaReasoner通过三个关键组件实现：(1)可扩展的数据整理流程，让模型接触长视野、多步骤的工具交互；(2)Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和序列；(3)自适应学习机制，动态调节工具使用频率。

Result: AdaReasoner表现出强大的工具自适应和泛化行为：自主采用有益工具、抑制无关工具、根据任务需求调整工具使用频率，尽管从未被明确训练这样做。这些能力转化为在多个挑战性基准测试中的最先进性能，将7B基础模型平均提升24.9%，并在VSP和Jigsaw等多个任务上超越GPT-5等强大专有系统。

Conclusion: AdaReasoner通过学习工具使用作为通用推理技能，实现了有效的工具自适应和泛化，为多模态大语言模型的视觉推理能力提升提供了有前景的解决方案，在多个任务上达到最先进水平。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [45] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE是一个可扩展的基于量规的LLM训练和评估框架，显著降低医疗领域量规开发成本，同时保持评估质量。


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，基于量规评估开放式LLM响应至关重要，但创建高质量、领域特定的量规需要大量专家时间和开发成本，难以规模化。

Method: 开发Health-SCORE框架，这是一个通用且可扩展的基于量规的训练和评估系统，显著减少量规开发成本，同时保持性能。

Result: Health-SCORE在开放式医疗任务中实现与人工创建量规相当的评估质量，同时显著降低开发工作量，使基于量规的评估和训练更具可扩展性。

Conclusion: Health-SCORE框架不仅提供评估功能，还可作为结构化奖励信号指导强化学习，并通过上下文学习直接融入提示中提高响应质量，为医疗领域LLM评估和训练提供了实用解决方案。

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [46] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: 本文提出了一种AI辅助的药物设计方法，通过E3连接酶导向的分子胶促进Aβ-42的靶向降解，用于阿尔茨海默病治疗。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病以Aβ-42的病理性积累为特征，导致突触功能障碍和神经退行性变。虽然细胞外淀粉样斑块已被广泛研究，但越来越多的证据表明细胞内Aβ-42是疾病进展的早期和毒性驱动因素。需要开发新的治疗方法来靶向降解Aβ-42。

Method: 采用AI辅助药物设计方法，通过结构建模、ADMET筛选和对接评估Aβ-42与三种E3连接酶（CRBN、VHL、MDM2）的三元复合物形成潜力。开发了连接酶条件化连接树变分自编码器（LC-JT-VAE），结合蛋白质序列嵌入和扭转角感知分子图，生成连接酶特异性小分子。

Result: 生成模型能够产生化学有效、新颖且靶向特异性的分子胶，能够促进Aβ-42的降解。该方法为设计UPS靶向疗法提供了有前景的框架。

Conclusion: 这种集成方法为神经退行性疾病设计UPS靶向疗法提供了一个有前景的框架，通过AI辅助生成分子胶来促进Aβ-42的靶向降解。

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [47] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域、4个维度，评估了30多个领先模型，揭示了感知和推理的扩展规律、预测中的语义理解与数值预测脱节等问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在现实世界中无处不在且对关键应用至关重要，但现有通用模型基准测试中缺乏时间序列维度。为了填补这一空白，需要创建一个全面的时间序列推理基准来评估通用模型的实践问题解决能力。

Method: 引入TSRBench多模态基准测试，包含4125个来自14个领域的问题，分为感知、推理、预测和决策4个主要维度，包含15个任务评估基本推理能力。对30多个领先的专有和开源LLM、VLM和TSLLM进行了广泛实验。

Result: 研究发现：1）扩展规律适用于感知和推理但在预测中失效；2）强大的推理能力不能保证准确的上下文感知预测，表明语义理解和数值预测之间存在脱节；3）尽管时间序列的文本和视觉表示具有互补性，但当前多模态模型未能有效融合它们以获得相互性能增益。

Conclusion: TSRBench提供了一个标准化评估平台，不仅突显了现有挑战，还为推进通用模型的发展提供了有价值的见解。代码和数据集已开源。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>
