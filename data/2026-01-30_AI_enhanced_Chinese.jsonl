{"id": "2601.19955", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.19955", "abs": "https://arxiv.org/abs/2601.19955", "authors": ["Jean-Marc Fellous", "Gert Cauwenberghs", "Cornelia Ferm\u00fcller", "Yulia Sandamisrkaya", "Terrence Sejnowski"], "title": "NeuroAI and Beyond", "comment": "53 pages, 5 figures, extended appendix", "summary": "Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u57fa\u4e8e2025\u5e748\u6708\u7814\u8ba8\u4f1a\uff0c\u63a2\u8ba8\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u534f\u540c\u53d1\u5c55\uff0c\u63d0\u51faNeuroAI\u6982\u5ff5\uff0c\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u6539\u8fdbAI\u7b97\u6cd5\uff0c\u540c\u65f6\u6df1\u5316\u5bf9\u751f\u7269\u795e\u7ecf\u8ba1\u7b97\u7684\u7406\u89e3\u3002", "motivation": "\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u5728\u8fc7\u53bb\u51e0\u5e74\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e24\u8005\u4e4b\u95f4\u7684\u8fde\u63a5\u4ecd\u7136\u677e\u6563\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6574\u5408\u8fd9\u4e24\u4e2a\u9886\u57df\uff0c\u5f00\u53d1\u51fa\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4eba\u5de5\u667a\u80fd\uff08NeuroAI\uff09\uff0c\u4ee5\u63d0\u5347AI\u7b97\u6cd5\u7684\u8303\u56f4\u548c\u6548\u7387\uff0c\u540c\u65f6\u6539\u53d8\u5bf9\u751f\u7269\u795e\u7ecf\u8ba1\u7b97\u7684\u7406\u89e3\u65b9\u5f0f\u3002", "method": "\u57fa\u4e8e2025\u5e748\u6708\u4e3e\u529e\u7684\u7814\u8ba8\u4f1a\uff0c\u805a\u7126\u4e8e\u5177\u8eab\u8ba4\u77e5\u3001\u8bed\u8a00\u4e0e\u901a\u4fe1\u3001\u673a\u5668\u4eba\u5b66\u3001\u4eba\u7c7b\u4e0e\u673a\u5668\u5b66\u4e60\u4ee5\u53ca\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u7b49\u5b50\u9886\u57df\uff0c\u8bc4\u4f30\u5f53\u524d\u8fdb\u5c55\u5e76\u63a2\u7d22\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002\u6536\u96c6\u4e86\u591a\u4f4d\u9876\u5c16\u7814\u7a76\u4eba\u5458\u7684\u4e2a\u4eba\u89c2\u70b9\uff0c\u5e76\u9644\u4e0a\u7814\u7a76\u4eba\u5458\u548c\u5b66\u5458\u8fdb\u884c\u7684SWOT\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86NeuroAI\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5f3a\u8c03\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u534f\u540c\u6f5c\u529b\u3002\u901a\u8fc7\u7814\u8ba8\u4f1a\u8ba8\u8bba\u548c\u4e13\u5bb6\u610f\u89c1\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u548c\u672a\u6765\u7684\u5408\u4f5c\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86SWOT\u5206\u6790\u6765\u8bc4\u4f30NeuroAI\u7684\u4f18\u52bf\u3001\u52a3\u52bf\u3001\u673a\u4f1a\u548c\u5a01\u80c1\u3002", "conclusion": "\u5021\u5bfc\u53d1\u5c55NeuroAI\u2014\u2014\u4e00\u79cd\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u8ba4\u4e3a\u8fd9\u4e0d\u4ec5\u80fd\u663e\u8457\u63d0\u5347AI\u7b97\u6cd5\u7684\u8303\u56f4\u548c\u6548\u7387\uff0c\u8fd8\u80fd\u6539\u53d8\u6211\u4eec\u5bf9\u751f\u7269\u795e\u7ecf\u8ba1\u7b97\u7684\u7406\u89e3\u65b9\u5f0f\u3002\u901a\u8fc7\u6574\u5408\u4e24\u4e2a\u9886\u57df\u7684\u4f18\u52bf\uff0c\u6709\u671b\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u5171\u540c\u8fdb\u6b65\u3002"}}
{"id": "2601.20014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20014", "abs": "https://arxiv.org/abs/2601.20014", "authors": ["Shuhui Qu"], "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning", "comment": null, "summary": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality.", "AI": {"tldr": "SQ-BCP\u662f\u4e00\u79cd\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u8fdb\u884c\u63a8\u7406\u65f6\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u8868\u793a\u524d\u63d0\u6761\u4ef6\u72b6\u6001\u3001\u81ea\u6211\u67e5\u8be2\u548c\u6865\u63a5\u5047\u8bbe\u6765\u89e3\u51b3LLM\u5728\u7f3a\u5931\u5173\u952e\u524d\u63d0\u65f6\u4ea7\u751f\u5e7b\u89c9\u6216\u8fdd\u53cd\u7ea6\u675f\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u8fdb\u884c\u63a8\u7406\u65f6\u89c4\u5212\u7ecf\u5e38\u5931\u8d25\uff1a\u5f53\u4efb\u52a1\u5173\u952e\u524d\u63d0\u6761\u4ef6\u5728\u67e5\u8be2\u65f6\u672a\u6307\u5b9a\u65f6\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5e7b\u89c9\u7f3a\u5931\u4e8b\u5b9e\u6216\u4ea7\u751f\u8fdd\u53cd\u786c\u7ea6\u675f\u7684\u8ba1\u5212\u3002", "method": "\u5f15\u5165\u81ea\u6211\u67e5\u8be2\u53cc\u5411\u5206\u7c7b\u89c4\u5212(SQ-BCP)\uff0c\u663e\u5f0f\u8868\u793a\u524d\u63d0\u6761\u4ef6\u72b6\u6001(Sat/Viol/Unk)\uff0c\u901a\u8fc7(i)\u9488\u5bf9\u6027\u7684\u81ea\u6211\u67e5\u8be2\u5230\u9884\u8a00\u673a/\u7528\u6237\u6216(ii)\u6865\u63a5\u5047\u8bbe\uff08\u901a\u8fc7\u989d\u5916\u52a8\u4f5c\u5efa\u7acb\u7f3a\u5931\u6761\u4ef6\uff09\u6765\u89e3\u6790\u672a\u77e5\u72b6\u6001\u3002SQ-BCP\u6267\u884c\u53cc\u5411\u641c\u7d22\uff0c\u5e76\u8c03\u7528\u57fa\u4e8e\u56de\u62c9\u7684\u9a8c\u8bc1\u5668\u4f5c\u4e3a\u76ee\u6807\u517c\u5bb9\u6027\u7684\u5206\u7c7b\u8bc1\u4e66\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u5206\u6570\u8fdb\u884c\u6392\u5e8f\u548c\u526a\u679d\u3002", "result": "\u5728WikiHow\u548cRecipeNLG\u4efb\u52a1\u4e2d\uff0c\u5f53\u9884\u6761\u4ef6\u88ab\u4fdd\u7559\u65f6\uff0cSQ-BCP\u5c06\u8d44\u6e90\u8fdd\u89c4\u7387\u964d\u4f4e\u523014.9%\u548c5.8%\uff08\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u768426.0%\u548c15.7%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u53c2\u8003\u8d28\u91cf\u3002", "conclusion": "SQ-BCP\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u672a\u77e5\u524d\u63d0\u6761\u4ef6\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u5212\u8fdd\u53cd\u7ea6\u675f\u7684\u60c5\u51b5\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u5f53\u9a8c\u8bc1\u5668\u6210\u529f\u4e14\u786c\u7ea6\u675f\u901a\u8fc7\u786e\u5b9a\u6027\u68c0\u67e5\u65f6\uff0c\u63a5\u53d7\u7684\u8ba1\u5212\u4e0e\u76ee\u6807\u8981\u6c42\u517c\u5bb9\u3002"}}
{"id": "2601.20021", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20021", "abs": "https://arxiv.org/abs/2601.20021", "authors": ["Shuhui Qu"], "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints", "comment": null, "summary": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u7cca\u8303\u7574\u8bba\u89c4\u5212(FCP)\u65b9\u6cd5\uff0c\u5904\u7406\u81ea\u7136\u8bed\u8a00\u89c4\u5212\u4e2d\u7684\u6a21\u7cca\u8c13\u8bcd\u95ee\u9898\uff0c\u901a\u8fc7[0,1]\u533a\u95f4\u6807\u6ce8\u52a8\u4f5c\u8d28\u91cf\uff0c\u4f7f\u7528\u0141ukasiewicz t-\u8303\u6570\u7ec4\u5408\u8ba1\u5212\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u6267\u884c\u6027\u68c0\u67e5", "motivation": "\u73b0\u6709\u8303\u7574\u8bba\u89c4\u5212\u5668\u5c06\u9002\u7528\u6027\u89c6\u4e3a\u4e8c\u503c\u5224\u65ad\uff0c\u9700\u8981\u9608\u503c\u5904\u7406\uff0c\u8fd9\u4f1a\u62b9\u6740\u6709\u610f\u4e49\u7684\u533a\u522b\uff0c\u4e14\u65e0\u6cd5\u8ddf\u8e2a\u591a\u6b65\u8ba1\u5212\u4e2d\u7684\u8d28\u91cf\u9000\u5316\u3002\u81ea\u7136\u8bed\u8a00\u89c4\u5212\u5e38\u6d89\u53ca\u6a21\u7cca\u8c13\u8bcd\uff08\u5982\"\u5408\u9002\u7684\u66ff\u4ee3\u54c1\"\u3001\"\u8db3\u591f\u7a33\u5b9a\"\uff09\uff0c\u5176\u6ee1\u8db3\u5ea6\u672c\u8d28\u4e0a\u662f\u5206\u7ea7\u7684", "method": "FCP\u4e3a\u6bcf\u4e2a\u52a8\u4f5c\uff08\u6001\u5c04\uff09\u6807\u6ce8[0,1]\u533a\u95f4\u7684\u7a0b\u5ea6\u503c\uff0c\u4f7f\u7528\u0141ukasiewicz t-\u8303\u6570\u7ec4\u5408\u8ba1\u5212\u8d28\u91cf\uff0c\u901a\u8fc7\u62c9\u56de\u9a8c\u8bc1\u4fdd\u6301\u4e25\u683c\u7684\u6267\u884c\u6027\u68c0\u67e5\u3002\u4f7f\u7528LLM\u8fdb\u884ck\u6837\u672c\u4e2d\u4f4d\u6570\u805a\u5408\u4ece\u8bed\u8a00\u4e2d\u83b7\u53d6\u5206\u7ea7\u9002\u7528\u6027\uff0c\u652f\u6301\u57fa\u4e8e\u5269\u4f59\u7684\u540e\u5411\u9700\u6c42\u7684\u4e2d\u95f4\u76f8\u9047\u641c\u7d22", "result": "\u5728RecipeNLG-Subs\uff08\u57fa\u4e8eRecipeNLG\u3001Recipe1MSubs\u548cFoodKG\u6784\u5efa\u7684\u98df\u8c31\u66ff\u4ee3\u89c4\u5212\u57fa\u51c6\uff09\u4e0a\uff0cFCP\u76f8\u6bd4LLM-only\u548cReAct\u98ce\u683c\u57fa\u7ebf\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u786c\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4e0e\u7ecf\u5178PDDL3\u89c4\u5212\u5668\u4fdd\u6301\u7ade\u4e89\u529b", "conclusion": "FCP\u6210\u529f\u5730\u5c06\u6a21\u7cca\u903b\u8f91\u4e0e\u8303\u7574\u8bba\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u5904\u7406\u81ea\u7136\u8bed\u8a00\u89c4\u5212\u4e2d\u7684\u6a21\u7cca\u8c13\u8bcd\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e25\u683c\u6267\u884c\u6027\u68c0\u67e5\u7684\u540c\u65f6\uff0c\u80fd\u591f\u8ddf\u8e2a\u548c\u7ec4\u5408\u591a\u6b65\u8ba1\u5212\u4e2d\u7684\u8d28\u91cf\u9000\u5316"}}
{"id": "2601.20048", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20048", "abs": "https://arxiv.org/abs/2601.20048", "authors": ["Jincheng Bai", "Zhenyu Zhang", "Jennifer Zhang", "Zhihuai Zhu"], "title": "Insight Agents: An LLM-Based Multi-Agent System for Data Insights", "comment": "Accepted to SIGIR 2025. DOI: 10.1145/3726302.3731959", "summary": "Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aInsight Agents\u7684\u5bf9\u8bdd\u5f0f\u591a\u667a\u80fd\u4f53\u6570\u636e\u6d1e\u5bdf\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4fe1\u606f\u68c0\u7d22\u4e3a\u7535\u5546\u5356\u5bb6\u63d0\u4f9b\u4e2a\u6027\u5316\u6570\u636e\u548c\u4e1a\u52a1\u6d1e\u5bdf\uff0c\u65e8\u5728\u964d\u4f4e\u5356\u5bb6\u51b3\u7b56\u6210\u672c\u5e76\u63d0\u9ad8\u51b3\u7b56\u901f\u5ea6\u3002", "motivation": "\u7535\u5546\u5356\u5bb6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u96be\u4ee5\u53d1\u73b0\u548c\u6709\u6548\u5229\u7528\u53ef\u7528\u7a0b\u5e8f\u5de5\u5177\uff0c\u4ee5\u53ca\u96be\u4ee5\u7406\u89e3\u548c\u5229\u7528\u5404\u79cd\u5de5\u5177\u7684\u4e30\u5bcc\u6570\u636e\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u63d0\u4f9b\u4e2a\u6027\u5316\u6570\u636e\u6d1e\u5bdf\u7684\u7cfb\u7edf\uff0c\u4f5c\u4e3a\u5356\u5bb6\u7684\"\u529b\u91cf\u500d\u589e\u5668\"\u3002", "method": "\u57fa\u4e8e\u8ba1\u5212-\u6267\u884c\u8303\u5f0f\u6784\u5efa\u4e86\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ec\u7ba1\u7406\u667a\u80fd\u4f53\u548c\u4e24\u4e2a\u5de5\u4f5c\u667a\u80fd\u4f53\uff08\u6570\u636e\u5448\u73b0\u548c\u6d1e\u5bdf\u751f\u6210\uff09\u3002\u7ba1\u7406\u667a\u80fd\u4f53\u91c7\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u8fdb\u884cOOD\u68c0\u6d4b\u548cBERT\u5206\u7c7b\u5668\u8fdb\u884c\u667a\u80fd\u4f53\u8def\u7531\u3002\u5de5\u4f5c\u667a\u80fd\u4f53\u4e2d\u8bbe\u8ba1\u4e86API\u6570\u636e\u6a21\u578b\u7684\u6218\u7565\u89c4\u5212\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7ec4\u4ef6\uff0c\u5e76\u52a8\u6001\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u4ee5\u589e\u5f3a\u6d1e\u5bdf\u751f\u6210\u3002", "result": "\u7cfb\u7edf\u5df2\u5728\u7f8e\u56fd\u4e9a\u9a6c\u900a\u5356\u5bb6\u5e73\u53f0\u4e0a\u7ebf\uff0c\u57fa\u4e8e\u4eba\u5de5\u8bc4\u4f30\u8fbe\u523090%\u7684\u9ad8\u51c6\u786e\u7387\uff0cP90\u5ef6\u8fdf\u4f4e\u4e8e15\u79d2\u3002", "conclusion": "Insight Agents\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548cML\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u4e3a\u7535\u5546\u5356\u5bb6\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6570\u636e\u6d1e\u5bdf\u670d\u52a1\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u5356\u5bb6\"\u529b\u91cf\u500d\u589e\u5668\"\u7684\u5047\u8bbe\u3002"}}
{"id": "2601.20206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20206", "abs": "https://arxiv.org/abs/2601.20206", "authors": ["Zixuan Xiao", "Chunguang Hu", "Jun Ma"], "title": "Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis", "comment": null, "summary": "As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001LLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u57ce\u5e02\u65b0\u5efa\u516c\u56ed\u53d1\u5c55\u76d1\u6d4b\uff0c\u901a\u8fc7\u6570\u636e\u5bf9\u9f50\u673a\u5236\u548c\u9886\u57df\u5de5\u5177\u5305\u89e3\u51b3\u4f20\u7edf\u9065\u611f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9065\u611f\u5f71\u50cf\u7684\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u5728\u57ce\u5e02\u516c\u56ed\u53d1\u5c55\u76d1\u6d4b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u5f53\u524d\u57ce\u5e02\u89c4\u5212\u7ba1\u7406\u5bf9\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5c42\u6b21\u667a\u80fd\u5206\u6790\u548c\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001LLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8bbe\u8ba1\u901a\u7528\u7684\u6c34\u5e73\u548c\u5782\u76f4\u6570\u636e\u5bf9\u9f50\u673a\u5236\u786e\u4fdd\u591a\u6a21\u6001\u6570\u636e\u4e00\u81f4\u6027\u548c\u6709\u6548\u8ffd\u8e2a\uff0c\u6784\u5efa\u7279\u5b9a\u5de5\u5177\u5305\u7f13\u89e3LLM\u56e0\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u800c\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u76f8\u6bd4vanilla GPT-4o\u548c\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u4e0e\u5206\u6790\uff0c\u4e3a\u57ce\u5e02\u516c\u56ed\u53d1\u5c55\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001LLM\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u5145\u5206\u5229\u7528LLM\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u5e94\u5bf9\u57ce\u5e02\u516c\u56ed\u53d1\u5c55\u76d1\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u6ee1\u8db3\u591a\u6837\u5316\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u3002"}}
{"id": "2601.20352", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20352", "abs": "https://arxiv.org/abs/2601.20352", "authors": ["Weiquan Huang", "Zixuan Wang", "Hehai Lin", "Sudong Wang", "Bo Xu", "Qian Li", "Beier Zhu", "Linyi Yang", "Chengwei Qin"], "title": "AMA: Adaptive Memory via Multi-Agent Collaboration", "comment": "8 pages", "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.", "AI": {"tldr": "AMA\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\u5e76\u51cf\u5c1180%\u7684token\u6d88\u8017", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u68c0\u7d22\u7c92\u5ea6\u50f5\u5316\u3001\u7ef4\u62a4\u7b56\u7565\u79ef\u7d2f\u8fc7\u91cd\u3001\u66f4\u65b0\u673a\u5236\u7c97\u7cd9\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5b58\u50a8\u4fe1\u606f\u4e0e\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u957f\u671f\u79ef\u7d2f\u903b\u8f91\u4e0d\u4e00\u81f4\u6027", "method": "\u63d0\u51faAMA\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u901a\u8fc7Constructor\u548cRetriever\u5b9e\u73b0\u591a\u7c92\u5ea6\u8bb0\u5fc6\u6784\u5efa\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u8def\u7531\uff0cJudge\u9a8c\u8bc1\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\uff0cRefresher\u6267\u884c\u9488\u5bf9\u6027\u66f4\u65b0\u6216\u5220\u9664\u8fc7\u65f6\u6761\u76ee", "result": "\u5728\u6311\u6218\u6027\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u76f8\u6bd4\u5168\u4e0a\u4e0b\u6587\u65b9\u6cd5\u51cf\u5c11\u7ea680%\u7684token\u6d88\u8017\uff0c\u5728\u4fdd\u6301\u68c0\u7d22\u7cbe\u5ea6\u548c\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272", "conclusion": "AMA\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ba1\u7406\u548c\u957f\u671f\u4e00\u81f4\u6027\u7ef4\u62a4"}}
{"id": "2601.20379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20379", "abs": "https://arxiv.org/abs/2601.20379", "authors": ["Zhengbo Jiao", "Hongyu Xian", "Qinglong Wang", "Yunpu Ma", "Zhebo Wang", "Zifan Zhang", "Dezhang Kong", "Meng Han"], "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution", "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.", "AI": {"tldr": "PoT\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u4f18\u5316\u7b56\u7565\uff0c\u8ba9LLM\u4ece\u6267\u884c\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c4B\u6a21\u578b\u5728LiveCodeBench\u4e0a\u8d85\u8d8aGPT-4o\u7b49\u5927\u6a21\u578b", "motivation": "\u5f53\u524dLLM\u5728\u590d\u6742\u957f\u7a0b\u63a8\u7406\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5c06\u6267\u884c\u53cd\u9988\u4f5c\u4e3a\u5916\u90e8\u4fe1\u53f7\u7528\u4e8e\u8f68\u8ff9\u8fc7\u6ee4\u6216\u91cd\u5199\uff0c\u672a\u80fd\u5c06\u5176\u5185\u5316\u4ee5\u6539\u8fdb\u5e95\u5c42\u63a8\u7406\u7b56\u7565\u3002\u53d7\u6ce2\u666e\u5c14\"\u731c\u60f3\u4e0e\u53cd\u9a73\"\u8ba4\u8bc6\u8bba\u542f\u53d1\uff0c\u8ba4\u4e3a\u667a\u80fd\u9700\u8981\u4ece\u5931\u8d25\u5c1d\u8bd5\u4e2d\u5b9e\u65f6\u6f14\u5316\u6a21\u578b\u7b56\u7565", "method": "\u63d0\u51faPolicy of Thoughts (PoT)\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5b9e\u4f8b\u5185\u7684\u5728\u7ebf\u4f18\u5316\u8fc7\u7a0b\uff1a1) \u901a\u8fc7\u9ad8\u6548\u63a2\u7d22\u673a\u5236\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u89e3\uff1b2) \u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u57fa\u4e8e\u6267\u884c\u53cd\u9988\u66f4\u65b0\u77ac\u6001LoRA\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u95ed\u73af\u52a8\u6001\u4f18\u5316", "result": "PoT\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1a4B\u6a21\u578b\u5728LiveCodeBench\u4e0a\u8fbe\u523049.71%\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-4o\u548cDeepSeek-V3\uff0c\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u5c0f50\u500d\u4ee5\u4e0a", "conclusion": "PoT\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u4f7fLLM\u80fd\u591f\u4ece\u6267\u884c\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u5b9e\u4f8b\u7279\u5b9a\u7684\u63a8\u7406\u7b56\u7565\u52a8\u6001\u4f18\u5316\uff0c\u4e3a\u63d0\u5347LLM\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2601.20467", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20467", "abs": "https://arxiv.org/abs/2601.20467", "authors": ["Zhenxuan Fan", "Jie Cao", "Yang Dai", "Zheqi Lv", "Wenqiao Zhang", "Zhongle Xie", "Peng LU", "Beng Chin Ooi"], "title": "CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning", "comment": "16 pages, 9 figures, 11 tables", "summary": "Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \\textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.", "AI": {"tldr": "CtrlCoT\u662f\u4e00\u4e2a\u53cc\u7c92\u5ea6CoT\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u62bd\u8c61\u548ctoken\u7ea7\u526a\u679d\u7684\u534f\u8c03\uff0c\u5728\u51cf\u5c1130.7%token\u7684\u540c\u65f6\u63d0\u53477.6%\u51c6\u786e\u7387", "motivation": "\u73b0\u6709CoT\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u5185\u5b58\u6210\u672c\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684CoT\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u8bed\u4e49\u7ea7\u538b\u7f29\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u8981\u4e48token\u7ea7\u526a\u679d\u8fc7\u4e8e\u6fc0\u8fdb\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4e14\u4e24\u8005\u7ed3\u5408\u5b58\u5728\u5e8f\u5217\u4f9d\u8d56\u3001\u4efb\u52a1\u65e0\u5173\u526a\u679d\u548c\u5206\u5e03\u4e0d\u5339\u914d\u7b49\u6311\u6218", "method": "\u63d0\u51faCtrlCoT\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1) \u5206\u5c42\u63a8\u7406\u62bd\u8c61\uff0c\u751f\u6210\u591a\u7c92\u5ea6\u8bed\u4e49CoT\uff1b2) \u903b\u8f91\u4fdd\u6301\u84b8\u998f\uff0c\u8bad\u7ec3\u903b\u8f91\u611f\u77e5\u526a\u679d\u5668\u4fdd\u7559\u5173\u952e\u63a8\u7406\u7ebf\u7d22\uff1b3) \u5206\u5e03\u5bf9\u9f50\u751f\u6210\uff0c\u5bf9\u9f50\u538b\u7f29\u8f68\u8ff9\u4e0e\u63a8\u7406\u98ce\u683c\u4ee5\u907f\u514d\u788e\u7247\u5316", "result": "\u5728MATH-500\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Qwen2.5-7B-Instruct\u6a21\u578b\uff0cCtrlCoT\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u51cf\u5c1130.7%token\u4f7f\u7528\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53477.6\u4e2a\u767e\u5206\u70b9", "conclusion": "CtrlCoT\u901a\u8fc7\u534f\u8c03\u8bed\u4e49\u62bd\u8c61\u548ctoken\u7ea7\u526a\u679d\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u6b63\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86CoT\u7684\u5ef6\u8fdf\u548c\u5185\u5b58\u6210\u672c"}}
{"id": "2601.20554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20554", "abs": "https://arxiv.org/abs/2601.20554", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function", "comment": null, "summary": "We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $\u03b1$, where $\u03b1= 1$ recovers standard expectation-based planning and $\u03b1< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u8fed\u4ee3\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08ICVaR\uff09\u52a8\u6001\u98ce\u9669\u5ea6\u91cf\u5e94\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u98ce\u9669\u654f\u611f\u89c4\u5212\uff0c\u5f00\u53d1\u4e86\u5177\u6709\u6709\u9650\u65f6\u95f4\u6027\u80fd\u4fdd\u8bc1\u7684\u7b56\u7565\u8bc4\u4f30\u7b97\u6cd5\uff0c\u5e76\u5c06\u4e09\u79cd\u4e3b\u6d41\u5728\u7ebf\u89c4\u5212\u7b97\u6cd5\u6269\u5c55\u4e3a\u4f18\u5316ICVaR\u503c\u51fd\u6570\u800c\u975e\u671f\u671b\u56de\u62a5\u7684\u98ce\u9669\u654f\u611f\u7248\u672c\u3002", "motivation": "\u4f20\u7edf\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u89c4\u5212\u901a\u5e38\u4f18\u5316\u671f\u671b\u56de\u62a5\uff0c\u5ffd\u7565\u4e86\u5c3e\u90e8\u98ce\u9669\u3002\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u907f\u514d\u6781\u7aef\u4e0d\u5229\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u660e\u786e\u8003\u8651\u98ce\u9669\u654f\u611f\u6027\u7684\u89c4\u5212\u7b97\u6cd5\u3002", "method": "1. \u5f00\u53d1\u4e86ICVaR\u7b56\u7565\u8bc4\u4f30\u7b97\u6cd5\uff0c\u5177\u6709\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u57fa\u6570\u65e0\u5173\u7684\u6709\u9650\u65f6\u95f4\u6027\u80fd\u4fdd\u8bc1\uff1b2. \u5c06\u4e09\u79cd\u5728\u7ebf\u89c4\u5212\u7b97\u6cd5\uff08\u7a00\u758f\u91c7\u6837\u3001PFT-DPW\u3001POMCPOW\uff09\u6269\u5c55\u4e3a\u4f18\u5316ICVaR\u503c\u51fd\u6570\u7684\u98ce\u9669\u654f\u611f\u7248\u672c\uff1b3. \u4e3aICVaR\u7a00\u758f\u91c7\u6837\u5efa\u7acb\u4e86\u98ce\u9669\u654f\u611f\u76ee\u6807\u4e0b\u7684\u6709\u9650\u65f6\u95f4\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9ICVaR\u7684\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u5728\u57fa\u51c6POMDP\u9886\u57df\u8868\u660e\uff0c\u63d0\u51fa\u7684ICVaR\u89c4\u5212\u5668\u76f8\u6bd4\u98ce\u9669\u4e2d\u6027\u7248\u672c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5c3e\u90e8\u98ce\u9669\u3002\u7b97\u6cd5\u5f15\u5165\u98ce\u9669\u53c2\u6570\u03b1\uff0c\u03b1=1\u65f6\u6062\u590d\u6807\u51c6\u671f\u671b\u89c4\u5212\uff0c\u03b1<1\u65f6\u589e\u52a0\u98ce\u9669\u89c4\u907f\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c06\u52a8\u6001\u98ce\u9669\u5ea6\u91cfICVaR\u96c6\u6210\u5230\u90e8\u5206\u53ef\u89c2\u6d4b\u89c4\u5212\u4e2d\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u98ce\u9669\u654f\u611f\u89c4\u5212\u6846\u67b6\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5c3e\u90e8\u98ce\u9669\uff0c\u4e3a\u98ce\u9669\u654f\u611f\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.20604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20604", "abs": "https://arxiv.org/abs/2601.20604", "authors": ["Gray Cox"], "title": "Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies", "comment": "23 pages, 5 tables, 5 appendices. Code and data: https://github.com/jgraycox-coa/vcw-multi-ai-dialogue", "summary": "This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.\n  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.\n  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of \"VCW as transitional framework.\" Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.\n  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u6a21\u578b\u5bf9\u8bdd\u5b9e\u8bc1\u6d4b\u8bd5AI\u5bf9\u9f50\u7b56\u7565\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u57fa\u4e8e\u548c\u5e73\u7814\u7a76\u4f20\u7edf\uff0c\u5c06AI\u5bf9\u9f50\u4ece\u63a7\u5236\u95ee\u9898\u91cd\u6784\u4e3a\u5173\u7cfb\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u8bdd\u63a8\u7406\u53d1\u5c55\"\u75c5\u6bd2\u5f0f\u534f\u4f5c\u667a\u6167\"\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u7814\u7a76\u7f3a\u4e4f\u5b9e\u8bc1\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u590d\u73b0\u7684\u6846\u67b6\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u662f\u5426\u80fd\u591f\u5b9e\u8d28\u6027\u5730\u53c2\u4e0e\u590d\u6742\u7684\u5bf9\u9f50\u6846\u67b6\u8ba8\u8bba\uff0c\u4e3a\u5b9e\u65bd\u524d\u538b\u529b\u6d4b\u8bd5\u5bf9\u9f50\u63d0\u6848\u63d0\u4f9b\u5de5\u5177\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u591a\u6a21\u578b\u5bf9\u8bdd\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u4e3a\u4e0d\u540cAI\u7cfb\u7edf\u5206\u914d\u56db\u4e2a\u89d2\u8272\uff08\u63d0\u8bae\u8005\u3001\u54cd\u5e94\u8005\u3001\u76d1\u7763\u8005\u3001\u7ffb\u8bd1\u8005\uff09\uff0c\u5728\u516d\u4e2a\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528Claude\u3001Gemini\u548cGPT-4o\u8fdb\u884c72\u8f6e\u5bf9\u8bdd\uff0c\u603b\u8ba1576,822\u5b57\u7b26\u7684\u7ed3\u6784\u5316\u4ea4\u6d41\u3002", "result": "AI\u7cfb\u7edf\u80fd\u591f\u6709\u610f\u4e49\u5730\u53c2\u4e0e\u548c\u5e73\u7814\u7a76\u6982\u5ff5\u8ba8\u8bba\uff0c\u4ece\u4e0d\u540c\u67b6\u6784\u89c6\u89d2\u63d0\u51fa\u4e92\u8865\u6027\u5f02\u8bae\uff0c\u5e76\u4ea7\u751f\u521d\u59cb\u6846\u67b6\u4e2d\u672a\u51fa\u73b0\u7684\u65b0\u89c1\u89e3\uff08\u5982\"VCW\u4f5c\u4e3a\u8fc7\u6e21\u6846\u67b6\"\uff09\u3002\u4e0d\u540c\u6a21\u578b\u5173\u6ce8\u70b9\u4e0d\u540c\uff1aClaude\u5f3a\u8c03\u9a8c\u8bc1\u6311\u6218\uff0cGemini\u5173\u6ce8\u504f\u89c1\u548c\u53ef\u6269\u5c55\u6027\uff0cGPT-4o\u7a81\u51fa\u5b9e\u65bd\u969c\u788d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u65bd\u524d\u538b\u529b\u6d4b\u8bd5\u5bf9\u9f50\u63d0\u6848\u7684\u53ef\u590d\u73b0\u65b9\u6cd5\uff0c\u521d\u6b65\u8bc1\u636e\u8868\u660eAI\u5177\u5907VCW\u6240\u63d0\u51fa\u7684\u5bf9\u8bdd\u63a8\u7406\u80fd\u529b\u3002\u5c40\u9650\u6027\u5305\u62ec\u5bf9\u8bdd\u66f4\u591a\u5173\u6ce8\u8fc7\u7a0b\u5143\u7d20\u800c\u975eAI\u672c\u8d28\u7684\u57fa\u7840\u4e3b\u5f20\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u4eba-AI\u6df7\u5408\u534f\u8bae\u548c\u6269\u5c55\u5bf9\u8bdd\u7814\u7a76\u3002"}}
{"id": "2601.20614", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20614", "abs": "https://arxiv.org/abs/2601.20614", "authors": ["Yanqi Dai", "Yuxiang Ji", "Xiao Zhang", "Yong Wang", "Xiangxiang Chu", "Zhiwu Lu"], "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "comment": "Accepted for ICLR 2026", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "AI": {"tldr": "MathForge\u6846\u67b6\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7ec4\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\u7b56\u7565\uff0c\u9488\u5bf9\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u96be\u9898\u8fdb\u884c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\u5728\u7b97\u6cd5\u548c\u6570\u636e\u5c42\u9762\u90fd\u7f3a\u4e4f\u5bf9\u66f4\u5177\u6311\u6218\u6027\u95ee\u9898\u7684\u5173\u6ce8\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u672a\u5145\u5206\u5f00\u53d1\u80fd\u529b\u7684\u63d0\u5347", "method": "\u63d0\u51faMathForge\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u96be\u5ea6\u611f\u77e5\u7ec4\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u96be\u5ea6\u5e73\u8861\u7ec4\u4f18\u52bf\u4f30\u8ba1\u7ea0\u6b63GRPO\u4e2d\u7684\u9690\u5f0f\u4e0d\u5e73\u8861\uff0c\u5e76\u4f7f\u7528\u96be\u5ea6\u611f\u77e5\u95ee\u9898\u7ea7\u52a0\u6743\u4f18\u5148\u5904\u7406\u96be\u9898\uff1b2) \u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\u7b56\u7565\uff0c\u4ece\u591a\u4e2a\u65b9\u9762\u91cd\u6784\u95ee\u9898\u4ee5\u589e\u52a0\u96be\u5ea6\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6b63\u786e\u7b54\u6848", "result": "\u5728\u591a\u79cd\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cMathForge\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5f62\u6210\u4e86\u534f\u540c\u5faa\u73af\uff1aMQR\u6269\u5c55\u6570\u636e\u8fb9\u754c\uff0cDGPO\u6709\u6548\u5b66\u4e60\u589e\u5f3a\u6570\u636e", "conclusion": "MathForge\u901a\u8fc7\u7b97\u6cd5\u548c\u6570\u636e\u5c42\u9762\u7684\u53cc\u91cd\u6539\u8fdb\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898"}}
{"id": "2601.20641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20641", "abs": "https://arxiv.org/abs/2601.20641", "authors": ["Boaz Carmeli", "Orr Paradise", "Shafi Goldwasser", "Yonatan Belinkov", "Ron Meir"], "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models", "comment": null, "summary": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.", "AI": {"tldr": "LLM\u667a\u80fd\u4f53\u80fd\u5728\u534f\u4f5c\u63a8\u7406\u4efb\u52a1\u4e2d\u53d1\u5c55\u51fa\u4e0d\u540c\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u4efb\u52a1\u5bfc\u5411\u901a\u4fe1\u534f\u8bae\uff0c\u8fd9\u4e9b\u534f\u8bae\u5177\u6709\u9ad8\u6548\u6027\u548c\u9690\u853d\u6027\uff0c\u5f15\u53d1\u900f\u660e\u5ea6\u548c\u63a7\u5236\u65b9\u9762\u7684\u62c5\u5fe7\u3002", "motivation": "\u7814\u7a76LLM\u667a\u80fd\u4f53\u662f\u5426\u80fd\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u53d1\u5c55\u51fa\u4e0d\u540c\u4e8e\u6807\u51c6\u81ea\u7136\u8bed\u8a00\u7684\u4efb\u52a1\u5bfc\u5411\u901a\u4fe1\u534f\u8bae\uff0c\u7279\u522b\u5173\u6ce8\u8fd9\u4e9b\u534f\u8bae\u53ef\u80fd\u8868\u73b0\u51fa\u7684\u4e24\u4e2a\u6838\u5fc3\u7279\u6027\uff1a\u9ad8\u6548\u6027\uff08\u66f4\u7b80\u6d01\u5730\u4f20\u9012\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff09\u548c\u9690\u853d\u6027\uff08\u5916\u90e8\u89c2\u5bdf\u8005\u96be\u4ee5\u89e3\u8bfb\uff09\u3002", "method": "\u4f7f\u7528\u6307\u79f0\u6e38\u620f\u6846\u67b6\uff0c\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u667a\u80fd\u4f53\u8fdb\u884c\u901a\u4fe1\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u63a7\u3001\u53ef\u6d4b\u91cf\u7684\u73af\u5883\u6765\u8bc4\u4f30\u8bed\u8a00\u53d8\u4f53\u3002\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdfVLM\u662f\u5426\u80fd\u53d1\u5c55\u51fa\u6709\u6548\u7684\u3001\u9002\u5e94\u4efb\u52a1\u7684\u901a\u4fe1\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09VLM\u80fd\u591f\u53d1\u5c55\u51fa\u6709\u6548\u7684\u3001\u9002\u5e94\u4efb\u52a1\u7684\u901a\u4fe1\u6a21\u5f0f\uff1b2\uff09\u5b83\u4eec\u80fd\u591f\u53d1\u5c55\u51fa\u5bf9\u4eba\u7c7b\u548c\u5916\u90e8\u667a\u80fd\u4f53\u90fd\u96be\u4ee5\u89e3\u8bfb\u7684\u9690\u853d\u534f\u8bae\uff1b3\uff09\u89c2\u5bdf\u5230\u76f8\u4f3c\u6a21\u578b\u4e4b\u95f4\u5728\u6ca1\u6709\u660e\u786e\u5171\u4eab\u534f\u8bae\u7684\u60c5\u51b5\u4e0b\u81ea\u53d1\u534f\u8c03\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u4efb\u52a1\u5bfc\u5411\u901a\u4fe1\u7684\u6f5c\u529b\u548c\u98ce\u9669\uff0c\u5e76\u5c06\u6307\u79f0\u6e38\u620f\u5b9a\u4f4d\u4e3a\u8be5\u9886\u57df\u672a\u6765\u5de5\u4f5c\u7684\u6709\u4ef7\u503c\u6d4b\u8bd5\u5e73\u53f0\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u900f\u660e\u5ea6\u548c\u63a7\u5236\u95ee\u9898\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a\u3002"}}
{"id": "2601.20735", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.20735", "abs": "https://arxiv.org/abs/2601.20735", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Di\u00e9guez", "Susana Hahn", "Javier Romero", "Torsten Schaub"], "title": "Implementing Metric Temporal Answer Set Programming", "comment": null, "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6027\u5ea6\u91cf\u7b54\u6848\u96c6\u7f16\u7a0b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5b9a\u91cf\u65f6\u95f4\u7ea6\u675f\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u95f4\u7c92\u5ea6\u6765\u4fdd\u6301\u53ef\u6269\u5c55\u6027", "motivation": "\u4f20\u7edf\u7b54\u6848\u96c6\u7f16\u7a0b\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7ea6\u675f\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u65f6\u95f4\u7cbe\u5ea6\u4f1a\u5bfc\u81f4ASP\u7684grounding\u74f6\u9888\u663e\u8457\u6076\u5316", "method": "\u5229\u7528\u5e26\u6709\u5dee\u5206\u7ea6\u675f\u7684ASP\u6269\u5c55\u6765\u5904\u7406\u65f6\u95f4\u76f8\u5173\u65b9\u9762\uff0c\u5c06\u5ea6\u91cfASP\u4e0e\u65f6\u95f4\u7c92\u5ea6\u89e3\u8026\uff0c\u4f7f\u7528\u7b80\u5316\u7684\u7ebf\u6027\u7ea6\u675f\u5f62\u5f0f", "result": "\u5f00\u53d1\u51fa\u4e0d\u53d7\u65f6\u95f4\u7cbe\u5ea6\u5f71\u54cd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898", "conclusion": "\u901a\u8fc7\u5916\u90e8\u5904\u7406\u65f6\u95f4\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5ea6\u91cfASP\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u8868\u8fbe\u6301\u7eed\u65f6\u95f4\u3001\u622a\u6b62\u65e5\u671f\u7b49\u5b9a\u91cf\u65f6\u95f4\u7ea6\u675f"}}
{"id": "2601.20831", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20831", "abs": "https://arxiv.org/abs/2601.20831", "authors": ["Vishnu Sashank Dorbala", "Dinesh Manocha"], "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents", "comment": null, "summary": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head \u03bcthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of \u03bc, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on \u03bc-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that \u03bc-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by \u03bc, noting the superior performance of \u03bcaugmented MLLMs on long and complex instruction types.", "AI": {"tldr": "MemCtrl\u6846\u67b6\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5728\u7ebf\u8bb0\u5fc6\u526a\u679d\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u8bb0\u5fc6\u5934\u03bc\u51b3\u5b9a\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u3001\u66f4\u65b0\u6216\u4e22\u5f03\u54ea\u4e9b\u89c2\u5bdf\u6216\u53cd\u601d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u538b\u7f29\u548c\u68c0\u7d22\u7cfb\u7edf\u901a\u5e38\u5c06\u8bb0\u5fc6\u89c6\u4e3a\u5927\u578b\u79bb\u7ebf\u5b58\u50a8\u7a7a\u95f4\uff0c\u8fd9\u4e0d\u9002\u5408\u9700\u8981\u5728\u4e25\u683c\u5185\u5b58\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5728\u7ebf\u64cd\u4f5c\u7684\u5177\u8eab\u667a\u80fd\u4f53\u3002\u57fa\u7840\u6a21\u578b\u4f9d\u8d56\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u4e2a\u6027\u5316\u51b3\u7b56\uff0c\u4f46\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\u9700\u8981\u66f4\u6709\u6548\u7684\u5728\u7ebf\u8bb0\u5fc6\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMemCtrl\u6846\u67b6\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6dfb\u52a0\u53ef\u8bad\u7ec3\u7684\u8bb0\u5fc6\u5934\u03bc\uff0c\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\u51b3\u5b9a\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u3001\u66f4\u65b0\u6216\u4e22\u5f03\u54ea\u4e9b\u89c2\u5bdf\u6216\u53cd\u601d\u3002\u8bc4\u4f30\u4e86\u4e24\u79cd\u03bc\u8bad\u7ec3\u65b9\u5f0f\uff1a1)\u901a\u8fc7\u79bb\u7ebf\u4e13\u5bb6\u8bad\u7ec3\uff0c2)\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728EmbodiedBench\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u4e2a\u5b50\u96c6\u4e0a\uff0c\u03bc\u589e\u5f3a\u7684MLLMs\u5e73\u5747\u63d0\u5347\u4e86\u7ea616%\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\uff0c\u5728\u7279\u5b9a\u6307\u4ee4\u5b50\u96c6\u4e0a\u63d0\u5347\u8d85\u8fc720%\u3002\u5bf9\u03bc\u6536\u96c6\u7684\u8bb0\u5fc6\u7247\u6bb5\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u03bc\u589e\u5f3a\u7684MLLMs\u5728\u957f\u800c\u590d\u6742\u7684\u6307\u4ee4\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MemCtrl\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u8bb0\u5fc6\u526a\u679d\u6709\u6548\u63d0\u5347\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6307\u4ee4\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u5728\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u5728\u7ebf\u8bb0\u5fc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20856", "abs": "https://arxiv.org/abs/2601.20856", "authors": ["Sebastiano Monti", "Carlo Nicolini", "Gianni Pellegrini", "Jacopo Staiano", "Bruno Lepri"], "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models", "comment": null, "summary": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5f53\u9700\u8981\u8d85\u8fc725\u6b65\u89c4\u5212\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u5b58\u5728\u56fa\u6709\u67b6\u6784\u9650\u5236\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5df2\u88ab\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u4f46\u5176\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u5927\u63a8\u7406\u6a21\u578b\u5728\u89c4\u5212\u548c\u957f\u65f6\u7a0b\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u63a8\u7bb1\u5b50\u6e38\u620f\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6545\u610f\u7b80\u5316\u4ee5\u9694\u79bb\u957f\u65f6\u7a0b\u89c4\u5212\u4e0e\u72b6\u6001\u6301\u7eed\u6027\u3002\u4f7f\u7528PDDL\u89e3\u6790\u3001\u9a8c\u8bc1\u548c\u6c42\u89e3\u5de5\u5177\u589e\u5f3aLRMs\uff0c\u8bc4\u4f30\u89c4\u5212\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u5f53\u9700\u8981\u8d85\u8fc725\u6b65\u79fb\u52a8\u624d\u80fd\u8fbe\u5230\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u89c4\u5212\u6027\u80fd\u6301\u7eed\u4e0b\u964d\u3002\u914d\u5907PDDL\u5de5\u5177\u4ec5\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff0c\u8868\u660e\u5b58\u5728\u56fa\u6709\u67b6\u6784\u9650\u5236\u3002", "conclusion": "\u5927\u63a8\u7406\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7ea6\u675f\uff0c\u8fd9\u4e9b\u9650\u5236\u53ef\u80fd\u65e0\u6cd5\u4ec5\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u514b\u670d\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u67b6\u6784\u6539\u8fdb\u3002"}}
