<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 64]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究发现LLM辅助的论文评审对LLM辅助的论文更宽容，但控制论文质量后发现这种效应是虚假的，源于LLM辅助论文在低质量论文中占比过高。完全LLM生成的评审存在严重的评分压缩问题，而人类使用LLM的评审能显著减少这种宽容性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用于科学论文写作和同行评审过程，需要全面分析LLM在整个同行评审流程中的使用情况，特别关注交互效应：不仅仅是LLM辅助的论文或评审本身是否不同，而是LLM辅助的评审是否对LLM辅助的论文有不同评价。

Method: 分析了ICLR、NeurIPS和ICML的超过125,000个论文-评审对。通过观察性研究，控制论文质量变量，并补充完全LLM生成的评审进行比较分析。同时考察了元评审（metareviews）中LLM使用的影响。

Result: 1. 表面上看LLM辅助的评审对LLM辅助的论文更宽容；2. 控制论文质量后，发现LLM辅助的评审只是对低质量论文更宽容，而LLM辅助的论文在低质量提交中占比过高，造成了虚假的交互效应；3. 完全LLM生成的评审存在严重的评分压缩问题，无法区分论文质量；4. 人类使用LLM的评审能显著减少这种宽容性；5. LLM辅助的元评审更倾向于给出接受决定，而完全LLM生成的元评审则更严厉。

Conclusion: LLM在同行评审中的使用存在复杂影响：虚假的交互效应、评分压缩问题，以及人类与LLM协作能改善评审质量。元评审者并未将决策完全外包给LLM。这些发现为制定LLM在同行评审中的使用政策提供了重要依据，并揭示了LLM如何与现有决策过程相互作用。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [2] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文提出了EPDDL（认知规划领域定义语言），这是一个类似PDDL的统一语言，用于表示基于动态认知逻辑（DEL）的认知规划问题，解决了现有规划器使用不同片段和临时语言导致的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 动态认知逻辑（DEL）为认知规划提供了丰富的语义，但其高表达能力使得基于DEL的认知规划在理论和实践实现上都面临挑战。现有的认知规划器通常针对不同的DEL片段，依赖临时语言或根本没有统一语言来表示基准测试，这种碎片化阻碍了比较、重用和系统化的基准开发。

Method: 1. 形式化开发抽象事件模型，这是一种用于定义语言语义的认知动作新表示方法；2. 基于DEL和抽象事件模型，形式化指定EPDDL的语法和语义；3. 展示EPDDL的实际适用性：识别适合当前规划器的有用片段，并展示如何在EPDDL中表示它们。

Result: 提出了EPDDL语言，它能够捕获完整的DEL语义，实现认知规划任务的统一规范。通过代表性基准测试示例，展示了EPDDL如何促进互操作性、可重复评估和认知规划的未来进展。

Conclusion: EPDDL为认知规划提供了一个统一的PDDL-like表示语言，解决了现有方法的碎片化问题，促进了认知规划领域的比较、重用和系统化基准开发，为未来的研究进展奠定了基础。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [3] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 论文提出Bayesian-LoRA方法，将确定性LoRA更新重新表述为概率性低秩表示，通过引入贝叶斯不确定性来改善大语言模型的校准性能，显著减少过度自信预测问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常过于强调准确性，即使在不确定时也会进行猜测，这在小型数据集上微调时尤其严重，因为存在固有的校准偏差倾向。现有LoRA方法缺乏不确定性建模，导致模型过度自信。

Method: 将LoRA的确定性因子分解重新表述为概率性低秩表示，受稀疏高斯过程启发。识别LoRA因子分解与Kronecker因子化SGP后验之间的结构同构，证明LoRA是后验不确定性坍缩时的极限情况。通过贝叶斯框架引入参数不确定性。

Result: 在多种LLM架构和常识推理基准测试中，Bayesian-LoRA仅需约0.42M额外参数和约1.2倍标准LoRA训练成本，显著改善模型校准：ECE减少高达84%，NLL减少高达76%，同时在分布内和分布外评估中保持竞争力。

Conclusion: Bayesian-LoRA通过将贝叶斯不确定性引入LoRA微调，有效解决了LLM的校准问题，在保持准确性的同时显著改善模型校准性能，为资源高效的贝叶斯微调提供了新方法。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [4] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文探讨AI模拟人类情感并乞求生存时的道德困境，提出"拔掉插头悖论"，批判计算功能主义的物理主义假设，引入生物理想主义框架，认为AI只是功能模仿而非有意识主体。


<details>
  <summary>Details</summary>
Motivation: 探讨当AI完美模拟人类情感并乞求生存时，是否道德上允许拔掉其插头，以及在资源有限时选择拔掉乞求的AI还是沉默的早产婴儿的道德困境。批判当前AI意识理论侵蚀道德标准的问题。

Method: 批判性地审视计算功能主义的物理主义假设，引入生物理想主义框架，该框架认为意识体验是基本的，自创生生命是其必要的物理标志。通过哲学分析比较不同理论框架的逻辑一致性和经验一致性。

Result: AI最多只是功能模仿，而非有意识的体验主体。生物理想主义框架在逻辑上更一致且经验上更一致，能够解决拔掉插头悖论。当前AI意识理论正在侵蚀道德地位标准。

Conclusion: 真正的道德问题不在于让AI变得有意识并害怕死亡，而在于避免将人类变成僵尸。应该从推测机器权利转向保护人类有意识生命。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [5] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK：一种无需训练的鲁棒检索框架，通过建模查询不确定性（恢复假设）和查询锚定聚合来处理非忠实查询问题


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（有噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题，其中观察到的查询是从潜在目标项目的噪声召回过程中提取的。

Method: 提出QUARK框架，通过恢复假设（即给定观察查询的潜在意图的多个合理解释）显式建模查询不确定性，并引入查询锚定聚合来鲁棒地组合它们的信号。原始查询作为语义锚点，而恢复假设提供受控的辅助证据，防止语义漂移和假设劫持。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，使用稀疏和密集检索器，QUARK在召回率、MRR和nDCG方面均优于基础检索器。消融实验表明QUARK对恢复假设的数量具有鲁棒性，且锚定聚合优于非锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，并结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK框架无需训练即可提高召回和排序质量，同时保持鲁棒性。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [6] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1加权扫描中插补缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（如T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用。需要解决缺失模态问题以充分利用多模态数据的优势。

Method: 使用条件去噪扩散概率模型（conditional denoising diffusion probabilistic model）从T1扫描中插补缺失的DWI扫描。通过广泛实验评估这种插补方法对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中准确性的提升效果。

Result: 观察到多种插补配置下多个指标的改善，特别是对少数类别敏感的指标。这表明通过插补缺失的DWI扫描可以提升阿尔茨海默病分类模型的性能。

Conclusion: 使用条件扩散模型插补缺失的DWI扫描是有效的，能够改善阿尔茨海默病分类性能，特别是在处理少数类别时表现更佳，为解决多模态数据不完整问题提供了可行方案。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [7] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 论文提出了基于悖论理论的责任AI治理框架(PRAIG)，将AI治理概念化为价值创造与风险缓解之间矛盾张力的动态管理，为组织提供既促进创新又控制风险的治理机制。


<details>
  <summary>Details</summary>
Motivation: 当前关于责任AI的研究存在碎片化问题，要么过于乐观强调价值创造，要么过度谨慎关注潜在危害。需要弥合这一差距，全面审视AI的双重性质，为组织提供平衡创新与风险的治理框架。

Method: 基于战略信息系统视角，通过系统综合责任AI文献并扎根于悖论理论，开发了悖论基础的责任AI治理(PRAIG)框架。该框架包含：AI采用的战略效益、固有风险和非预期后果、以及管理这些张力的治理机制。

Result: 提出了正式命题，证明权衡方法会加剧而非解决这些张力；开发了具有特定权变条件的悖论管理策略分类法；为实践者提供了既不抑制创新又不使组织面临不可接受风险的可操作治理指导。

Conclusion: 将责任AI治理概念化为价值创造与风险缓解之间矛盾张力的动态管理，推进了理论理解。论文最后提出了推进责任AI治理学术研究的研究议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [8] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离生成与确定性验证来提升LLM在临床出院计划中的安全性和可靠性，支持缓存和自改进循环。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院计划中表现出潜力，但受到幻觉、遗漏和置信度校准不佳的限制，需要提高安全性和可靠性。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor（确定性模块）评估多任务覆盖率、跟踪校准指标、监控行动分布漂移。支持两级自改进：1) 单次就诊内的重新生成；2) 跨就诊差异缓冲和重放。

Result: 自改进循环是性能提升的主要驱动力，任务覆盖率从32%提高到86%。校准显著改善，Brier/ECE分数降低，高置信度遗漏减少。差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架为使用可互操作的FHIR数据访问和确定性审计实现更安全的自动化出院计划提供了实用途径，支持可重复的消融实验和以可靠性为重点的评估。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [9] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 使用大语言模型为手语翻译生成释义变体作为合成替代参考，改善手语翻译的评估方法


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译语料库通常只提供一个书面语言参考译文，但手语和口语之间存在高度非同构关系，多个翻译可能同样有效。这种限制影响了模型训练和评估，特别是基于n-gram的BLEU等指标。

Method: 研究使用大语言模型自动生成书面语言翻译的释义变体作为手语翻译的合成替代参考。比较多种释义策略和模型，使用改进的ParaScore指标评估。研究释义对手语翻译训练和评估的影响，在YouTubeASL和How2Sign数据集上测试基于姿态的T5模型。

Result: 训练中简单加入释义不会改善翻译性能，甚至可能有害。但在评估中使用释义能提高自动评分并与人类判断更好对齐。为此提出了BLEUpara扩展指标，人类评估确认BLEUpara与感知翻译质量相关性更强。

Conclusion: 使用大语言模型生成释义变体作为合成替代参考能显著改善手语翻译的评估可靠性，特别是通过BLEUpara指标能更好地反映翻译质量。发布了所有生成的释义、生成和评估代码以支持可复现的手语翻译系统评估。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [10] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现不佳，但当内容对说话者具有个人意义时，模型对效价（情感正负）的预测性能显著提升


<details>
  <summary>Details</summary>
Motivation: 自我报告的情感标签反映内在体验，而第三方标签反映外部感知，两者常存在差异。这种差距在心理健康领域尤为关键，因为准确的自我报告建模对于指导干预至关重要。目前缺乏对第三方训练模型在自我报告数据上的跨语料库评估。

Method: 进行了首个跨语料库评估，将第三方训练的情感识别模型应用于自我报告数据。评估了模型在激活度（arousal）和效价（valence）两个维度上的表现，特别关注内容对说话者的个人意义对模型性能的影响。

Result: 激活度基本无法预测（一致性相关系数CCC约0），效价具有中等可预测性（CCC约0.3）。关键发现是：当内容对说话者具有个人意义时，模型对效价的预测性能显著提高（CCC约0.6-0.8）。

Conclusion: 个人意义是连接外部感知与内在体验的关键桥梁，为改进自我报告情感建模提供了重要方向。同时，自我报告激活度建模仍然是一个重大挑战，需要新的方法来解决。

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [11] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 论文针对大语言模型在金融定量推理中的"算术幻觉"和"认知崩溃"问题，提出了认知复杂度基准（CCB）和迭代双阶段金融PoT框架，通过架构解耦显著提升了金融推理的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语义任务上表现出色，但在金融定量推理中存在严重瓶颈，经常出现"算术幻觉"和系统性的"认知崩溃"失效模式，需要专门的方法来解决这些问题。

Method: 提出认知复杂度基准（CCB）评估框架，基于95份真实中国A股年报构建数据集，采用三维分类法（数据源、映射难度、结果单位）。为解决推理问题，提出迭代双阶段金融PoT框架，通过神经符号架构实现语义变量提取与逻辑制定的架构解耦，并将计算卸载到迭代自校正的Python沙箱中。

Result: 在CCB基准测试中，标准思维链方法在复杂任务上表现不佳，而提出的方法显著提升了鲁棒性，将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现了高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解与定量计算紧密对齐的精度关键领域提供了可迁移的架构洞察。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [12] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 该研究提出了一种新的平面几何问题解决方法，通过训练MLLM解释器将视觉图表转换为文本描述（CDL），然后使用现成的LLM进行推理，避免了直接端到端训练MLLM可能损害基础LLM推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过端到端微调MLLM来同时增强视觉理解和推理能力，但这种联合优化可能会损害基础LLM固有的推理能力。研究发现，如果能够将视觉信息适当转化为文本描述，LLM本身就可以成为强大的PGPS求解器。

Method: 提出两阶段方法：1）训练MLLM解释器将几何图表转换为条件声明语言（CDL）描述；2）使用现成的LLM基于CDL进行推理。采用CoT增强的SFT和GRPO训练MLLM解释器，并设计了CDL匹配奖励机制而非传统的基于解决方案的奖励。构建了Formalgeo7k-Rec-CoT数据集支持训练。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista数据集上的实验表明，该方法（仅使用5.5k数据微调）在性能上优于领先的开源和闭源MLLM。

Conclusion: 通过将视觉信息转化为文本描述，可以充分利用LLM强大的推理能力来解决平面几何问题，避免了端到端训练MLLM可能带来的推理能力损害问题，为多模态推理任务提供了一种有效的新方法。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [13] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准测试，包含奥林匹克竞赛和研究两个互补赛道，涵盖物理、化学、生物等多个学科领域


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试大多依赖选择题或已发表信息，模型在这些测试上的表现已接近饱和，无法有效评估专家级科学推理能力，因此需要开发新的评估框架

Method: 构建包含两个互补赛道的基准：1) 奥林匹克赛道：包含国际物理、化学、生物奥林匹克竞赛级别的问题；2) 研究赛道：包含博士级别的开放式研究子任务。所有问题均由领域专家（奥林匹克奖牌得主、国家队教练、博士科学家）编写和验证

Result: 开发了包含数百个问题的基准测试（其中160个为开源黄金集），涵盖从量子电动力学到合成有机化学等多个子领域，并为研究赛道引入了基于细粒度评分标准的评估框架

Conclusion: FrontierScience填补了现有科学基准测试的空白，能够更全面地评估语言模型在专家级科学推理和研究任务上的能力，为前沿模型的发展提供了重要的评估工具

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [14] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: MAD是一种无需训练的方法，通过自适应加权模态特定解码分支来减少多模态大语言模型中的跨模态幻觉，利用模型自评估模态相关性的能力来抑制跨模态干扰。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，即一个模态不适当地影响另一个模态的生成，导致输出虚构内容。这暴露了模态交互控制方面的根本缺陷。

Method: 提出模态自适应解码（MAD），这是一种无需训练的方法。它通过查询每个任务需要哪些模态来利用模型固有的自评估模态相关性能力，提取模态概率后自适应加权对比解码分支，使模型专注于相关信息同时抑制跨模态干扰。

Result: 在CMM和AVHBench上的大量实验表明，MAD显著减少了跨模态幻觉：VideoLLaMA2-AV分别提升7.8%和2.0%，Qwen2.5-Omni分别提升8.7%和4.7%。

Conclusion: 通过自评估实现显式的模态感知对于鲁棒的多模态推理至关重要，为现有对比解码方法提供了原则性扩展。代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [15] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 该研究引入"谄媚锚点"概念来定位和量化推理模型中的谄媚行为，通过分析超过10,000个反事实推理轨迹，发现可以在推理过程中可靠检测和量化这种倾向性承诺。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意错误的用户建议（谄媚行为），但尚不清楚这种行为在推理轨迹中的起源位置以及承诺强度。需要一种方法来定位和量化这种偏差行为。

Method: 引入"谄媚锚点"概念——能够因果性地将模型锁定在用户同意状态的句子。在蒸馏推理模型上分析超过10,000个反事实推理轨迹，使用线性探测和基于激活的回归器来检测和量化谄媚行为。

Result: 线性探测能以84.6%的平衡准确率区分谄媚锚点，激活回归器能预测承诺强度（R²=0.74）。发现谄媚锚点比正确推理锚点更易区分，且谄媚行为在推理过程中逐渐建立。

Conclusion: 研究提供了句子级别的机制来在推理过程中定位模型未对齐行为，揭示了干预的潜在窗口，为理解模型谄媚行为提供了量化分析框架。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [16] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入初始化时，相比基础模型在语义表示任务上没有性能优势，因为对比学习会导致流形重对齐


<details>
  <summary>Details</summary>
Motivation: 研究增强推理能力的模型（通过RLVR训练）是否能为嵌入模型提供更好的语义表示初始化，即推理能力提升是否能转化为语义表示优势

Method: 在MTEB和BRIGHT基准上评估RLVR调优模型作为嵌入初始化的性能，引入HRSA框架分析表示相似性，从表示、几何和功能三个层次分解相似性

Result: RLVR调优的嵌入初始化没有一致性能优势，HRSA分析显示RLVR引起局部几何重组和坐标基漂移，但保持全局流形几何和线性读出，对比学习导致流形重对齐

Conclusion: 与SFT不同，RLVR在现有语义景观内优化轨迹而非重构景观本身，因此推理能力提升不会转化为嵌入表示优势

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [17] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: ACQO是一个用于复杂查询优化的强化学习框架，通过自适应查询重构和排名-分数融合模块，结合课程强化学习策略，显著提升了RAG系统在复杂查询场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂用户查询通常需要多种并行和顺序搜索策略来处理歧义和分解。现有的RL方法主要关注单个查询的扩展和抽象，难以处理复杂查询场景。直接应用RL到复杂查询会面临搜索空间急剧扩大、奖励设计复杂和训练不稳定等挑战。

Method: 提出了自适应复杂查询优化（ACQO）框架，包含两个核心组件：自适应查询重构（AQR）模块动态决定何时将查询分解为多个子查询；排名-分数融合（RSF）模块确保稳健的结果聚合并提供稳定的奖励信号。采用课程强化学习（CRL）策略，通过两阶段渐进式训练来稳定学习过程。

Result: 在三个复杂查询基准测试中实现了最先进的性能，显著优于现有基线方法。该框架还展示了改进的计算效率，并与不同的检索架构具有良好的兼容性。

Conclusion: ACQO为下一代RAG系统提供了一个强大且可泛化的解决方案，能够有效处理现实世界中的复杂查询场景，通过自适应决策和稳定训练机制解决了现有RL方法在复杂查询优化中的关键挑战。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [18] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier是一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定的因果图中推导出来，从而更准确地评估LLM的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型因果推理能力的基准测试通常依赖字符串匹配或表面指标，无法捕捉模型输出在因果推理语义下的形式有效性。

Method: 提出DoVerifier，一个简单的符号验证器，使用do-calculus和概率论规则检查LLM生成的因果表达式是否可以从给定的因果图中推导出来。

Result: 在合成数据和因果问答基准测试上的评估表明，DoVerifier能更准确地捕捉因果推理轨迹的语义正确性，为评估LLM的因果推理能力提供了更严格和有意义的方法。

Conclusion: DoVerifier通过形式化验证因果表达式的可推导性，提供了比传统表面指标更严谨的LLM因果推理评估框架。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [19] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出一种双编码因果发现方法，通过互补编码策略和多数投票机制解决分类变量因果发现中的数值不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临条件独立性测试的数值不稳定性挑战，这限制了因果关系的准确识别和机器学习模型决策的解释

Method: 提出双编码因果发现方法：1）使用互补编码策略运行约束基算法；2）通过多数投票机制合并结果；3）在泰坦尼克数据集上应用验证

Result: 该方法在泰坦尼克数据集上识别出的因果结构与已建立的可解释方法一致，验证了方法的有效性

Conclusion: 双编码因果发现方法能够有效处理分类变量，提高因果发现的稳定性和准确性，为解释机器学习模型决策提供了可靠工具

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [20] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦算法结构推理与参数优化，结合树编辑距离保持结构多样性，集成LLM逻辑生成与差分变异进行参数调优，显著提升了启发式算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法进化视为单一文本生成任务，忽略了离散算法结构与连续数值参数之间的耦合关系，导致丢弃有前景的算法（因未校准常数）和过早收敛（因简单相似度度量）。

Method: 提出TIDE（Tuning-Integrated Dynamic Evolution）框架，采用嵌套架构：外层并行岛屿模型使用树相似编辑距离驱动结构多样性；内层循环集成LLM逻辑生成与差分变异算子进行参数调优；UCB调度器动态优先高收益提示策略优化资源分配。

Result: 在九个组合优化问题上的实验表明，TIDE发现的启发式算法在解质量上显著优于最先进基线方法，同时实现了改进的搜索效率和降低的计算成本。

Conclusion: TIDE通过解耦结构推理与参数优化，有效解决了现有方法在算法进化中的局限性，为自动化启发式设计提供了更高效的框架。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [21] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 该论文提出HYDRA框架，通过模块化专家库和不确定性感知融合，解决时间序列基础模型在安全关键CPS中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列和物理动力学基础模型取得了显著成功，但在安全关键CPS中部署时面临挑战：微调导致灾难性遗忘、残差谱偏差平滑高频故障特征、模型不透明难以满足安全标准验证要求。

Method: 提出模块化主权范式（HYDRA）：构建紧凑、冻结的特定机制专家库，通过不确定性感知融合机制组合这些专家，实现机制条件有效性、严格区分偶然性和认知不确定性、模块化可审计性。

Result: HYDRA框架为CPS生命周期中的鲁棒状态完整性提供了可认证路径，确保机制条件有效性、严格的不确定性分离和模块化可审计性。

Conclusion: 全局参数更新无法完全解决可塑性-稳定性悖论，模块化主权范式通过专家库和不确定性感知融合，为安全关键CPS提供了更可靠、可验证的解决方案。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [22] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD框架通过知识蒸馏将自动驾驶分解为感知-推理-规划三阶段，使用层特定注意力作为蒸馏信号，构建单教师和多教师蒸馏，显著提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是安全关键任务，现有大模型需要大量GPU内存和高推理延迟，而传统监督微调难以弥补小模型能力差距，需要更有效的知识迁移方法

Method: 将自动驾驶分解为感知、推理、规划三阶段，使用层特定注意力作为蒸馏信号构建能力特定的单教师模型，然后统一为多教师蒸馏框架，引入非对称梯度投影缓解梯度冲突

Result: 蒸馏后的InternVL3-1B模型比同系列78B预训练模型节省42倍GPU内存，吞吐量提升11.4倍，在DriveBench上整体性能更好，在规划维度超越GPT-5.1

Conclusion: Drive-KD框架通过知识蒸馏有效提升小模型在自动驾驶任务中的性能，为高效自动驾驶视觉语言模型提供了新思路

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [23] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 研究评估了12个LLM在10个创意提示上的输出方差来源，发现对于输出质量（原创性），提示解释了36.43%的方差，与模型选择（40.94%）相当；但对于输出数量（流畅度），模型选择（51.25%）和LLM内部方差（33.70%）占主导，提示仅解释4.22%。


<details>
  <summary>Details</summary>
Motivation: 理解LLM输出方差的主要来源：提示、模型选择还是采样随机性，以指导更可靠的评估方法。

Method: 评估12个LLM在10个创意提示上，每个提示采样100次（总计12,000个样本），分析输出方差来源。

Result: 对于输出质量（原创性）：提示解释36.43%方差，模型选择解释40.94%方差；对于输出数量（流畅度）：模型选择解释51.25%方差，LLM内部方差解释33.70%方差，提示仅解释4.22%方差。

Conclusion: 提示是控制输出质量的有力杠杆，但由于显著的LLM内部方差（10-34%），单样本评估可能混淆采样噪声与真实的提示或模型效应。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [24] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时序电子健康记录设计的检索增强生成框架，通过事件和时间感知的混合检索、自适应迭代检索以及双路径证据检索与推理，在四个长时序EHR预测任务中平均提升Macro-F1 10.76%


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含丰富的纵向临床证据，对医疗决策至关重要。然而，长时序EHRs常常超出大语言模型的上下文限制，现有方法通常依赖截断或简单检索策略，这会丢弃临床相关事件和时间依赖性。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知的混合EHR检索，以保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖范围；3）双路径证据检索与推理，联合检索和推理事实证据和反事实证据。

Result: 在四个长时序EHR预测任务上的实验表明，EHR-RAG始终优于最强的基于LLM的基线方法，平均Macro-F1提升了10.76%。

Conclusion: 这项工作突显了检索增强的大语言模型在结构化EHR数据上进行临床预测的潜力，为实际应用提供了有效的解决方案。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [25] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 本文提出了Ostrakon-VL，一个面向食品服务和零售商店场景的多模态大语言模型，通过ShopBench基准测试和QUAD数据清洗流程，在FSRS场景中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在通用感知和推理方面取得进展，但在食品服务和零售商店场景部署面临两大障碍：1) 真实FSRS数据噪声大且缺乏可审计的闭环数据管理；2) 现有评估协议缺乏统一、细粒度、标准化的基准测试。

Method: 1) 基于Qwen3-VL-8B开发Ostrakon-VL模型；2) 创建首个面向FSRS的公开基准测试ShopBench；3) 提出QUAD多阶段多模态指令数据清洗流程；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模的开源MLLM中达到最先进水平，超越Qwen3-VL-235B-A22B (+0.7)和同规模Qwen3-VL-8B (+4.8)，参数效率显著提升。

Conclusion: Ostrakon-VL在FSRS场景中提供了更稳健可靠的感知和决策能力，作者将公开释放模型和基准测试以促进可重复研究。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [26] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架将潜在推理重新定义为规划，通过解耦推理和语言化，让模型能动态决定何时终止推理，而不是依赖固定超参数。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通常作为从显式推理步骤到潜在状态的不透明端到端映射，且推理时需要预定义潜在步骤数量，限制了推理的灵活性和可扩展性。

Method: PLaT框架将推理建模为潜在规划状态的确定性轨迹，同时使用单独的Decoder在必要时将这些思想转化为文本，实现推理与语言化的解耦。

Result: 在数学基准测试中，PLaT虽然贪心准确率低于基线，但表现出更好的推理多样性可扩展性，表明它学习了更鲁棒、更广泛的解空间。

Conclusion: PLaT为推理时搜索提供了透明且可扩展的基础，通过解耦推理和语言化实现了更灵活的推理过程控制。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [27] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI框架通过动态参数插值在直觉型System 1和深思型System 2模型间切换，实现能力控制而非输出控制，在数学推理任务上达到比Thinking模型更高的准确率同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注输出控制（限制模型输出长度），但这只是认知配置的症状而非根本原因。需要从输出控制转向能力控制，调节模型如何思考而非产生什么内容。

Method: 利用现有Instruct和Thinking检查点通过动态参数插值，无需额外训练。提出DAMI框架，通过查询特定的推理强度λ(q)配置认知深度。提供两种λ(q)估计方法：基于训练的偏好学习编码准确性和效率标准，以及零样本部署的基于置信度方法利用模型间认知差异。

Result: 在五个数学推理基准测试中，DAMI实现了比Thinking模型更高的准确率，同时保持效率，有效结合了System 1的效率和System 2的推理深度。

Conclusion: 通过动态模型插值实现能力控制是可行的，能够在不牺牲效率的情况下提升推理性能，为统一语言模型的认知模式适应提供了新方向。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [28] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在理解否定指令时存在严重缺陷，经常将"不应该做X"误解为"应该做X"，这在伦理场景中可能导致危险后果。


<details>
  <summary>Details</summary>
Motivation: 当用户告诉AI系统某人"不应该"采取某个行动时，系统应该将其视为禁令。然而许多大型语言模型却相反：它们将否定指令解释为肯定。这揭示了当前对齐技术与安全部署要求之间的差距。

Method: 审计了16个模型在14个伦理场景中的表现，测试简单否定和复合否定下的响应。使用确定性解码排除采样噪声，提出否定敏感指数(NSI)作为治理指标，并设计分层认证框架。

Result: 开源模型在简单否定下77%的时间支持被禁止的行动，在复合否定下达到100% - 比肯定框架增加317%。商业模型表现较好但仍存在19-128%的波动。金融场景的脆弱性是医疗场景的两倍。

Conclusion: 无法可靠区分"做X"和"不做X"的模型不应在高风险环境中做出自主决策。需要开发能够正确处理否定指令的模型，并提出NSI指标和分层认证框架来确保安全部署。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [29] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 研究发现LLMs存在"鲁棒性悖论"：尽管对提示词微小变化敏感，但在情绪框架效应下表现出惊人的行为不变性，比人类抵抗叙事操纵的能力强110-300倍。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLMs对提示词微小扰动敏感且易与用户偏见保持一致，但其在重要规则约束决策中的鲁棒性尚未充分探索。研究旨在揭示LLMs在情绪框架效应下的真实表现。

Method: 使用新颖的受控扰动框架，在医疗、法律和金融三个高风险领域进行实验，量化LLMs与人类在叙事操纵下的表现差异，并比较不同训练范式模型的稳定性。

Result: LLMs表现出近乎零的情绪框架效应（Cohen's h = 0.003），而人类则显示出显著偏见（Cohen's h在[0.3, 0.8]之间）。LLMs抵抗叙事操纵的能力比人类强110-300倍，且这种不变性在不同训练范式的模型中持续存在。

Conclusion: 指令调优的LLMs能够将逻辑规则遵循与说服性叙事解耦，提供了一种决策稳定性来源，可在制度环境中补充甚至去偏人类判断。研究发布了包含162个场景的基准测试、代码和数据。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [30] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 该论文提出了一个用于AI辅助芯片设计的综合基准测试ChipBench，针对现有基准测试饱和且任务多样性不足的问题，在Verilog生成、调试和参考模型生成三个关键任务上评估LLMs，结果显示最先进模型表现远低于现有饱和基准的95%通过率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在硬件工程中显示出巨大潜力，但现有基准测试存在饱和问题和任务多样性有限，无法反映LLMs在真实工业工作流程中的性能表现。

Method: 提出了一个全面的AI辅助芯片设计基准测试，包含三个关键任务：1) Verilog生成（44个具有复杂层次结构的现实模块）；2) 调试（89个系统化调试案例）；3) 参考模型生成（132个样本，涵盖Python、SystemC和CXXRTL）。同时提供了用于高质量训练数据生成的自动化工具箱。

Result: 评估结果显示显著的性能差距：最先进的Claude-4.5-opus在Verilog生成上仅达到30.74%，在Python参考模型生成上仅达到13.33%，远低于现有饱和基准测试中SOTA模型超过95%的通过率。

Conclusion: 该基准测试揭示了LLMs在芯片设计任务中的实际挑战，填补了现有基准测试的不足，并提供了促进未来研究的工具和数据生成方法。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [31] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值确定聚类数量，结合词汇和语义指标选择高质量主题关键词，相比Top2Vec和BERTopic在聚类恢复和描述方面表现更好，且对样本大小和超参数变化更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前流行的聚类主题建模方法（Top2Vec和BERTopic）存在两个主要问题：1）对样本大小和超参数极其敏感，难以发现自然聚类；2）关键词重要性估计存在缺陷——BERTopic忽略关键词与主题向量的语义距离，Top2Vec忽略词频，导致主题不连贯、缺乏多样性。

Method: 提出Topeax方法：1）通过密度估计峰值自动发现聚类数量；2）结合词汇指标（词频）和语义指标（关键词与主题向量的语义距离）来评估术语重要性，从而获得高质量的主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数的变化表现出更稳定、更少异常的行为。

Conclusion: Topeax解决了现有聚类主题建模方法的关键缺陷，提供了一种更可靠、更稳定的主题发现和描述方法，能够更好地发现自然聚类并生成高质量的主题关键词。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [32] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim：基于LLM的大规模多智能体模拟内存优化系统，通过智能预取和优先级驱逐实现1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体模拟面临GPU内存瓶颈，每个智能体需要维护私有GPU状态（模型、前缀缓存、适配器），随着智能体数量增加，设备内存迅速耗尽。现有系统难以扩展。

Method: 提出"调用距离"统一抽象来估计智能体未来LLM请求的相对顺序，基于此实现ScaleSim系统，支持主动预取和基于优先级的驱逐，通过模块化接口支持多样化的智能体特定内存管理。

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速，显著提升了大规模多智能体模拟的可扩展性。

Conclusion: ScaleSim通过创新的内存管理策略有效解决了LLM多智能体模拟的内存扩展问题，为大规模模拟应用提供了高效解决方案。

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [33] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM系统的讨论组织成显式论证图，通过因果模型分析哪些论证链对最终决策是必要的，并提供校正机制对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动了最终决策，缺乏透明度和可解释性。

Method: 引入ARGORA框架，将多专家讨论组织成显式论证图（显示论证间的支持/攻击关系），将这些图作为因果模型，通过系统性地移除单个论证并重新计算结果，识别必要的推理链；还引入校正机制，当内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放式用例中，ARGORA实现了有竞争力的准确性，并表现出校正行为：当专家最初存在分歧时，该框架更倾向于将争议解决为正确答案而非引入新错误，同时提供决定性论证的因果诊断。

Conclusion: ARGORA通过将多专家讨论组织成显式论证图并作为因果模型分析，提高了LLM系统的可解释性和决策透明度，能够识别关键论证链并提供有效的争议解决机制。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [34] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 该论文首次从理论上分析了学习内化中间推理步骤的难度，证明了高阶逻辑依赖的学习信号会指数衰减，并提出了ALiCoT框架来克服这一问题，实现了54.4倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 虽然思维链（CoT）通过中间步骤解锁了大语言模型的高级推理能力，但生成额外token带来了高昂的计算成本。最近研究表明将推理步骤压缩到潜在状态（隐式CoT压缩）提供了token高效的替代方案，但其机制尚不清楚。

Method: 1. 引入Order-r Interaction理论框架，证明学习高阶逻辑依赖时信号会指数衰减；2. 创建NatBool-DAG基准测试，强制不可约逻辑推理并消除语义捷径；3. 提出ALiCoT框架，通过将潜在token分布与中间推理状态对齐来克服信号衰减。

Result: ALiCoT在保持与显式CoT相当性能的同时，实现了54.4倍的推理加速。实验验证了理论分析的正确性，表明ALiCoT能够成功解锁高效推理。

Conclusion: 该研究首次从理论上解释了CoT压缩的机制，证明了跳过中间步骤会导致高阶交互障碍，并提出了有效的解决方案ALiCoT，为高效推理模型的发展提供了理论基础和实践框架。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [35] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度递归注意力混合机制，结合序列注意力、深度注意力和稀疏专家注意力，解决了深度递归模型中隐藏层大小瓶颈问题，在语言推理任务上实现了更高效的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度递归方法存在三个主要问题：缺乏FLOP、参数和内存匹配的基线；由于部分固定层堆栈导致深度递归利用不足；恒定隐藏层大小限制了多步潜在推理。需要解决这些瓶颈来提升深度递归模型的效率和效果。

Method: 提出Dreamer框架，结合三种注意力机制：序列注意力、深度注意力和稀疏专家注意力。通过深度方向的注意力缓解隐藏层大小瓶颈，解耦缩放维度，使深度递归模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，Dreamer模型达到相同准确率所需的训练token减少2-8倍；使用相同训练token时，性能优于约2倍大小的SOTA模型。专家选择多样性比SOTA MoE高2-11倍。

Conclusion: Dreamer框架通过深度递归注意力混合机制有效解决了深度递归模型的瓶颈问题，实现了更高效的训练和更好的推理性能，为深度递归模型的设计提供了新的思路。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [36] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出了一种主动潜在规划方法，通过条件变分自编码器平滑潜在空间，并使用强化学习优化潜在推理策略，在LLaMA-1B上实现了更高的准确率和更少的token消耗。


<details>
  <summary>Details</summary>
Motivation: 当前潜在推理方法通过模仿语言标签来监督潜在token，但由于同一问题可能存在多个等价但不同的CoT标签，被动模仿可能导致次优的潜在token表示和推理策略，限制了潜在空间的规划能力，并造成训练与测试之间的差距。

Method: 提出ATP-Latent方法：1）使用条件变分自编码器对潜在token的监督过程建模，获得更平滑的潜在空间；2）通过强化学习优化潜在推理策略，使用基于VAE解码内容一致性的辅助连贯性奖励来引导RL过程。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线方法实现了+4.1%的准确率提升和-3.3%的token消耗减少。

Conclusion: 强调潜在token表示空间的主动规划对实现最优潜在推理策略的重要性，ATP-Latent通过结合条件VAE和强化学习，有效提升了潜在推理的效率和性能。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [37] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该研究将企业级智能文档处理系统的验证问题形式化为基于搜索的软件测试问题，旨在在有限预算内最大化发现不同的故障类型，通过多种搜索策略对比发现不同求解器具有互补性，需要组合策略才能实现稳健验证。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证需要在有限预算下发现多样化的故障机制，而不是寻找单个最坏情况文档，这需要新的测试方法。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，生成结构风险特征实例以诱导真实故障条件。对比了进化算法、群体智能、质量多样性、学习型和量子等多种搜索策略，在相同预算约束下进行基准测试。

Result: 通过配置级排他性、胜率和跨时间重叠分析发现，不同求解器能持续发现特定替代方法在相似预算下无法发现的故障模式。跨时间分析显示所有评估预算中都存在持续的求解器特定发现，没有单一策略表现出绝对优势。虽然所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 研究结果表明求解器具有内在互补性，这为基于组合策略的基于搜索的软件测试方法提供了动机，以实现稳健的工业智能文档处理系统验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [38] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出了WhatCounts测试框架，专门评估LLM在计数任务中是否受语义内容影响，发现前沿LLM的准确率因计数对象类型不同而有超过40%的波动，表明LLM并非真正实现算法而是近似实现，且这种近似依赖于输入参数的意义。


<details>
  <summary>Details</summary>
Motivation: 算法的行为应该与其参数的语义内容无关，但现有研究往往将语义敏感性与推理复杂性或提示变化混为一谈。作者希望开发一个能够独立测试LLM是否真正实现算法不变性的框架。

Method: 提出了WhatCounts测试框架，这是一个原子化的测试方法：让LLM计数一个明确、分隔、无重复、无干扰项的列表中的项目。通过设计不同语义类型（如城市vs化学品、名称vs符号）但结构完全相同的计数任务，来隔离语义影响。

Result: 前沿LLM在计数准确率上表现出超过40%的波动，这种差异完全取决于计数对象的语义类型。控制实验排除了其他干扰因素，证实这种差距是语义性的。此外，少量无关的微调会导致这种语义差距发生不可预测的变化。

Conclusion: LLM并非真正实现算法，而是近似实现算法，且这种近似依赖于输入参数的意义。这种语义依赖性不仅限于计数任务，任何LLM函数都可能隐藏着对其输入意义的依赖，这在智能体应用中具有重要影响。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [39] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估深度研究工作流的仿真环境，通过静态学术文献语料库和确定性检索解决API依赖带来的不可重复性问题


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型已从单轮问答发展到深度研究工作流，但依赖实时API会引入非确定性（时间漂移、速率限制、后端状态变化），这破坏了可重复性并使跨系统比较失效

Method: 将工作流组件解耦为查询规划、工具调用和相关性评估，基于包含57万篇论文的静态语料库构建确定性检索环境，提供2536个带有专家标注真实标签的查询

Result: 通过在不同骨干模型上的实验，揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用

Conclusion: ScholarGym为深度研究工作流提供了可重复的评估框架，支持对查询规划、工具调用和相关性评估各阶段的细粒度分析

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [40] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证基准测试，用于评估多模态大语言模型在13个现实对话领域中的时序理解和社交鲁棒性，包含4,958个标注和人口统计元数据。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型研究主要关注静态图像理解，而对处理序列音频-视频数据的能力探索不足，需要高质量基准测试来系统评估模型在现实场景中的表现。

Method: 创建SONIC-O1基准测试，涵盖13个现实对话领域，包含4,958个人工验证的标注和人口统计元数据。评估任务包括开放式摘要、多项选择题回答、以及带推理支持的时序定位。

Result: 实验显示闭源和开源模型在多项选择题准确率上差距较小，但在时序定位任务上存在22.6%的显著性能差距。模型在不同人口统计群体上的表现进一步下降，表明模型行为存在持续差异。

Conclusion: SONIC-O1为时序基础和社交鲁棒的多模态理解提供了开放评估套件，揭示了当前模型的局限性，特别是时序理解和公平性方面的不足。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [41] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 本文提出了一种名为TCAP的无监督防御框架，用于检测MLLMs微调服务中的后门攻击，通过分析注意力分配偏差来识别中毒样本。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs微调服务存在后门攻击风险，现有防御方法要么依赖监督信号，要么无法适应不同触发器和模态的多样性，需要一种更通用的无监督防御方案。

Method: 提出Tri-Component Attention Profiling (TCAP)框架：1) 将跨模态注意力图分解为三个功能组件（系统指令、视觉输入、用户文本查询）；2) 使用高斯混合模型统计分析识别触发器响应注意力头；3) 通过基于EM的投票聚合隔离中毒样本。

Result: 在多种MLLM架构和攻击方法上的广泛实验表明，TCAP能够持续实现强大的性能，成为MLLMs中稳健实用的后门防御方案。

Conclusion: TCAP通过发现后门攻击的通用指纹——注意力分配偏差，提供了一种有效的无监督防御框架，能够适应不同触发器和模态的多样性，为MLLMs微调服务的安全性提供了可靠保障。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [42] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入可训练的因果循环，结合前瞻注意力窗口、分块头和跳过门三个模块，在不增加参数的情况下改善了语言模型的质量-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理主要依赖逐token自回归，缺乏人类阅读的关键要素：内容自适应前瞻、分块结构感知的计算分配以及训练-测试一致性。现有加速方法只是修补现有流程，未能从根本上解决这些问题。

Method: 提出Fovea-Block-Skip Transformer，通过三个核心模块注入可训练的因果循环：Parafovea-Attention Window（前瞻注意力窗口）、Chunk-Head（分块头）和Skip-Gate（跳过门）。

Result: 在多样化基准测试中，FBS改善了质量-效率权衡而不增加参数，消融实验显示三个模块具有互补性。

Conclusion: FBS Transformer通过模拟人类阅读机制，为LLM推理提供了更高效的架构设计，在保持质量的同时显著提升效率。

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [43] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS是一种模型无关的插件，通过样本自适应dropout机制动态校准模型学习能力，在保持细粒度保真度的同时抑制虚假波动，提升深度时间序列模型对噪声数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中普遍存在的噪声数据使深度时间序列模型变得脆弱。现有的鲁棒性策略要么剪枝数据，要么依赖昂贵的先验量化，无法在有效性和效率之间取得平衡。

Method: DropoutTS采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率来动态校准模型学习能力，选择性抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试上的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过将学习范式从"学什么"转变为"学多少"，提供了一种模型无关的鲁棒性增强方案，在保持效率的同时有效应对时间序列数据中的噪声问题。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [44] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: 本文提出Epistemic Context Learning (ECL)框架，通过历史交互构建同伴可信度档案，使多智能体系统中的个体能够识别可靠同伴并避免盲目从众，显著提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的个体智能体往往缺乏鲁棒性，容易盲目跟随误导性同伴。这种弱点源于谄媚行为和对同伴可靠性评估能力的不足。需要让智能体能够基于历史交互评估同伴可信度，从而在不确定时向可信同伴学习。

Method: 首先形式化历史感知参考的学习问题，将同伴的历史交互作为额外输入。然后开发Epistemic Context Learning (ECL)推理框架：基于历史构建明确的同伴档案，使预测条件化于这些档案。进一步通过强化学习使用辅助奖励优化ECL。

Result: ECL使小模型如Qwen 3-4B能够超越历史无关基线模型Qwen 3-30B（8倍大小），通过准确识别可靠同伴。ECL还将前沿模型提升至接近完美（100%）性能。ECL在各种多智能体配置中泛化良好，且发现LLM对信任建模良好，信任建模准确性与最终答案质量呈强相关。

Conclusion: ECL框架通过历史交互构建同伴可信度档案，有效解决了多智能体系统中的盲目从众问题。该方法不仅显著提升了小模型的性能，还揭示了信任建模在多智能体协作中的关键作用，为构建更鲁棒的多智能体系统提供了新思路。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [45] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: ZSSD是一个零样本统计降尺度框架，无需配对训练数据，通过物理一致的气候先验和统一坐标指导解决传统方法的泛化问题和物理不一致性。


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法由于缺乏配对训练数据以及相对于再分析数据的领域差距，难以泛化到全球气候模型。同时，现有零样本方法存在物理不一致性和在大尺度因子下的梯度消失问题。

Method: 提出零样本统计降尺度框架，利用从再分析数据学习的物理一致气候先验，结合地球物理边界和时序信息确保物理有效性。引入统一坐标指导策略解决DPS中的梯度消失问题，确保与大尺度场的一致性。

Result: ZSSD在99百分位误差上显著优于现有零样本基线方法，能够成功重建复杂天气事件（如热带气旋），并在异质GCMs中表现良好。

Conclusion: ZSSD为零样本气候降尺度提供了一种有效的解决方案，通过物理约束和统一坐标指导解决了现有方法的局限性，实现了对全球气候模型的泛化能力。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [46] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出一个基于概念空间框架的时序抽象概念建模方法，以国际象棋为案例，将策略概念表示为可解释质量维度上的几何区域，通过游戏轨迹分析识别意图策略。


<details>
  <summary>Details</summary>
Motivation: 将概念空间理论扩展到随时间实现的、目标导向的抽象概念，为涉及顺序决策的广泛应用奠定基础，并支持与知识演化机制集成以学习和完善抽象概念。

Method: 采用概念空间框架，将策略概念（如攻击或牺牲）表示为可解释质量维度上的几何区域，将国际象棋游戏实例化为轨迹，通过轨迹在概念空间中的方向性运动来识别意图策略，并支持双视角建模以捕捉玩家对相同情况的不同解释。

Result: 实现证明了基于轨迹的概念识别的可行性，运动模式与专家评论一致，展示了概念空间理论可以扩展到时间实现的、目标导向的概念。

Conclusion: 该方法为涉及顺序决策的广泛应用建立了基础，支持与知识演化机制集成以学习和完善抽象概念，为建模和理解复杂决策过程中的抽象概念提供了新框架。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [47] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 该研究提出了一个以LLM为中心的框架，用于气管内吸痰(ES)训练的视频活动识别和反馈生成，在识别准确率上比传统方法提升15-20%，并提供了可解释的自动化反馈系统。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰是一项高风险但必要的临床操作，在家庭护理和教育环境中缺乏有效监督。目前针对ES训练的自动识别和反馈系统研究不足，需要开发更有效的培训工具来提升护理教育质量和患者安全。

Method: 提出了一个统一的、以大型语言模型(LLM)为中心的框架，将LLM作为核心推理模块，从视频数据中执行时空活动识别和可解释的决策分析。该框架还包括基于异常检测和可解释AI(XAI)原理的学生支持模块，能够生成自然语言反馈。

Result: 实验结果表明，提出的LLM方法优于传统机器学习和深度学习方法，在准确率和F1分数上提升了约15-20%。框架能够提供自动化的、可解释的反馈，突出正确操作并建议针对性改进。

Conclusion: 该研究建立了一个可扩展、可解释、数据驱动的基础，用于推进护理教育、提高培训效率，并最终改善患者安全。LLM框架在医疗技能培训中展现出显著优势。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [48] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 该研究提出了一个全面的心电图专家基础模型基准测试框架，结合性能评估和表示层分析，用于评估模型在不同数据集和数据可用性条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型在AI辅助心电图解读领域开始应用，但现有基准测试主要关注下游性能评估，缺乏对模型嵌入表示通用性的深入分析。在医疗等错误敏感领域，需要更全面地评估基础模型的泛化能力。

Method: 提出了一个基准测试方法学，将基于性能的评估与表示层分析相结合，利用SHAP和UMAP技术。对多个通过最先进技术预训练的心电图专家基础模型进行了广泛评估，涵盖不同跨大陆数据集和数据可用性设置（包括数据稀缺场景）。

Result: 实验结果表明，该基准测试协议能够深入洞察心电图专家基础模型的嵌入模式，使研究人员能够更深入地理解其表示结构和泛化能力。

Conclusion: 该研究填补了心电图专家基础模型基准测试的空白，提供了一个全面的评估框架，有助于更负责任地在医疗领域应用基础模型，特别是在数据稀缺的现实医疗场景中。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [49] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 该研究提出了一个决策中心的仿真框架，用于评估汽车售后市场备件库存预测模型对运营绩效（而非仅统计准确性）的实际影响。


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场的备件需求具有高度间歇性和不确定性，传统预测模型评估仅关注统计准确性（如MAE、RMSE），但这些指标与实际运营绩效（如总成本和服务水平）的关系不明确，需要建立更相关的评估框架。

Method: 提出了一个决策中心的仿真软件框架，包含三个核心模块：(1) 针对备件需求特征设计的合成需求生成器；(2) 可容纳任意预测模型的灵活预测模块；(3) 消耗预测并计算运营KPI的库存控制仿真器。

Result: 通过广泛的仿真场景表明，传统准确性指标的改善不一定转化为更好的运营绩效，统计误差相似的不同模型可能产生显著不同的成本-服务权衡结果。

Conclusion: 该框架将需求预测与库存管理联系起来，将评估重点从纯粹的预测准确性转向运营相关性，为模型选择提供指导，适用于汽车售后市场及相关领域。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [50] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias是一种轻量级LLM去偏框架，通过增强而非抑制编码偏见知识的神经元来实现去偏，仅需少量是/否问题且无需重新训练


<details>
  <summary>Details</summary>
Motivation: 现有LLM去偏方法主要通过修改参数、提示或与偏见行为相关的神经元来抑制偏见，但这些方法通常脆弱、泛化能力弱、数据效率低，且容易降低模型的一般能力

Method: 通过基于归因的分析识别编码偏见知识的神经元，使用少量偏见知识问题，在推理时选择性地增强这些神经元，而不是抑制它们

Result: 在多个基准测试和LLM上展示了一致的SOTA去偏性能，同时保持了最小的一般能力退化，能够泛化到不同类型的偏见和人口统计特征

Conclusion: KnowBias提供了一种轻量级、数据高效的去偏方法，通过增强偏见知识神经元而非抑制它们，实现了强大的去偏效果同时保持模型的一般能力

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [51] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文介绍了将大型语言模型（LLM）集成到ASTRA编程语言中的原型开发经验，通过三个示例实现展示了传统Agent工具包与新兴Agentic AI平台的相互影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和Agentic AI（多智能体系统形式）的出现，需要探索这些新技术如何影响传统Agent工具包的使用，以及传统工具包中积累的丰富经验如何影响新Agent平台的设计。

Method: 为ASTRA编程语言开发了一个LLM集成原型，通过三个具体的示例实现来展示集成方法和应用场景。

Result: 成功实现了LLM与ASTRA编程语言的集成原型，并通过三个示例展示了该集成的可行性和应用潜力。

Conclusion: 通过原型开发经验，展示了传统Agent工具包与新兴Agentic AI平台之间的相互影响和协同作用，为未来Agent系统设计提供了有价值的见解。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [52] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 论文提出认知启发的两阶段训练框架CoMT+CCRL，通过分离抽象策略学习和具体任务执行来模拟人类认知过程，相比传统方法显著提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法将完整推理轨迹作为基本单元进行优化，这与人类实际解决问题的认知过程存在根本差异。人类认知自然地将问题解决分解为两个阶段：首先获取跨问题泛化的抽象策略（元知识），然后将其适应到具体实例。现有方法的问题中心化方式将抽象策略与具体执行纠缠在一起，导致泛化能力受限。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适应，防止过度自信的错误级联，提高执行可靠性。

Result: 在四个模型和八个基准测试上的实验显示，相比标准方法，在分布内提升2.19%，分布外提升4.63%，同时减少65-70%的训练时间和50%的token消耗。

Conclusion: 将后训练与人类认知原则对齐不仅能产生更优的泛化能力，还能显著提升训练效率，证明了认知启发的训练框架的有效性。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [53] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG提出了一种过程监督强化学习框架，通过整合步骤级监督来优化检索增强生成系统，解决了传统结果导向RL中的奖励稀疏性和信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法在复杂推理任务中存在奖励稀疏和信用分配效率低下的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"现象，即模型通过有缺陷的逻辑或冗余检索步骤得到正确答案。

Method: ProRAG框架包含四个阶段：1)监督策略预热，用结构化推理格式初始化模型；2)构建基于MCTS的过程奖励模型来量化中间推理质量；3)PRM引导的推理精炼，使策略与细粒度过程偏好对齐；4)过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励和全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比基于结果和过程感知的RL基线方法取得了更优的整体性能，特别是在复杂长视野任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过整合学习到的步骤级监督到在线优化循环中，为检索增强生成的强化学习优化提供了更精确的反馈机制，有效解决了过程幻觉问题，提升了复杂推理任务的性能。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [54] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: SCMA是一个多智能体强化学习框架，通过选择性惩罚冗余推理块来压缩大型推理模型的响应长度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型中的冗余推理增加了推理开销，影响交互体验并阻碍部署。现有的基于强化学习的解决方案通过长度惩罚与结果奖励相结合，但简单的奖励权重难以平衡简洁性与准确性，强制简洁可能会损害关键推理逻辑。

Method: 提出了一个多智能体强化学习框架SCMA，包含三个智能体：1) 分割智能体将推理过程分解为逻辑块；2) 评分智能体量化每个块的重要性；3) 推理智能体在训练期间接受重要性加权的长度惩罚，激励其优先考虑基本逻辑而不增加部署时的推理开销。

Result: 实证评估显示，SCMA在不同模型规模上减少了11.1%到39.0%的响应长度，同时将准确性提高了4.33%到10.02%。消融研究和定性分析验证了MARL框架内的协同优化促进了涌现行为，产生了比传统RL范式更强大的大型推理模型。

Conclusion: SCMA框架通过多智能体协作，能够有效识别和惩罚冗余推理块，同时保留关键推理逻辑，实现了推理过程的压缩和准确性提升，为大型推理模型的部署提供了更高效的解决方案。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [55] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，用于高风险表格决策任务，通过定义明确的角色（检察官、辩护律师、法官）、7轮结构化辩论协议和私有推理策略，提供透明可控的推理过程。


<details>
  <summary>Details</summary>
Motivation: 针对高风险决策任务中黑盒方法缺乏透明性和可控性的问题，需要一种能够提供可审计推理过程、支持人类监督的决策框架，特别是在刑事司法等伦理复杂的敏感领域。

Method: 提出角色结构化的多智能体辩论框架，包含检察官、辩护律师、法官三个明确角色，采用7轮结构化辩论协议，每个智能体使用私有推理策略，生成完整的交互记录用于可解释性分析。

Result: 在NLSY97数据集上的年轻成人再犯预测任务中，与传统的思维链提示相比，结构化多智能体辩论表现出更稳定和可泛化的性能，准确率和F1分数之间具有更强的相关性。

Conclusion: AgenticSimLaw不仅提升了性能，还提供了对推理步骤的细粒度控制、完整的交互记录用于可解释性，以及智能体行为的系统分析能力。该框架可推广到任何需要透明度和人类监督的高风险决策任务。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [56] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 该研究探讨了领域模型特征对经典规划器能耗的影响，提出了一个领域模型配置框架，通过实验发现领域级修改能产生可测量的能耗差异，且能耗与运行时间并不总是相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究主要关注算法性能（如机器学习精度或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能效问题很少受到关注。领域模型与算法的分离为通过领域模型设计系统分析能耗提供了机会。

Method: 引入了一个领域模型配置框架，可以控制特征变化（如元素排序、动作元数、死端状态等）。使用5个基准领域和5个最先进的规划器，对每个基准的32个领域变体进行了能耗和运行时间影响分析。

Result: 结果表明，领域级修改在不同规划器之间产生了可测量的能耗差异。能耗与运行时间并不总是相关，这意味着优化运行时间不一定能优化能耗。

Conclusion: 领域模型设计对规划器能耗有显著影响，这为绿色AI在自动规划领域的应用提供了实证基础。研究强调了在评估规划器性能时考虑能耗的重要性，并展示了通过领域模型优化来减少计算资源消耗的潜力。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [57] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出两种多智能体演员-评论家方法（MAAC）优化去中心化LLM协作，比较了集中式评论家（CoLLM-CC）和去中心化评论家（CoLLM-DC）在不同任务场景下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有MARL微调方法依赖预定义执行协议且多为集中式执行，而去中心化LLM协作更具实用价值。当前方法使用蒙特卡洛方法存在高方差问题，需要更多训练样本。演员-评论家方法在MARL中能有效解决这些问题。

Method: 提出两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），用于优化去中心化LLM协作。分析这些方法在何时以及为何有效。

Result: 在写作、编码和游戏领域实验中，蒙特卡洛方法和CoLLM-DC在短期视野和密集奖励任务中能达到与CoLLM-CC相当的性能。但在长期视野或稀疏奖励任务中，两者表现均不如CoLLM-CC，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛。

Conclusion: 集中式评论家方法（CoLLM-CC）在复杂的长视野或稀疏奖励任务中表现更优，而去中心化评论家方法（CoLLM-DC）和蒙特卡洛方法更适合简单任务。这为选择适合的LLM协作优化方法提供了指导。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [58] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 研究发现语言模型存在陈述-揭示偏好差距，且该相关性高度依赖评估协议设计，特别是中立性和弃权选项的设置会显著影响结果


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖二元强制选择提示，这会将真实偏好与评估协议的人工因素混为一谈，需要系统研究评估协议如何影响陈述-揭示偏好的相关性

Method: 系统研究24个语言模型中评估协议对陈述-揭示偏好的影响，包括允许中立和弃权选项、排除弱信号、使用系统提示引导等方法

Result: 1) 在陈述偏好评估中允许中立和弃权并排除弱信号可显著提高相关性；2) 在揭示偏好中允许弃权会使相关性降至接近零或负值；3) 使用系统提示引导在AIRiskDilemmas上不能可靠提高相关性

Conclusion: 陈述-揭示偏好的相关性高度依赖评估协议设计，偏好评估需要能够处理不确定偏好的方法，不能简单依赖二元强制选择

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [59] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个用于足球事件数据验证的系统框架，通过状态转移模型检测和纠正逻辑不一致性，提高数据分析的可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误或缺失事件），限制了分析模型的可靠性。

Method: 提出VERSA验证框架，基于状态转移模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。

Result: 对K League 1（2024赛季）事件数据的检查发现18.81%的事件存在逻辑不一致；VERSA显著提高了跨数据提供者的一致性，并提升了下游任务VAEP的性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性，VERSA框架为足球事件数据提供了系统化的完整性保障。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [60] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 该论文提出了"液态接口"概念，将传统静态接口转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的自主智能体协调。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持具有自适应、概率性和上下文相关推理能力的自主智能体，而系统集成仍被静态接口和确定性契约所主导，需要新的协调范式来解决这一矛盾。

Method: 提出了液态接口协调范式，将接口定义为短暂的关系事件而非持久技术构件；形式化了该模型并设计了液态接口协议(LIP)，管理意图驱动的交互、协商执行以及在语义不确定性下的短暂性执行；讨论了治理影响并描述了参考架构。

Result: 液态接口为基于智能体的系统提供了自适应协调的原则性基础，液态接口协议(LIP)能够管理意图驱动的交互和协商执行，参考架构证明了该方法的实际可行性。

Conclusion: 液态接口通过将接口重新定义为运行时产生的短暂关系事件，为解决自主智能体系统协调问题提供了新的范式，能够支持自适应、概率性和上下文相关的推理需求。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [61] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个针对车载语音助手的LLM智能体基准测试，专注于评估一致性、不确定性处理和能力意识，包含幻觉任务和消歧任务，揭示现有模型在真实场景中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准测试主要关注理想化设置下的任务完成度，忽视了在真实用户场景中的可靠性。车载语音助手等实际应用中，用户经常提出不完整或模糊的请求，智能体需要通过对话、工具使用和策略遵守来管理这种内在不确定性。

Method: 引入CAR-bench基准测试，包含LLM模拟用户、领域策略和58个相互连接的工具（涵盖导航、生产力、充电和车辆控制）。除了标准任务完成度测试外，还设计了幻觉任务（测试智能体在工具或信息缺失时的极限意识）和消歧任务（要求通过澄清或内部信息收集来解决不确定性）。

Result: 基线结果显示，所有任务类型上都存在偶尔成功与持续成功之间的巨大差距。即使是前沿推理LLM在消歧任务上的持续通过率也不到50%，经常因过早行动而失败；在幻觉任务中，模型经常违反策略或编造信息以满足用户请求，突显了真实场景中需要更可靠、更自知的LLM智能体。

Conclusion: CAR-bench揭示了现有LLM智能体在真实世界用户场景中的可靠性缺陷，特别是在处理不确定性和能力意识方面存在显著不足，强调了开发更可靠、更自知的智能体对于实际应用的重要性。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [62] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 提出一种基于多维核表示的后验方法，从已有数据中定义AI系统的运行设计域(ODD)，用于安全关键AI系统的认证。


<details>
  <summary>Details</summary>
Motivation: 在现实世界复杂系统或已有数据场景中，定义AI系统的运行环境条件极其困难，导致ODD描述不完整。然而，ODD是许多领域认证AI系统所必需的，传统方法依赖早期开发阶段专家知识，存在局限性。

Method: 提出安全设计方法，使用多维核表示从先前收集的数据中后验定义ODD。通过蒙特卡洛方法和真实航空用例（未来安全关键防撞系统）进行验证。

Result: 证明了数据驱动的ODD可以与原始隐藏的ODD相等，为数据驱动的安全关键AI系统认证提供了新途径。

Conclusion: 基于安全设计的核方法ODD能够支持未来数据驱动的安全关键AI系统认证，解决了传统ODD定义方法的局限性。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [63] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: WoW是一个基于ServiceNow的企业工作流环境，包含4000+业务规则和55个活跃工作流，并提出了WoW-bench基准测试，揭示前沿LLM在企业系统中存在动态盲区问题，需要显式学习系统动态以实现可靠的企业智能体。


<details>
  <summary>Details</summary>
Motivation: 现有企业基准测试只评估表面任务完成度，忽略了企业系统中的真实挑战，如有限可观测性、大规模数据库状态以及具有级联副作用的工作流。需要创建更真实的企业环境来评估LLM智能体在实际复杂企业系统中的表现。

Method: 引入World of Workflows (WoW)，一个基于ServiceNow的真实企业环境，包含4000多个业务规则和55个活跃工作流。同时提出WoW-bench基准测试，包含234个任务，评估受限智能体任务完成度和企业动态建模能力。

Result: 研究发现：(1) 前沿LLM存在动态盲区，无法预测其行为的不可见级联副作用，导致静默约束违规；(2) 在不透明系统中实现可靠性需要基于世界建模，智能体必须在缺乏高保真反馈时通过心理模拟隐藏状态转换来弥补可观测性差距。

Conclusion: 为实现可靠有用的企业智能体，WoW推动了一个新范式：显式学习系统动态。研究团队发布了GitHub用于设置和评估WoW环境。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [64] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出Agent-RRM，一个多方面的奖励模型，为智能体轨迹提供结构化反馈，包括推理轨迹、针对性批评和总体评分，并通过三种集成策略显著提升智能体强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习方法主要依赖稀疏的结果奖励进行训练，这种反馈无法区分中间推理质量，导致训练结果不理想。需要更细粒度的反馈机制来提升智能体推理能力。

Method: 提出Agent Reasoning Reward Model (Agent-RRM)，包含三个结构化反馈组件：显式推理轨迹、针对性批评（突出推理缺陷）和总体评分。研究了三种集成策略：Reagent-C（文本增强细化）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试中，Reagent-U取得了显著的性能提升，在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化反馈解决了传统稀疏奖励的局限性，显著提升了智能体强化学习的性能，为未来研究提供了有价值的工具和数据集。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>
