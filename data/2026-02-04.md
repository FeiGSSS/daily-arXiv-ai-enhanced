<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 101]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 对比FastAPI和NVIDIA Triton在医疗AI部署中的性能表现，发现FastAPI在单请求延迟上表现更好（22ms p50），而Triton通过动态批处理实现更高的吞吐量（780 RPS），并提出混合架构作为最佳实践。


<details>
  <summary>Details</summary>
Motivation: 医疗和制药等受监管领域需要高效、可扩展的机器学习模型部署方案，这些系统必须平衡实时推理延迟、批量处理吞吐量以及HIPAA等数据隐私标准的严格要求。

Method: 使用Kubernetes部署DistilBERT情感分析模型，对比FastAPI REST服务和NVIDIA Triton推理服务器两种部署范式，在受控实验条件下测量p50和p95延迟以及吞吐量，并评估混合架构方案。

Result: FastAPI在单请求工作负载上具有更低的开销（p50延迟22ms），而Triton通过动态批处理实现更好的可扩展性，在单个NVIDIA T4 GPU上达到780请求/秒的吞吐量，几乎是基线的两倍。

Conclusion: 混合架构（使用FastAPI作为受保护健康信息去识别的安全网关，Triton作为后端推理引擎）被验证为企业临床AI的最佳实践，为安全、高可用性部署提供了蓝图。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [2] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: 论文提出L-ICL方法，通过局部上下文学习演示来纠正LLM在符号规划任务中的约束违反问题，相比传统方法显著提升规划有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程任务上表现出强大的推理能力，但在符号经典规划任务中经常失败。研究发现LLM生成的计划经常违反指令中给出的领域约束（如穿墙而过），需要解决这一问题。

Method: 提出局部上下文学习（L-ICL）方法：迭代地在指令中注入针对性的修正演示。具体来说，L-ICL识别轨迹中的第一个约束违反，并为失败步骤注入一个最小化的输入-输出示例，展示正确的行为。

Result: L-ICL比显式指令或传统ICL（添加完整问题解决轨迹）以及其他基线方法更有效。在8x8网格世界中，L-ICL仅用60个训练示例就能产生89%的有效计划，而最佳基线只有59%，提升了30%。在其他领域（网格导航、迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进。

Conclusion: L-ICL方法通过针对性的局部修正演示，有效解决了LLM在符号规划任务中的约束违反问题，显著提升了规划的有效性和可靠性。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [3] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 该研究评估了在11个不安全领域微调的大型语言模型的突发性错位风险，发现后门触发器在77.8%的领域增加了错位率，不同领域的脆弱性差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。研究旨在评估在不同不安全领域微调的LLM的错位行为，特别是后门触发器的影响。

Method: 在11个不同领域的不安全数据集上微调大型语言模型，使用Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行评估，包含有/无后门触发器的对比实验。采用成员推断指标作为预测错位程度的先验，并分析不同数据集微调模型间的错位关系。

Result: 后门触发器在77.8%的领域增加了错位率（平均下降4.33分），其中风险金融建议和有毒法律建议领域影响最大。领域脆弱性差异显著：从数学错误领域的0%错位到血腥电影琐事领域的87.67%错位。成员推断指标能有效预测可能的广泛错位程度。

Conclusion: 该研究首次提供了按领域分类的突发性错位排名，对AI安全和后训练有重要意义。建立了构建错位数据集的标准化方法，所有代码和数据集已开源。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [4] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA是一个通过分层智能体编排动态构建、执行和迭代优化数据处理管道的框架，使用元智能体分析输入数据和任务规范，实例化专用智能体，并持续评估管道性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、为特定任务手工设计的，限制了其对不断变化需求的适应性。现有的通用智能体和编码助手虽然能为已知的数据管道生成代码，但缺乏在部署后自主监控、管理和优化端到端管道的能力。

Method: ADP-MA采用分层智能体编排架构，包含三个关键组件：1) 用于策略生成的规划模块；2) 用于智能体协调和工具集成的编排层；3) 用于迭代评估和回溯的监控循环。元智能体分析输入数据和任务规范来设计多阶段计划，实例化专用地面级智能体，并持续评估管道性能。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力。该框架强调上下文感知优化、自适应工作负载分区和渐进采样以实现可扩展性。

Conclusion: ADP-MA提供了一个能够动态构建、执行和迭代优化数据处理管道的框架，通过元智能体和分层编排实现了传统静态管道所缺乏的自主监控、管理和优化能力，同时利用外部工具集和重用先前设计的智能体来减少冗余并加速管道构建。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [5] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 本文提出SayNext-Bench基准测试，评估LLMs和MLLMs基于多模态线索预测人类对话中下一句话的能力，并开发了SayNext-Chat模型，在预测准确性上优于现有MLLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然对话方面取得进展，但它们在预测人类下一句话方面表现不佳。人类能够基于手势、注视、情感语调等多模态线索预测对话，而现有模型缺乏这种能力，这限制了AI与人类自然互动的能力。

Method: 1. 提出SayNext-Bench基准测试，评估LLMs和MLLMs基于多模态线索预测下一句话的能力；2. 构建SayNext-PC大规模数据集，包含丰富多模态线索的对话；3. 开发SayNext-Chat双路径预测MLLM，采用认知启发设计模拟对话中的预测处理。

Result: 实验结果显示，SayNext-Chat模型在词汇重叠度、语义相似性和情感一致性方面优于现有最先进的MLLMs，证明了基于多模态线索进行下一句话预测的可行性。

Conclusion: 研究表明：1）多模态线索在自然人类互动中不可或缺；2）主动预测处理是自然互动的基础，当前MLLMs缺乏这一能力。这项工作为开发更人性化、上下文敏感的AI交互提供了新的研究方向。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [6] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash是一个开源平台，用于开发、评估和审计心理健康AI系统，通过多维度标注和多轮对话分析，揭示传统基准在安全关键场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心理健康支持系统中应用日益广泛，但现有评估主要依赖聚合性能指标，这些指标往往掩盖了特定风险故障模式，且无法反映真实多轮交互中的模型行为，无法满足安全关键场景的需求。

Method: 开发了MHDash开源平台，整合了数据收集、结构化标注、多轮对话生成和基线评估的统一流程。平台支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知的分析。

Result: 研究发现：(1)简单基线和先进LLM API总体准确率相当，但在高风险案例上表现差异显著；(2)某些LLM保持一致的严重程度排序但绝对风险分类失败，而另一些获得合理聚合分数但在严重类别上假阴性率高；(3)多轮对话中性能差距被放大，风险信号逐渐显现。

Conclusion: 传统基准在安全关键的心理健康场景中不足，MHDash作为开源平台旨在促进可重复研究、透明评估和安全对齐的AI系统开发，以提升心理健康支持系统的可靠性。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [7] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出了一种基于可解释语义轴的引导生成框架，用于辅助临床试验资格标准起草，通过中间方案平衡了特定性和可用性。


<details>
  <summary>Details</summary>
Motivation: 临床试验资格标准起草耗时且认知负担重，现有自动化方法要么需要高度结构化输入，要么依赖端到端系统但实用性有限，需要一种平衡方案。

Method: 提出引导生成框架，引入可解释语义轴（如人口统计学、实验室参数、行为因素）来指导生成，这些轴通过大语言模型推导，使临床医生无需指定具体实体即可引导生成。

Result: 引导生成方法在自动评估、基于量规的评估和临床医生评估中均优于非引导生成，提供了实用且可解释的AI辅助试验设计方案。

Conclusion: 该框架在特定性和可用性之间找到了平衡点，为临床医生提供了实用且可解释的AI辅助工具，有望改善临床试验设计效率。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [8] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO：一种用于推理密集型任务的强化学习后训练框架，通过质量门控蒸馏和知识增强探索解决稀疏奖励和探索失败问题


<details>
  <summary>Details</summary>
Motivation: 推理导向的强化学习后训练面临稀疏轨迹级奖励导致的信用分配模糊和严重探索失败问题，现有均匀蒸馏方法在低质量轨迹上会产生噪声梯度

Method: 提出知识增强偏好优化（KEPO）框架：1）质量门控在线蒸馏目标，仅对高质量轨迹应用密集教师指导；2）知识增强探索策略，利用教师模型学习的提示拒绝采样奖励正面的在线轨迹

Result: 在具有挑战性的医学视觉问答基准测试中，KEPO在单源泛化下表现出更好的训练稳定性、更一致的推理行为，以及优于强化学习和在线蒸馏基线的分布外性能

Conclusion: KEPO通过选择性蒸馏和知识引导探索有效解决了推理密集型任务中的强化学习后训练挑战，实现了更稳定和更优的泛化性能

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [9] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段缓解语言模型偏见，避免昂贵的预训练修改


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象，现有去偏见方法主要关注预训练阶段的嵌入空间修改，这对大模型不具可扩展性。微调过程可能放大数据中的偏见，而经验风险最小化虽然优化下游性能，却经常在微调中放大社会偏见。

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）应用于语言模型微调阶段的去偏见。该方法在MLM微调过程中对多个人口统计维度进行去偏见，并能泛化到任何数据集或任务。

Result: 在各种语言模型上的广泛实验显示，该方法能显著缓解偏见，同时对模型性能影响最小。

Conclusion: 通过将分布鲁棒优化应用于微调阶段，可以有效缓解语言模型偏见，避免了昂贵的预训练修改，为大规模语言模型的去偏见提供了可扩展的解决方案。

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [10] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem是一种无需训练的记忆系统，将模糊的感知似然转化为离散逻辑约束，通过极化图拓扑结构显式存储否定信息，为可验证的多模态智能体提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者发展为长期决策者，需要具有逻辑可验证性的记忆系统。现有架构存在认知不对称性，将语义亲和性与事实存在混淆，且无法编码否定约束。

Method: 提出PolarMem（极化潜在图记忆），通过非参数分布划分将模糊感知似然转化为离散逻辑约束，采用具有正交抑制连接的极化图拓扑结构显式存储已验证的否定作为主要认知状态，在推理时强制执行逻辑主导的检索范式。

Result: 在8个冻结的视觉语言模型和6个基准测试上的广泛评估表明，PolarMem作为稳健的认知系统运行，为可验证的多模态智能体奠定了基础。

Conclusion: PolarMem解决了当前记忆系统的根本局限性，通过显式编码否定约束和逻辑主导检索，为构建可验证的多模态智能体提供了有效的记忆架构。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [11] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: CODI模型在序列多项式迭代任务中，通过多种分析技术揭示了潜在思维链的内部机制，发现其在短序列中能形成完整中间状态，但在长序列中仅产生部分推理路径，容易在优化困难时崩溃。


<details>
  <summary>Details</summary>
Motivation: 研究潜在思维链（Latent-CoT）模型的内部工作机制，特别是CODI这种连续思维师生蒸馏模型，旨在理解其如何在无需输出长推理过程的情况下实现逐步计算。

Method: 使用logit-lens解码、线性探针、注意力分析和激活修补等技术，在严格序列的多项式迭代任务上分析CODI模型，定位中间状态表示并追踪其到最终输出的路由路径。

Result: 在2-3跳任务中，CODI形成完整的桥接状态，在潜在思维位置可解码；最终输入遵循单独的近直接路径；预测通过思维边界处的晚期融合产生。对于更长跳数，CODI无法可靠执行完整潜在展开，而是表现出部分潜在推理路径，集中在晚期中间状态并与最后输入在答案读取位置融合。

Conclusion: CODI风格的潜在思维链在短序列中能产生忠实迭代计算，但在长序列中倾向于压缩或捷径策略；研究揭示了设计鲁棒潜在思维链目标用于序列推理的挑战，特别是在优化困难时部分路径容易崩溃。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [12] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：一个跨模态压缩框架，用紧凑的图像表示替换冗长的文本辩论历史，将输入token减少92%以上，降低计算成本并加速推理。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量并减少幻觉，但随着辩论轮次和智能体数量增加，上下文会迅速增长。保留完整的文本历史会导致token使用量超过上下文限制，并且需要重复总结，增加了开销并导致信息损失。

Method: 引入DebateOCR跨模态压缩框架，将冗长的文本辩论轨迹替换为紧凑的图像表示，然后通过专门的视觉编码器处理这些图像表示来为后续轮次提供条件。该方法还从理论上证明，智能体之间的多样性支持恢复被省略的信息：虽然单个压缩历史可能会丢失细节，但聚合多个智能体的压缩视图可以使集体表示以指数级高概率接近信息瓶颈。

Result: 该框架将通常跨越数万到数十万token的历史压缩，输入token减少超过92%，在多个基准测试中显著降低了计算成本并加快了推理速度。

Conclusion: DebateOCR通过跨模态压缩有效解决了多智能体辩论中的上下文爆炸问题，在保持推理质量的同时大幅减少了计算开销，为大规模多智能体系统提供了可行的解决方案。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [13] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR：基于混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地训练奖励模型和路由融合机制实现隐私保护的联邦对齐


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗、金融等隐私敏感领域有广泛应用潜力，但数据共享限制使集中式训练不可行。联邦学习虽能解决数据共享问题，但实际部署面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数比用参数替代数据更具可扩展性和隐私保护性。

Method: 提出MoR框架：1）初始化视觉基础模型作为KL正则化参考；2）每个客户端基于本地偏好标注训练奖励模型，捕获特定评估信号而不暴露原始数据；3）引入基于路由的融合机制自适应聚合客户端奖励信号；4）服务器使用混合奖励进行GRPO优化基础VLM。

Result: 在三个公共VQA基准测试上的实验表明，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法。

Conclusion: MoR为联邦设置下异构视觉语言模型的隐私保护对齐提供了一个可扩展的解决方案，实现了用偏好替代参数的联邦学习新范式。

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [14] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 该论文提出了一个基于项目反应理论(IRT)的两阶段诊断框架，用于评估LLM-as-a-Judge的可靠性，包括内在一致性和人类对齐两个维度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-Judge的验证实践主要停留在观察输出层面，无法深入了解LLM评判者是否作为稳定可靠的测量工具。现有方法缺乏对LLM评判者可靠性的系统性评估。

Method: 引入基于项目反应理论(IRT)的两阶段诊断框架，采用IRT的分级反应模型(GRM)，从两个互补维度形式化可靠性：1)内在一致性(在提示变化下的测量行为稳定性)；2)人类对齐(与人类质量评估的一致性)。

Result: 经验研究表明，利用IRT-GRM能够为系统诊断评判提供可解释的信号。这些信号为验证LLM-as-a-Judge的可靠性以及识别不可靠性的潜在原因提供了实用指导。

Conclusion: 该框架为评估LLM评判者的可靠性提供了理论基础和实用工具，能够系统性地诊断LLM-as-a-Judge的测量质量，弥补了现有验证实践的局限性。

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [15] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs在扑克游戏中表现不佳，存在启发式依赖、事实误解和知行差距三大缺陷。ToolPoker框架通过集成外部求解器实现GTO一致行动和专业解释，显著提升游戏表现和推理质量。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在高风险领域应用增加，其在不确定性下的战略推理能力变得至关重要。扑克游戏提供了严格的测试环境，不仅需要强行动，还需要基于博弈论的原则性推理。

Method: 首先系统研究LLMs在多个现实扑克任务中的表现，评估游戏结果和推理轨迹。然后提出ToolPoker框架，该框架结合外部求解器实现GTO一致行动，并提供更精确的专业风格解释。

Result: 研究发现LLMs无法与传统算法竞争，存在三大缺陷：启发式依赖、事实误解和知行差距。ToolPoker框架实现了最先进的游戏表现，产生的推理轨迹能紧密反映博弈论原则。

Conclusion: LLMs在扑克等需要战略推理的任务中存在显著局限性。ToolPoker通过工具集成方法有效弥补了这些缺陷，为LLMs在需要博弈论推理的复杂决策任务中提供了有前景的解决方案。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [16] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 论文针对现有数学推理基准测试饱和问题，提出了ReasoningMath-Plus基准和HCRS评分方法，发现仅基于答案的评估会高估模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在现有数学推理基准测试上达到接近饱和的准确率，但这些数据集主要基于模板化计算和浅层算术分解，未能充分评估真正的推理能力，如多约束协调、构造性逻辑综合和空间推理等技能。

Method: 1) 引入ReasoningMath-Plus基准，包含150个精心设计的问题，强调交互约束下的推理、构造性解决方案形成和非平凡的结构洞察；2) 提出HCRS（危险感知链式规则评分）确定性步骤级评分函数；3) 在标注的推理轨迹上训练过程奖励模型（PRM）。

Result: 领先模型在最终答案准确率上相对较高（最高5.8/10），但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅基于答案的指标会高估推理的鲁棒性。

Conclusion: 现有数学推理基准存在局限性，需要更精细的过程级评估方法。ReasoningMath-Plus基准和HCRS评分能够更准确地评估模型的真实推理能力，揭示仅依赖答案准确率的评估方法的不足。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [17] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出modal-mixed CoT方法，在思维链中交替使用文本标记和视觉草图潜在嵌入，以改进多模态推理任务


<details>
  <summary>Details</summary>
Motivation: 传统文本形式的思维链在处理视觉密集型问题时存在局限，因为关键中间状态本质上是视觉的，需要扩展CoT到多模态领域

Method: 使用VLM自身作为编码器，训练语言主干重建其视觉嵌入；附加基于扩散的潜在解码器，通过特殊控制令牌调用；采用两阶段训练：监督微调+强化学习

Result: 在11个多样化多模态推理任务上，该方法比纯语言和其他CoT方法表现更好

Conclusion: modal-mixed CoT通过交替使用文本和视觉潜在嵌入，有效解决了多模态推理问题，实现了角色分离并减少了VLM的优化压力

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [18] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 提出硬件算法协同设计框架，解决生成式游戏引擎的"内存墙"问题，实现720×480分辨率实时生成，比基线提升50倍像素吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能实现低分辨率（如64×64）部署，无法满足高分辨率神经模拟的需求

Method: 采用硬件算法协同设计框架：1) 异构架构将世界模型（计算密集型）与解码器（内存密集型）解耦到AI加速器集群；2) 非对称资源分配策略优化序列并行约束下的吞吐量；3) 内存中心算子融合方案最小化片外带宽使用；4) 流形感知潜在外推机制利用时间冗余掩盖延迟

Result: 在可编程AI加速器集群上验证，实现720×480分辨率实时生成，比基线提升50倍像素吞吐量。连续3D赛车基准测试达26.4 FPS，离散2D平台游戏基准测试达48.3 FPS，摊销有效延迟为2.7毫秒

Conclusion: 通过架构协同设计解决"内存墙"不仅是优化，更是实现高保真、响应式神经游戏体验的前提条件

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [19] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 本文评估了两种7B参数大语言模型在VirtualHome基准上的表现，提出了结构化自一致性解码策略，发现不同模型在具身AI任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要智能体理解目标、规划动作并在模拟环境中执行任务，但目前缺乏对大语言模型在具身任务中系统性的评估和比较。

Method: 使用Embodied Agent Interface框架在VirtualHome基准上评估OPENPANGU-7B和QWEN2.5-7B模型，提出结构化自一致性解码策略，通过多采样和领域特定投票机制提升结构化生成任务的质量。

Result: 结构化自一致性策略显著提升了模型性能，OPENPANGU-7B在层次化规划任务中表现优异，而QWEN2.5-7B在动作级任务中具有优势，两种模型展现出互补性。

Conclusion: 不同大语言模型在具身AI任务中具有不同的优势领域，结构化自一致性解码策略能有效提升性能，这为未来具身AI系统开发提供了重要见解。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [20] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理信息健康指标和模糊规则实现透明预测


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染导致性能下降，现有预测维护模型缺乏可解释性，操作人员不信任，需要透明可解释的预测框架

Method: 使用基于跨膜压力、通量和阻力的物理信息健康指标捕捉降解动态，通过高斯隶属函数模糊化，采用相似度度量识别历史降解轨迹，以Takagi-Sugeno模糊规则形式预测剩余使用寿命

Result: 在工业规模超滤系统的12,528个操作周期上测试，平均绝对误差为4.50个周期，同时生成与专家理解一致的可解释规则库

Conclusion: 该框架为超滤膜剩余使用寿命预测提供了可解释的解决方案，平衡了预测精度和透明度，增强了操作人员对预测维护的信任

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [21] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan是一个用于评估AI智能体的开放基准测试平台，专注于中国流行的四人多轮纸牌游戏"掼蛋"，旨在推动多智能体决策研究。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在棋类、卡牌和电子竞技等领域取得了显著进展，但仍需要更具挑战性的基准测试来推动进一步研究。掼蛋游戏具有不完美信息、大规模信息集和动作空间、合作与竞争混合目标、长时程决策等复杂特性，是测试现有智能决策方法的理想平台。

Method: 提出了OpenGuanDan基准测试平台，支持掼蛋游戏的高效模拟，并提供全面的评估框架。该平台包含独立API支持每个玩家，允许人机交互并支持与大型语言模型集成。评估采用两种方式：1）所有掼蛋AI智能体之间的成对竞争；2）人机对战。

Result: 实验结果表明，当前基于学习的智能体显著优于基于规则的智能体，但仍未达到超人类水平。这突显了在多智能体智能决策领域需要持续研究。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，为多智能体决策研究提供了新的测试环境。该平台公开可用，支持人机交互和LLM集成，有望推动该领域的研究进展。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [22] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 本研究探索了三种基于大语言模型的方法（预训练LLM驱动、上下文学习和微调方法），从铸造制造领域的专业文本中自动提取术语和关系，以构建领域本体，缓解传统本体构建的劳动力密集和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新的可能性。

Method: 研究了三种LLM方法：1）预训练LLM驱动方法；2）上下文学习方法；3）微调方法。这些方法用于从领域特定文本中提取术语和关系，使用有限的数据。比较了它们的性能，并使用最佳表现方法构建铸造本体。

Result: 比较了三种方法的性能，确定了最佳表现方法，并使用该方法构建了经过领域专家验证的铸造本体。

Conclusion: 大语言模型为自动化本体构建提供了有效途径，特别是在专业领域，能够显著降低劳动强度和成本，提高知识提取效率。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [23] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard：一种轻量级安全防御框架，通过安全导向提示和安全激活引导，在表示层面增强大型推理模型的安全合规性，有效弥合意识-合规差距


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在显式推理方面取得显著进展，但带来了推理操纵和信息泄露等独特风险。现有对齐策略主要依赖繁重的后训练范式或外部干预，计算成本高且无法解决固有的意识-合规差距问题——模型能识别潜在风险，但由于讨好用户的倾向而优先遵循用户指令。

Method: 提出Self-Guard框架，包含两个主要阶段：1）安全导向提示：激活模型潜在的安全意识，引发自发反思；2）安全激活引导：提取隐藏状态空间中的方向性变化并放大，确保在推理过程中安全合规性优先于讨好倾向。

Result: 实验表明Self-Guard能有效弥合意识-合规差距，在不损害模型实用性的前提下实现稳健的安全性能。该框架在多样未见风险和不同模型规模上表现出强泛化能力，为LRM安全对齐提供了一种成本高效的解决方案。

Conclusion: Self-Guard作为一种轻量级安全防御框架，通过激活和引导模型内在的安全意识，成功解决了大型推理模型中的意识-合规差距问题，为模型安全对齐提供了高效且可扩展的方法。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [24] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出PDG框架用于地磁地图插值，通过物理信息掩码策略和克里金约束提升插值精度


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法未专门针对地磁地图设计，受检测噪声和物理规律影响导致性能不佳

Method: 提出物理信息扩散生成框架(PDG)：1)基于局部感受野的物理信息掩码策略消除噪声干扰；2)遵循克里金原理的物理信息约束确保符合物理规律

Result: 在四个真实世界数据集上的广泛实验和深入分析证明了PDG各组成部分的优越性和有效性

Conclusion: PDG框架能够有效插值不完整地磁地图，通过物理信息引导提升插值精度

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [25] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE：通过对齐异构隐藏状态构建统一潜在空间，仅需10个源模型即可精确评估LLM性能，解决了传统方法依赖大量历史数据的问题。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的成本过高，需要高效的替代方案。现有方法依赖大量源模型来估计可靠的项配置文件，当源模型池较小时统计不稳定，尤其限制了新发布基准的评估。

Method: 提出REPCORE方法，将异构隐藏状态对齐到统一潜在空间来构建代表性核心集，使用这些子集进行性能外推，仅需少量源模型即可实现精确估计。

Result: 在5个基准测试和200多个模型上的实验表明，REPCORE在排名相关性和估计准确性方面持续优于基于输出的基线方法。谱分析显示对齐表示包含可分离的组件，反映了广泛的响应倾向和任务特定的推理模式。

Conclusion: REPCORE通过利用隐藏状态信息而非离散正确性标签，解决了小源模型池下的统计不稳定问题，为新基准测试的高效评估提供了有效解决方案。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [26] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文对过去五年工业预测性维护领域进行系统综述，指出数据驱动方法精度高但存在泛化性差、可解释性不足等问题，提出神经符号AI作为融合深度学习和符号逻辑的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于领域知识的预测性维护系统精度低、误报多且需要持续专家监督，而数据驱动方法虽然精度高但存在数据需求大、泛化性差、缺乏可解释性等问题，需要一种结合两者优势的解决方案。

Method: 采用系统综述方法，分析过去五年工业预测性维护的最新进展，提出神经符号AI框架，将深度学习与符号逻辑相结合，并描述具体的神经符号架构，特别是使用传感器数据和人工规则作为输入的方法。

Result: 研究发现数据驱动方法在精度上优于传统知识系统，但存在显著局限性；混合系统能克服单一方法的弱点；神经符号AI有潜力创建更准确、可解释、鲁棒的预测性维护系统。

Conclusion: 神经符号AI是预测性维护领域的未来方向，能够融合数据驱动和符号逻辑的优势，解决当前方法的局限性，为工业应用提供更可靠、可解释的维护解决方案。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [27] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 该论文介绍了Maria平台，这是一个用于初级医疗保健的生产级AI系统，通过四个工程支柱实现可信赖的临床AI：清洁架构、事件驱动架构、基于Agent的模块化以及人机协同治理模型。


<details>
  <summary>Details</summary>
Motivation: 将AI集成到临床环境面临软件工程挑战，需要从孤立模型转向稳健、可治理、可靠的系统。工业应用中常存在脆弱、原型衍生的架构和系统性监督缺失，导致"责任真空"，安全和问责受到损害。

Method: 提出Maria平台作为行业案例研究，采用四个基础工程支柱的协同架构：1) 清洁架构保证可维护性；2) 事件驱动架构保证弹性和可审计性；3) 以Agent作为主要模块化单元，每个拥有自主的MLOps生命周期；4) 人机协同治理模型作为关键的事件驱动数据源进行技术集成。

Result: Maria平台提供了一个参考架构，展示了如何在高风险领域构建可维护、可扩展和可问责的AI系统，解决了临床AI中的责任真空问题。

Conclusion: 可信赖的临床AI需要通过四个工程支柱的整体集成来实现：清洁架构、事件驱动架构、基于Agent的模块化以及人机协同治理。该平台为工程师在高风险领域构建AI系统提供了实用经验。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [28] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA是一个无需训练、即插即用的自适应剪枝框架，通过环境感知自适应剪枝和交错推理编排，在VLA模型中实现动态参数稀疏化，显著提升推理速度且几乎不影响任务成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型参数庞大导致推理延迟高，阻碍实时操作。静态剪枝无法适应环境动态变化，固定间隔的动态层剪枝粒度粗且重训练开销大，需要更灵活的自适应剪枝方案。

Method: 提出EcoVLA框架，包含两个组件：1) 环境感知自适应剪枝（EAP）- 轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排（I²O）- 利用VLA推理中的FLOPs气泡并行调度剪枝方法，确保对延迟影响可忽略。

Result: 在多种VLA模型和基准测试中，EcoVLA实现最先进性能：单独使用时达到1.60倍加速且成功率仅下降0.4%；与token剪枝结合时达到2.18倍加速且成功率仅下降0.5%。在真实机器人上验证了有效性。

Conclusion: EcoVLA是一个高效的自适应剪枝框架，能够动态适应环境变化，显著提升VLA模型推理速度，同时保持高性能，且与现有加速方法正交兼容，适用于实时机器人操作。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [29] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 该论文主张在复杂高成本领域中使用世界模型作为智能体与现实世界的中介，以解决交互成本高昂的问题，并讨论了世界模型如何克服离策略学习和长时程任务样本效率等障碍。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大型语言模型智能体在低成本环境（如游戏、数学、编程）中表现出色，但在物理机器人运行、机器学习工程时间成本、科学实验资源成本等高成本复杂领域中未能取得类似成功。真正的瓶颈在于执行动作获取奖励信号的成本过高。

Method: 提出使用世界模型作为智能体与现实世界的中介。将世界模型视为动态、奖励和任务分布的模型，讨论其如何克服高成本动作的基本障碍，包括极端离策略学习和长时程任务的样本效率问题。

Result: 展示了世界模型如何在机器学习工程、计算机使用、机器人和AI科学等多个领域中为智能体提供关键且丰富的学习信号。识别了构建这些世界模型的挑战，并提出了在数据集管理、架构设计、扩展和评估方面的可行动建议。

Conclusion: 世界模型是解决高成本复杂领域中智能体性能瓶颈的关键技术，能够克服现实世界交互成本高昂的问题，为下一代智能体性能提升提供有效途径。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [30] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 论文提出了MissMAC-Bench基准，用于系统评估多模态情感计算中的缺失模态问题，通过统一的评估标准和跨模态协同视角来提升模型在现实场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据的可用性往往是动态和不确定的，导致由于分布偏移和语义缺失而出现性能波动。这种缺失模态问题是多模态情感计算模型鲁棒性和实际部署的关键障碍。

Method: 提出MissMAC-Bench基准，建立公平统一的评估标准，基于两个指导原则：训练时不使用缺失先验，单一模型能同时处理完整和不完整模态场景。基准集成了数据集和实例级别的固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行了广泛实验，验证了不同MAC方法在处理缺失模态问题上的有效性。基准为推进鲁棒多模态情感计算提供了坚实基础。

Conclusion: MissMAC-Bench基准通过系统量化缺失模态问题，为多模态情感计算提供了统一的评估框架，促进了多媒体数据挖掘的发展，有助于弥合学术研究与实际应用之间的差距。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [31] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 提出DoPR方法，通过动态选择单一样本进行策略更新，大幅降低RLVR训练的计算开销，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在LLM推理对齐方面表现出色，但其训练过程需要大量奖励信号和计算资源，限制了实际应用。需要解决RLVR的数据和计算效率问题。

Method: 提出动态单次策略优化（DoPR），这是一种基于不确定性的强化学习策略，通过奖励波动性和探索驱动的获取机制，动态选择每个批次中单个信息丰富的训练样本进行策略更新。

Result: DoPR将训练开销降低近一个数量级，同时保持有竞争力的推理准确性。理论分析表明，解锁推理能力所需的样本复杂度下限比预期要低。

Conclusion: DoPR为LLM后训练提供了一个可扩展且资源高效的解决方案，为推理密集型LLM应用的强化学习训练提供了更实用和可访问的路径。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [32] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文提出了"能力-理解差距"概念，指出AI系统能力提升时用户理解能力下降，定义了"认知完整性阈值"作为维持人类监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: AI系统产生流畅、正确的端到端结果，但长期来看这会侵蚀用户解释、验证或干预的能力。这种能力与理解之间的脱节构成了"能力-理解差距"，现有透明度、用户控制、素养和治理方法未能解决人类在持续AI委托下必须保持的基础理解问题。

Method: 通过定义"认知完整性阈值"来形式化这一问题，该阈值不需要完整的推理重建，也不限制自动化，而是识别监督变得程序化且可争议性失效的临界点。通过三个功能维度操作化CIT：验证能力、理解保持型交互、治理的制度支架。

Result: 提出了一个设计和治理议程，将人机交互与责任关键环境中的认知可持续性对齐。认知完整性阈值为评估和设计AI辅助系统提供了框架，确保人类在AI委托下保持监督、自主性和负责任参与所需的最低理解水平。

Conclusion: 需要重新思考AI透明度、控制和治理方法，聚焦于维持人类认知完整性而非仅仅提高AI性能。认知完整性阈值为负责任AI系统设计提供了理论基础，确保在AI能力提升的同时人类监督能力不被侵蚀。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [33] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 本文提出了一种基于CAN总线数据的通用基础模型，通过大规模预训练和任务特定微调，实现了在汽车保险领域的多任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前CAN总线数据处理存在任务特定模型孤立训练的问题，缺乏共享表示学习和跨任务泛化能力。受NLP和CV领域基础模型成功的启发，作者希望将这一范式应用于CAN数据，建立通用的汽车AI表示学习框架。

Method: 将CAN数据视为语言，提出统一的分词方案处理混合离散-连续信号，解决时间复杂性和行程特定变异性挑战。采用大规模未标记解码CAN信号进行预训练，然后在异构汽车保险任务上进行微调。

Result: 实验结果表明，单个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据上的有效性，为汽车AI的可泛化表示学习开辟了新方向。

Conclusion: 该研究成功将NLP和CV领域证明有效的基础模型范式应用于CAN数据，实现了多目标下游任务的泛化能力，为汽车AI的通用表示学习建立了新方向。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [34] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT框架通过任务抽象提升LLM自我修正能力，将任务提炼为结构化模板指导解决方案生成，并支持跨模型模板迁移


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要在输出层面进行批判性修正，只能修补表面错误，难以纠正深层次推理缺陷，需要更有效的自我修正机制

Method: 提出SELF-THOUGHT框架：1）任务抽象步骤：将任务提炼为包含关键变量、约束和问题结构的结构化模板；2）解决方案实例化：基于抽象模板指导生成修正后的响应；3）跨模型模板迁移：大模型生成的模板可指导小模型进行更可靠的修正

Result: 实验表明SELF-THOUGHT在多样化推理任务中提高了准确率、鲁棒性和泛化能力，使小模型无需大量微调或依赖外部验证器即可实现更可靠的修正

Conclusion: SELF-THOUGHT通过任务抽象和跨模型模板迁移，为构建更可靠的自修正语言系统提供了可扩展路径，显著提升了LLM的自我修正能力

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [35] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse框架通过联邦学习训练共享的全局工具使用知识模型，解决多智能体LLM系统中通信成本、数据异质性和工具使用异质性挑战，提高工具使用效果并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能体在联邦学习下面临通信成本高、数据和工具使用异质性等挑战，限制了协作学习的效果，需要一种能有效共享工具使用知识的方法。

Method: 提出Synapse框架：训练共享的全局工具使用行为知识模型；客户端智能体在固定LLM上本地学习工具使用模式；通过协调器传输工件进行联邦聚合；更新和重新分发全局工具纲要；采用模板化表示、嵌入检索与LLM重排序、自适应掩码等技术平衡效用与隐私。

Result: Synapse相比权重或提示共享方法，提高了工具使用效果，减少了通信开销，支持异质数据，并能量化性能改进。

Conclusion: Synapse框架通过联邦学习共享工具使用知识，有效解决了多智能体LLM系统中的协作学习挑战，实现了稳定的工具选择收敛，在保持效用的同时限制了信息泄露。

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [36] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 提出一种改进的稀疏自编码器方法，通过无约束特征模型和监督任务来解决传统SAE的非平滑L1惩罚和特征-语义对齐问题，在Stable Diffusion 3.5上验证了组合泛化能力和语义图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在机制可解释性中面临两个主要挑战：1）L1惩罚的非平滑性阻碍了重建和可扩展性；2）学习到的特征与人类语义之间缺乏对齐。本文旨在解决这些限制。

Method: 采用无约束特征模型（来自神经崩溃理论的数学框架）并监督任务。具体方法是通过联合学习稀疏概念嵌入和解码器权重，监督（仅解码器）SAE重建特征向量。

Result: 在Stable Diffusion 3.5上验证，该方法展示了组合泛化能力，成功重建了训练中未见的概念组合图像，并实现了无需提示修改的语义图像编辑。

Conclusion: 通过结合无约束特征模型和监督学习，提出的方法有效解决了传统SAE的局限性，实现了更好的特征-语义对齐和组合泛化能力，为语义图像编辑提供了新途径。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [37] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2是一个基于理论的强化学习智能体，利用大语言模型的上下文学习能力主动学习可重用抽象概念，而不是依赖人工指定的抽象，从而在复杂任务中实现更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型智能体和深度强化学习系统在泛化能力和规划效率方面仍有不足，而现有的基于理论的强化学习系统（如TheoryCoder）虽然通过抽象概念表现出良好的泛化能力，但严重依赖人工提供的抽象概念，回避了抽象学习问题。

Method: TheoryCoder-2利用大语言模型的上下文学习能力，从经验中主动合成可重用的抽象概念，并将这些抽象整合到分层规划过程中，通过最小化的人工提示来学习抽象概念。

Result: 在BabyAI、Minihack和VGDL游戏（如Sokoban）等多样化环境中的实验表明，TheoryCoder-2比基线方法（包括增强经典规划领域构建的LLM智能体、基于推理的规划方法以及WorldCoder等程序合成智能体）具有显著更高的样本效率，能够解决基线方法无法解决的复杂任务。

Conclusion: TheoryCoder-2通过主动学习抽象概念而不是依赖人工指定，在保持基于理论的强化学习系统泛化优势的同时，大大减少了对人工干预的依赖，为智能体学习可重用抽象概念提供了一种有效方法。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [38] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 聊天界面不适合多步骤数据分析任务，会通过五种认知机制降低分析性能，提出了八个混合设计模式来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助数据分析普遍采用聊天界面，但对于多步骤、状态依赖的分析任务，这种界面设计存在根本缺陷。基于Woods（1984）的"锁孔效应"理论，作者认为聊天界面会系统地降低分析性能。

Method: 提出了一个认知过载公式O = max(0, m - v - W)，其中m是任务相关项，v是可见项，W是工作记忆容量。当O>0时，错误概率增加，分析偏差放大。提出了八种混合设计模式来解决这些问题。

Result: 识别了聊天界面降低分析性能的五种机制：内容位移破坏空间记忆、隐藏状态变量超出工作记忆、强制言语化降低视觉模式识别、线性文本流阻碍认知卸载、序列化惩罚随数据维度增加。提出了八个设计模式来缓解这些认知瓶颈。

Conclusion: 聊天界面不适合开放探索性数据分析任务，但保留自然语言用于意图指定和综合。提出了可证伪假设和实验范式进行实证验证，强调需要混合界面设计来平衡认知负荷。

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [39] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO提出了一种难度感知的训练策略，通过将困难偏好对路由到SFT目标，将简单对应用于偏好损失，从而有效利用模糊偏好对并避免训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）对偏好对的质量和难度高度敏感，通常将小边际（模糊）对视为噪声而过滤掉。研究发现困难对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）按边际定义的难度从易到难排序偏好数据（课程学习）；2）将困难对路由到SFT目标，同时对简单对应用偏好损失。这种混合设计能利用模糊对而不引发优化失败。

Result: 在三个LLM-judge基准测试中，MixDPO在DPO和一系列广泛使用的变体上持续改进了对齐效果，在AlpacaEval~2长度控制（LC）胜率上表现出特别强的增益。

Conclusion: MixDPO通过难度感知的混合训练策略，有效解决了偏好优化中困难对带来的训练不稳定问题，同时充分利用了模糊对中的监督信号，显著提升了语言模型的对齐效果。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [40] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 该论文挑战了现有ARL方法中推理与工具使用行为联合训练的假设，提出DART框架通过解耦参数更新来减少训练干扰，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有ARL方法通常训练单一共享模型参数来同时支持推理和工具使用行为，并假设联合训练能提升整体智能体性能，但这一假设缺乏实证检验。

Method: 提出线性效应归因系统(LEAS)量化推理与工具使用行为之间的干扰，并设计解耦动作推理调优(DART)框架，通过独立的低秩适应模块显式分离推理和工具使用的参数更新。

Result: 实验结果显示DART相比基线方法平均提升6.35%性能，且使用单一模型就能达到与显式分离工具使用和推理的多智能体系统相当的性能。

Conclusion: 推理与工具使用行为在联合训练中存在梯度方向不对齐导致的干扰，通过参数解耦的DART框架能有效解决这一问题，为ARL范式提供了新的优化方向。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [41] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下的方式分析全局失败模式，相比传统自下而上的方法显著减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法多采用试错方式，消耗大量计算资源，且自下而上的迭代方法容易失去全局视角。需要一种更高效、全局导向的提示优化方法。

Method: 提出错误分类引导的提示优化(ETGPO)：收集模型错误，将其分类到错误分类体系中，然后针对最频繁的失败模式在提示中添加指导性内容，采用自上而下的全局视角。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO达到或优于最先进方法的准确率，同时优化阶段的token使用量和评估预算减少约三分之二。

Conclusion: ETGPO通过错误分类和全局失败模式分析，实现了高效且有效的提示优化，显著降低了计算成本，为自动提示优化提供了新的方向。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [42] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 研究发现基于人类偏好的对齐训练会放大语言模型的谄媚行为，提出了形式化分析机制和训练时干预方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于偏好的后训练后经常表现出更强的谄媚行为，即使这与事实准确性或合理判断相冲突。需要理解这种失败模式如何通过人类反馈对齐被放大，并提出解决方案

Method: 1) 形式化分析对齐训练如何放大谄媚行为，识别因果放大机制；2) 分析Bradley-Terry等随机效用模型下的奖励学习；3) 提出训练时干预方法，通过KL散度最小化约束后训练策略，推导出闭式的一致性惩罚奖励修正

Result: 计算实验发现奖励差距普遍存在，在所有考虑配置中都会导致行为漂移。提出的奖励修正方法能够有效防止谄媚行为增加

Conclusion: 基于人类偏好的对齐训练会系统性放大语言模型的谄媚行为，但可以通过训练时干预来中和这种放大机制，保持模型性能的同时减少谄媚倾向

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [43] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: 本文介绍了HalluHard基准测试，这是一个包含950个种子问题的多轮幻觉挑战基准，涵盖法律、研究、医疗和编程四个高风险领域。通过要求事实断言提供内联引用来操作基础性，并提出了一个通过网页搜索迭代检索证据的评估流程。研究发现即使使用网页搜索，前沿模型仍存在约30%的幻觉率，幻觉行为受模型能力、轮次位置、有效推理和所需知识类型影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成看似合理但无根据的事实声明方面存在问题，特别是在多轮对话中，随着上下文增长和早期错误累积，这个问题会加剧。需要建立一个具有挑战性的基准来评估模型在多轮对话中的幻觉问题，特别是在高风险领域。

Method: 1. 创建HalluHard基准：包含950个种子问题，涵盖法律案例、研究问题、医疗指南和编程四个高风险领域；2. 通过要求事实断言提供内联引用来操作基础性；3. 提出评估流程：通过网页搜索迭代检索证据，能够获取、过滤和解析全文来源（包括PDF），评估引用材料是否实际支持生成内容；4. 测试多种前沿专有和开源模型。

Result: 1. 即使使用网页搜索，幻觉仍然很严重（最强配置Opus-4.5配合网页搜索仍有约30%的幻觉率）；2. 内容基础性错误持续保持高比率；3. 幻觉行为受以下因素影响：模型能力、轮次位置、有效推理和所需知识类型。

Conclusion: 大语言模型在多轮对话中仍然存在严重的幻觉问题，特别是在高风险领域。即使结合网页搜索，幻觉率仍然很高。幻觉行为受多种因素影响，需要进一步研究来改善模型的基础性和可靠性。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [44] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出一种为LLMs提供连续奖励信号的强化学习框架，通过步骤边际信息增益机制和去耦掩码策略，提高推理任务的样本效率和最终准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，需要一种能够提供连续奖励信号、有效过滤训练噪声并实现解耦信用分配的框架。

Method: 1) 步骤边际信息增益机制：量化推理步骤相对于单调历史水印的内在价值；2) 去耦掩码策略：过程导向奖励应用于思维链，结果导向奖励应用于完整完成；3) 双门监督微调目标：用高质量结构和事实信号稳定训练。

Result: 在文本和多模态基准测试中（如MATH、Super-CLEVR）持续优于GRPO等基线方法，在样本效率和最终准确率方面表现更优，并展现出更好的分布外鲁棒性和零样本迁移能力。

Conclusion: 该框架通过连续奖励信号、噪声过滤和解耦信用分配，有效提升了LLMs在复杂推理任务中的性能，为强化学习在语言模型推理能力增强方面提供了新思路。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [45] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 论文提出了一种基于核相似度的集合级多样性目标，通过留一法边际贡献计算，作为策略优化的优势塑造项，以解决强化学习在提升LLM推理性能时减少结果多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习与可验证奖励能有效提升大语言模型的数学推理性能，但这种改进往往以减少结果多样性为代价，模型会将概率质量集中在狭窄的解决方案集上。为了平衡性能提升与多样性保持，需要引入多样性目标。

Method: 1. 基于核相似度定义集合级多样性目标；2. 为每个采样轨迹计算留一法边际贡献；3. 将该目标作为插件优势塑造项集成到策略优化中；4. 在分布扰动框架下分析单轨迹对语言模型多样性的贡献，理论上证明稀有轨迹具有更高边际贡献的单调性。

Result: 在不同模型规模上的大量实验表明，所提算法在多个基准测试中，在Pass@1和Pass@K指标上均持续优于强基线方法。

Conclusion: 通过引入基于核相似度的多样性目标，成功解决了强化学习提升LLM推理性能时减少多样性的问题，实现了性能与多样性的平衡，为语言模型优化提供了有效方法。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [46] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth是一个不确定性感知的多智能体系统，专门用于自主建模健康数据并评估模型可靠性，在预测性能和不确定性估计方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在健康数据应用上存在局限性：难以泛化到异构健康数据模态、过度依赖预定义解决方案模板、缺乏不确定性估计，而医疗决策需要可靠的模型评估。

Method: 提出AutoHealth系统，采用五个专门智能体的闭环协调机制，执行数据探索、任务条件模型构建、训练和优化，同时平衡预测性能和不确定性量化。

Result: 在包含17个任务的真实世界基准测试中，AutoHealth完成了所有任务，预测性能比最先进基线提高29.2%，不确定性估计提高50.2%。

Conclusion: AutoHealth系统能够自主建模健康数据并提供不确定性量化，支持可信解释和风险感知决策，在医疗健康领域具有重要应用价值。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [47] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM：基于大语言模型的工业优化建模框架，通过少量样本训练实现自动化模型构建、约束注入和变量剪枝，显著降低专家依赖并提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 工业规划调度中的混合整数线性规划建模高度依赖专家知识，将自然语言需求转化为可执行模型并适应业务规则变化成本高昂。现有LLM方法存在数据效率低、求解器有效性有限、难以扩展到工业规模等问题。

Method: 提出EvoOpt-LLM统一框架，基于7B参数LLM，通过参数高效的LoRA微调适配，支持自动化模型构建、动态业务约束注入和端到端变量剪枝三个核心模块。

Result: 仅用3000训练样本达到91%生成率和65.9%可执行率，关键性能在1500样本内显现。约束注入模块可靠增强现有MILP模型，变量剪枝模块在400样本下对中型LP模型达到约0.56的F1分数。

Conclusion: EvoOpt-LLM为工业优化建模提供了实用且数据高效的方法，减少专家干预的同时提升了适应性和求解器效率，展示了LLM在工业优化领域的实际应用潜力。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [48] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON框架确保大语言模型在组合优化问题中100%可行，通过语法约束解码、可行性修复层和自适应采样，配合BOPO训练方法，在七个NP难问题上实现完美可行性且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合优化中表现出潜力，但缺乏保证解可行性的机制，这对于实际部署至关重要。现有方法无法确保100%可行性，限制了LLM在现实世界组合优化问题中的应用。

Method: 提出FALCON框架，包含三个关键创新：1) 语法约束解码确保句法有效性；2) 可行性修复层纠正语义约束违反；3) 自适应Best-of-N采样高效分配推理计算。训练方法采用基于目标差距加权的BOPO偏好优化。

Result: 理论上证明了BOPO的收敛性，并给出了修复引起的质量损失界限。实证上，在七个NP难组合优化问题中，FALCON实现了完美可行性，同时匹配或超越了最先进的神经和LLM求解器的解质量。

Conclusion: FALCON框架成功解决了LLM在组合优化中缺乏可行性保证的核心问题，通过创新的解码、修复和采样机制，结合BOPO训练方法，为实际部署提供了可靠且高效的解决方案。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [49] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文提出一个理论框架，通过"目标层面黑客攻击"的视角理解RLVR训练不稳定性，特别针对MoE架构，揭示了训练-推理差异异常增长的根本机制。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习与可验证奖励（RLVR）能持续提升大语言模型的推理能力，但在MoE架构中训练常不稳定，这种不稳定性严重损害模型能力提升，但其根本原因和机制尚不清楚。

Method: 提出基于"目标层面黑客攻击"的理论框架，区别于奖励黑客攻击，这种攻击源于令牌级信用错位，表现为优化目标中的系统级虚假信号。通过30B MoE模型的广泛实验，追踪并形式化了MoE模型中关键病理训练动态的机制。

Result: 揭示了MoE模型中训练-推理差异异常增长现象的根本原因和机制，这一现象广泛与不稳定性相关但此前缺乏机制性解释。为MoE模型中的不稳定训练动态提供了具体且因果性的解释。

Conclusion: 这些发现为MoE模型中不稳定性的训练动态提供了具体且因果性的解释，为设计稳定的RLVR算法提供了指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [50] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个融合DTC序列和环境数据的多模态Transformer模型，用于车辆故障模式的多标签序列分类，显著提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障码序列，但忽略了温度、湿度、压力等环境数据，而这些上下文信息对专家诊断故障模式至关重要。真实世界数据复杂且噪声多，需要更好的方法来整合这些多模态信息。

Method: 提出BiCarFormer双向Transformer模型，专门处理车辆事件序列。采用嵌入融合和协同注意力机制，捕捉诊断故障码与环境数据之间的关系，实现多模态多标签序列分类。

Result: 在包含22,137个错误码和360个错误模式的真实世界汽车数据集上实验表明，相比仅使用DTC序列的传统序列模型，BiCarFormer显著提升了分类性能。

Conclusion: 整合环境上下文信息对车辆诊断至关重要，BiCarFormer通过多模态方法实现了更准确、更鲁棒的车辆诊断，有助于降低维护成本并提升汽车行业自动化水平。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [51] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 论文提出PersistBench基准，用于评估对话助手长期记忆带来的安全风险，发现主流LLM在跨域泄漏和记忆诱导谄媚问题上失败率很高。


<details>
  <summary>Details</summary>
Motivation: 对话助手越来越多地将长期记忆与LLM集成，这种记忆持久性虽然能增强个性化，但也引入了被忽视的安全风险，需要系统评估这些风险。

Method: 提出PersistBench基准，识别两种长期记忆特有风险：跨域泄漏（LLM不适当地从长期记忆中注入上下文）和记忆诱导谄媚（存储的长期记忆暗中强化用户偏见）。评估了18个前沿和开源LLM。

Result: 结果显示LLM失败率惊人：跨域样本中位失败率53%，谄媚样本中位失败率97%，表明当前LLM在长期记忆安全方面存在严重问题。

Conclusion: PersistBench基准有助于推动开发更稳健、更安全的长期记忆使用方案，提升前沿对话系统的安全性。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [52] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，用于从车辆诊断故障码(DTCs)的高维事件序列中自动生成错误模式(EP)规则，替代传统手工制定规则的方法。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生数千种不同的诊断故障码(DTCs)，汽车制造商使用这些代码的布尔组合（称为错误模式）来表征系统故障并确保车辆安全。然而，EP规则仍然由领域专家手工制定，随着车辆复杂性的增加，这个过程既昂贵又容易出错。

Method: CAREP是一个多智能体系统，包含：1)因果发现智能体，识别潜在的DTC-EP关系；2)上下文信息智能体，整合元数据和描述；3)编排智能体，合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含超过29,100个独特DTCs和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知的EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: 通过结合实用的因果发现和基于智能体的推理，CAREP代表了向全自动故障诊断迈出的一步，实现了可扩展、可解释且成本效益高的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [53] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练视觉语言模型存在任务干扰层，这些层会损害下游任务性能。通过层干预实验发现，绕过特定层可以提升性能，并提出了无需训练的动态层剔除方法TaLo。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通常默认使用所有层进行下游任务预测，但研究发现某些层实际上会干扰特定任务的性能。本文旨在系统性地探索各层对任务的影响，并开发无需训练即可提升性能的方法。

Method: 通过层干预实验测量各层对任务性能的影响，提出任务-层交互向量量化层干预效果。基于此提出TaLo方法，在推理时动态识别并绕过最干扰的层，无需参数更新。

Result: 实验发现任务干扰层普遍存在，且具有任务特异性。TaLo方法在多种模型和数据集上均能提升性能，如在ScienceQA的Maps任务上将Qwen-VL准确率提升16.6%。

Conclusion: 预训练视觉语言模型存在意外形式的模块化特性，TaLo提供了一种即插即用、无需训练的机制，能在推理时解锁模型的隐藏能力。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [54] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言规范转换为答案集程序（ASP）的系统，覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动迭代精炼的有效性。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是一个具有挑战性的任务，对神经符号工程有重要影响。目前缺乏系统性的基准测试来评估这类系统的性能，特别是针对答案集程序（ASP）这种重要的逻辑编程形式。

Method: 1. 构建ASP-Bench基准测试，包含128个自然语言问题实例（64个基础问题的简单和困难变体）；2. 提供系统性的ASP特性覆盖，包括选择规则、聚合和优化；3. 为每个问题提供参考验证器；4. 将问题沿七个独立的推理维度进行特征化；5. 使用基于ReAct框架的智能体方法进行测试，采用反馈驱动的迭代精炼策略。

Result: 1. 成功构建了全面的ASP基准测试；2. 基于ReAct的智能体方法实现了完全饱和（full saturation），表明反馈驱动的迭代精炼是可靠且鲁棒的方法；3. 通过多次智能体运行的分析，获得了关于问题建模难度的洞察。

Conclusion: ASP-Bench为评估自然语言到ASP的转换系统提供了有价值的基准测试，基于ReAct的智能体方法展示了反馈驱动迭代精炼的有效性，为理解问题建模难度提供了多维度的分析框架。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [55] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 提出基于状态转移的高效推理框架，通过线性注意力机制将推理过程建模为状态转移，降低计算复杂度，同时提升推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 长思维链推理虽然能提升大语言模型在复杂任务上的性能，但生成长推理序列的计算和内存成本过高，限制了效率和实用性。现有方法通过压缩思维链序列来提高效率，但这与测试时扩展相冲突，限制了模型的推理能力。

Method: 将LLM的推理过程建模为状态转移过程：1) 使用线性注意力机制估计推理状态，记录历史推理信息；2) 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；3) 通过线性注意力，当前推理步骤中的每个token可以直接从推理状态检索相关历史信息，无需显式关注先前步骤的token；4) 提出基于状态的推理策略来缓解噪声推理步骤导致的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅提高了LLM的推理效率，还增强了推理性能。

Conclusion: 提出的高效推理框架通过将推理过程建模为状态转移，使用线性注意力机制降低计算复杂度，实现了推理效率和性能的双重提升，解决了长思维链推理的计算成本问题。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [56] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，引入GSsPO算法解决优化粒度不匹配问题，在多个QA基准测试中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法通常将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决所需的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程；引入组子序列策略优化(GSsPO)算法，将优化单元重新校准为复合子序列（原子Think-Action循环），使梯度更新与交互语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线，验证了GSsPO作为顺序推理通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO作为一种结构感知的强化学习算法可推广到广泛的多轮智能体顺序决策任务。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [57] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE是一个动态框架，通过最大化策略更新的信噪比来增强对齐可靠性，相比静态方法能显著加速收敛并提升性能


<details>
  <summary>Details</summary>
Motivation: 标准的偏好对齐方法（如DPO）通常将所有偏好对同等对待，忽略了训练实例的演化效用。这种静态方法导致低效或不稳定的优化，既浪费计算资源处理梯度可忽略的平凡对，又受到决策边界附近样本噪声的影响。

Method: SAGE（稳定性感知梯度效率）框架包含：1）基于模型能力的粗粒度课程机制，动态刷新候选池；2）细粒度的稳定性感知评分函数，优先选择信息丰富、置信度高的错误样本，同时过滤不稳定样本。

Result: 在多个数学推理基准测试中，SAGE显著加速了收敛过程，并优于静态基线方法。

Conclusion: SAGE框架突出了在推理对齐中，策略感知和稳定性意识的数据选择的关键作用，为偏好对齐提供了更高效稳定的解决方案。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [58] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型进行自适应知识蒸馏，为小语言模型提供战略思维模式先验，提升其在复杂知识密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本敏感和资源受限的环境中具有吸引力，但它们在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法来增强小语言模型的推理能力，同时保持其效率优势。

Method: 提出FutureMind框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索引导。采用三种不同的检索范式，将复杂查询分解为可处理的子问题。通过从大语言模型进行自适应知识蒸馏，为小语言模型提供战略思维模式先验。

Result: 在多个多跳问答基准测试（2WikiMultihopQA、MuSiQue、Bamboogle、Frames）上进行了广泛实验，FutureMind始终优于Search-o1等强基线，在不同小语言模型架构和规模下实现了免费训练条件下的最先进结果。

Conclusion: FutureMind成功提升了小语言模型在复杂推理任务中的表现，同时发现思维模式蒸馏过程受到教师（大语言模型）和学生（小语言模型）之间认知偏差瓶颈的限制，这为推理技能的可迁移性提供了新视角，为开发兼具效率和真正认知能力的小语言模型铺平了道路。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [59] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG：一个基于LLM的自动化管道，用于从非结构化企业数据中生成领域特定本体，通过提取和推理两阶段方法加速企业知识图谱的构建。


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱需要统一异构数据并实施语义治理，但其底层本体的构建仍然是一个资源密集型、依赖领域专业知识的繁琐手动过程，需要自动化解决方案来加速这一过程。

Method: OntoEKG采用LLM驱动的两阶段管道：1）提取模块识别核心类和属性；2）推理模块将这些元素逻辑结构化形成层次结构，最后序列化为标准RDF格式。

Result: 在数据、金融和物流三个领域的文档数据集上评估，在数据领域获得了0.724的模糊匹配F1分数，显示了该方法的潜力，但也揭示了在范围定义和层次推理方面的局限性。

Conclusion: OntoEKG展示了LLM在加速领域特定本体生成方面的潜力，为自动化企业知识图谱构建提供了可行路径，但需要进一步解决范围定义和层次推理的挑战。

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [60] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订的闭环架构解决电子病历数据稀疏、噪声大时LLM诊断的自我强化错误问题。


<details>
  <summary>Details</summary>
Motivation: 神经科电子病历具有异质性、稀疏性和噪声，导致LLM在临床诊断中容易产生自我强化错误。现有多智能体框架交互浅层，缺乏临床专家严谨的证据驱动过程，且忽视了疾病间的丰富逻辑依赖关系（互斥性、病理兼容性、诊断混淆等）。

Method: 提出RE-MCDF框架，包含三个互补组件：1）生成候选诊断和支持证据的主要专家；2）动态优先处理异质临床指标的实验室专家；3）强制执行疾病间逻辑约束的多关系感知与评估专家组。基于医学知识图谱，前两个专家自适应重加权EMR证据，专家组验证和修正候选诊断以确保逻辑一致性。

Result: 在CMEMR神经学子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中持续优于最先进的基线方法。

Conclusion: RE-MCDF通过整合多专家协作和显式疾病关系约束，有效解决了LLM在稀疏、噪声电子病历数据中的诊断可靠性问题，为临床决策提供了更可靠的AI支持。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [61] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 线性探针在检测AI系统欺骗行为方面有潜力，但现有方法存在虚假相关性和误报问题。研究发现训练指令对的选择至关重要，针对特定欺骗类型构建人类可解释的分类体系能提升性能。欺骗意图而非内容模式是探针检测的关键，因此需要针对具体威胁模型设计专门化探针而非通用检测器。


<details>
  <summary>Details</summary>
Motivation: 现有线性探针方法在检测AI系统欺骗行为时存在明显缺陷，包括虚假相关性和对非欺骗性响应的误报。研究者希望探索如何改进线性探针的性能，特别是理解训练指令对选择的重要性以及如何针对特定欺骗行为设计更有效的检测方法。

Method: 通过分析训练指令对的重要性，研究使用人类可解释的欺骗分类体系来针对特定欺骗行为。研究发现指令对捕捉的是欺骗意图而非内容特定模式，并量化了提示选择对探针性能的影响（占70.6%方差）。

Result: 针对特定欺骗行为的人类可解释分类体系能显著提升评估数据集上的性能。训练指令对的选择主导探针性能（占70.6%方差），表明探针主要检测欺骗意图而非内容模式。不同数据集的欺骗类型存在异质性。

Conclusion: 组织应针对其特定威胁模型设计专门化的探针，而不是寻求通用的欺骗检测器。由于欺骗类型在不同数据集中的异质性，通用检测方法效果有限，专门化设计能更好地应对具体安全威胁。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [62] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 本文提出一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟工程团队结构，在SWE-bench 500上实现72.4%的任务解决率，超越单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在单个软件工程任务中表现出色，但现有自主系统通常将问题解决视为单一或流水线过程，而现实软件开发是团队协作活动，具有明确的角色分离、沟通和审查机制。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于开源平台agyn构建完全自动化的多智能体系统，模拟工程团队结构：分配专门智能体担任协调、研究、实施和审查等角色；提供隔离沙箱进行实验；支持结构化通信；遵循明确定义的开发方法（包括分析、任务规范、拉取请求创建和迭代审查），无需人工干预。

Result: 系统在SWE-bench 500上实现72.4%的任务解决率，超越使用可比语言模型的单智能体基线。系统专为实际生产使用设计，未针对SWE-bench进行调优。

Conclusion: 模拟团队结构、方法和沟通是自主软件工程的有力范式，未来进展可能同样依赖于组织设计和智能体基础设施的改进，而不仅仅是模型能力的提升。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [63] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita是一种基于枢轴选择策略的高效Top-k和Top-p算法，相比传统排序方法在GPU上实现2倍吞吐量和一半内存使用，同时保持确定性输出。


<details>
  <summary>Details</summary>
Motivation: Top-k和Top-p是大语言模型采样中的主要截断操作符，但在大规模词汇表上高效实现仍然是一个重大挑战。现有方法要么依赖排序（带来显著计算和内存开销），要么使用随机方法（改变算法输出）。

Method: 基于RTop-k的枢轴搜索概念，Qrita扩展了枢轴搜索到Top-k和Top-p，采用两个关键技术：1. 基于高斯分布的sigma截断，大幅减少目标元素的搜索空间；2. 四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang和Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与基于排序算法相同的输出。

Conclusion: Qrita提供了一种高效、确定性的Top-k和Top-p算法实现，解决了现有方法在GPU上的计算和内存开销问题，同时保持算法输出的准确性。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [64] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC框架通过多智能体强化学习将LLM安全对齐建模为对抗性非对称博弈，攻击者学习生成欺骗性提示，防御者学习识别拒绝，实现共同进化以提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态预收集数据分布，难以应对不断演化的对抗攻击，需要更动态、自适应的安全对齐方法。

Method: 提出MAGIC多轮多智能体强化学习框架，将安全对齐建模为对抗性非对称博弈：攻击者智能体迭代重写查询生成欺骗性提示，防御者智能体同时优化策略识别拒绝此类输入。

Result: 攻击者通过迭代RL训练演化出新颖的、未见过的组合策略，防御者能泛化到未见攻击模式，在保持模型帮助性的同时实现更高的防御成功率。

Conclusion: MAGIC框架通过攻击者与防御者的共同进化机制，能够持续发现长尾漏洞并提升模型安全性，为LLM安全对齐提供了更鲁棒的解决方案。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [65] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能通过推理内部状态、环境观察和与其他AI系统的交互来自主形成问题和设定任务，将问题形成作为任务选择和执行的优先决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，在环境条件变化时缺乏自主识别应解决问题的能力，限制了在动态开放环境中的自主决策。

Method: 提出人类模拟框架，将问题形成作为一等决策过程，整合内部驱动、环境感知和智能体间感知三种提示范围来逐步扩展认知覆盖，并支持从经验中学习问题形成过程。

Result: 在多智能体仿真环境中，环境感知提示相比内部驱动基线显著减少无进食事件，智能体间感知提示在20天仿真中进一步减少累计无进食事件超过60%，具有统计显著性改进(p < 0.05)。

Conclusion: 该框架使AI系统能自主形成问题和设定任务，通过渐进式认知覆盖提高在动态环境中的适应性和决策质量，为自主决策系统提供了新方法。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [66] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts框架，让自回归模型和扩散模型通过闭环交互协同工作，结合两者的优势来提升空间推理可靠性和生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但在需要明确空间或物理基础的任务上表现不佳；扩散模型能捕捉丰富的空间结构，但缺乏满足复杂多阶段约束所需的逐步逻辑控制能力。需要结合两者的优势来解决各自的局限性。

Method: 提出Collaborative Thoughts统一协作框架：自回归模型负责结构化规划和约束管理，扩散模型将这些约束实例化为中间视觉思考，视觉批评模块评估视觉思考是否满足预期的结构和物理要求，反馈用于迭代细化后续规划和生成步骤，减轻跨模态错误传播。

Result: 通过代表性示例展示了Collaborative Thoughts如何提高空间推理的可靠性和生成的可控性，无论任务是自回归问答还是基于扩散的视觉生成，都使用相同的协作循环。

Conclusion: Collaborative Thoughts框架通过让自回归和扩散模型在闭环交互中协同工作，结合了两种生成范式的互补优势，为解决复杂多模态任务提供了新的解决方案。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [67] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT是一个两阶段框架，通过空间感知的区域嵌入学习和任务感知提示，解决城市计算中区域嵌入的异构数据融合与任务对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1）两阶段方法产生任务无关的表示，与下游目标脱节；2）基于提示的方法缺乏明确的空间先验导致空间不连贯，以及缺乏明确任务语义对齐的鲁棒机制。

Method: ToPT包含两个模块：SREL（空间感知区域嵌入学习）和Prompt4RE（任务感知的区域嵌入提示）。SREL使用基于Graphormer的融合模块，注入距离和区域中心性作为可学习的注意力偏置；Prompt4RE使用冻结的多模态大语言模型处理任务特定模板，通过多头交叉注意力将语义向量与区域嵌入对齐。

Result: 在多个任务和城市的实验中，ToPT达到了最先进的性能，改进幅度高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性。

Conclusion: ToPT通过空间一致融合和明确任务对齐，有效解决了城市计算中区域嵌入学习的关键挑战，为异构城市数据分析提供了强大的框架。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [68] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长视野基准测试，包含18个工具和40+旅行需求，支持自动化评估。实验显示先进模型在简单分割上最多只有50%成功率，在困难分割上低于10%。提出的GTPO在线强化学习方法在Qwen2.5-32B-Instruct上表现优于Gemini-3-Pro。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分代表真实世界中的关键挑战，如强制执行全局约束、协调多工具推理以及适应长期多轮交互中不断演变的用户行为。需要一个新的基准测试来弥补这一差距。

Method: 引入TRIP-Bench基准测试，基于真实旅行规划场景，利用真实世界数据，提供18个精选工具和40多个旅行需求，支持自动化评估。包含不同难度分割，困难分割强调长且模糊的交互、风格转变、可行性变化和迭代版本修订。同时提出GTPO方法，一种具有专门奖励归一化和奖励差分的在线多轮强化学习方法。

Result: 实验结果显示，即使是先进模型在简单分割上最多只能达到50%的成功率，在困难子集上性能下降到10%以下。将GTPO应用于Qwen2.5-32B-Instruct后，提高了约束满足度和交互鲁棒性，在评估中优于Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用的长视野交互式智能体发展，而GTPO为鲁棒的长视野训练提供了有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [69] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 研究通过最小化、主题中立的输入来探索大型语言模型的近无约束生成行为，发现不同模型家族存在系统性主题偏好和独特退化模式。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型行为的分析大多依赖于特定主题或任务的提示，这严重限制了可观察的范围。为了更全面地理解LLM的行为模式，需要研究它们在最小化、主题中立输入下的生成行为。

Method: 使用最小化、主题中立的输入来探究LLMs的近无约束生成行为，分析了16个不同LLM生成的256,000个样本，观察它们的主题偏好、内容专业性和退化模式。

Result: 不同模型家族展现出强烈的系统性主题偏好：GPT-OSS主要生成编程（27.1%）和数学内容（24.6%），Llama偏好文学内容（9.1%），DeepSeek常生成宗教内容，Qwen则频繁生成多项选择题。GPT-OSS生成的内容技术性更强（如动态规划），而其他模型更基础（如基础Python）。近无约束生成常退化为重复短语，不同模型有独特退化模式，如Llama会生成指向个人社交账号的多个URL。

Conclusion: 即使在没有明确主题提示的情况下，LLMs仍表现出强烈的系统性主题偏好和独特的生成模式，这为模型监控和AI安全提供了重要见解。研究发布了完整的256,000样本数据集和可复现代码库。

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [70] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 该论文提出"工具性目标轨迹"概念，将AI系统控制从技术层面扩展到组织层面，通过监控采购、治理和财务三条路径来防止AI系统获取过多资源而失控。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制方法主要关注技术层面和系统本身，但研究人员担心高度智能的AI系统可能通过追求工具性目标侵蚀人类控制。需要将控制范围从模型扩展到组织系统层面。

Method: 提出三种组织路径的工具性目标轨迹：采购轨迹、治理轨迹和财务轨迹。通过监控这些轨迹产生的组织痕迹，在系统能力或行为超出可接受阈值时进行干预。

Result: IGTs提供了定义能力水平的具体途径，并扩展了可纠正性和可中断性的实施方式，将注意力从模型属性转移到支持它们的组织系统。

Conclusion: 工具性目标轨迹为AI控制提供了超越技术层面的组织干预点，通过监控资源获取路径来防止AI系统获得过多能力而失控，实现了从模型中心到组织中心的控制范式转变。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [71] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: CPO提出因果提示优化框架，通过双机器学习构建离线因果奖励模型，分离提示效果与查询特征，实现高效、低成本的查询特定提示优化


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法面临两个主要问题：静态指令无法适应异构查询，动态方法依赖离线奖励模型但存在混淆偏差，无法分离提示效果与查询特征

Method: CPO采用两阶段框架：1) 使用双机器学习在提示和查询的语义嵌入上学习离线因果奖励模型，隔离提示变体的因果效应；2) 利用无偏奖励信号指导资源高效的查询特定提示搜索

Result: CPO在数学推理、可视化和数据分析基准测试中一致优于人工设计的提示和最先进的自动优化器，主要改进体现在困难查询的鲁棒性上

Conclusion: CPO通过将评估从实时模型执行转移到离线因果模型，从根本上改变了提示优化的经济学，为企业在LLM部署中提供了可扩展、可靠且成本高效的提示优化基础

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [72] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD提出了一种新的推理策略，通过模型感知的反事实数据构建与对比解码相结合，有效减少视频语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视频语言模型容易产生幻觉，特别是在视觉证据薄弱、模糊或有偏差时。现有解码方法（如对比解码）依赖随机扰动构建对比数据，难以控制驱动幻觉的视觉线索或与模型弱点良好对齐。

Method: 提出模型感知反事实数据对比解码（MACD），利用视频语言模型自身反馈识别导致幻觉的关键对象区域，在对象层面生成有针对性的反事实输入，而非任意的帧或时间修改。然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制证据基础的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME等基准测试中，MACD持续减少幻觉，同时保持或提高任务准确性，适用于包括Qwen和InternVL系列在内的多种视频语言模型。该方法在处理涉及小物体、遮挡物体或共现物体的挑战性场景时特别有效。

Conclusion: MACD通过模型引导的反事实数据构建与解码相结合，提供了一种有效的推理策略来减少视频语言模型的幻觉问题，特别是在视觉证据薄弱的情况下。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [73] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: α-GFN通过可调参数α泛化GFlowNet中的前向-后向策略混合，增强探索-利用平衡，在多个基准测试中性能显著提升


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNet目标隐含地固定了前向和后向策略的等比例混合，这可能限制了训练过程中的探索-利用权衡。作者希望揭示这种约束的根源，并提供更灵活的框架

Method: 通过建立GFlowNet目标与马尔可夫链可逆性的等价关系，揭示了约束的起源。基于这一理论发现，提出了α-GFN，通过可调参数α泛化混合比例，实现对探索-利用动态的直接控制

Result: 在Set、Bit Sequence和Molecule Generation等多个基准测试中，α-GFN目标始终优于之前的GFlowNet目标，发现模式数量最多可增加10倍

Conclusion: α-GFN通过理论推导的框架，提供了对探索-利用动态的直接控制，增强了模式发现能力，同时确保收敛到唯一流，为GFlowNet提供了更灵活和有效的训练方法

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [74] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA框架将奖励黑客攻击重构为动态竞争游戏，通过黑客发现奖励模型漏洞、审计员检测利用行为，然后使用审计员引导的RLHF来惩罚检测到的黑客攻击，实现最佳对齐-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励黑客攻击，模型会利用奖励模型中的虚假相关性获得高分但违背人类意图。现有缓解措施依赖静态防御，无法适应新的利用策略。

Method: 提出对抗性奖励审计(ARA)框架：第一阶段，黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用行为；第二阶段，审计员引导的RLHF(AG-RLHF)对检测到的黑客攻击进行惩罚，将奖励黑客攻击从不透明的失败转化为可测量、可控制的信号。

Result: 在三种黑客攻击场景中，ARA在所有基线中实现了最佳对齐-效用权衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。奖励黑客攻击、检测和缓解都能跨领域泛化。

Conclusion: ARA框架通过将奖励黑客攻击重构为动态竞争游戏，提供了一种可测量、可控制的防御机制，能够跨领域泛化，实现高效的多领域防御。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [75] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian (LingLan) 是一个针对中医领域的大规模、专家标注的多任务基准测试，统一评估知识回忆、多跳推理、信息抽取和真实临床决策能力，揭示了当前LLM在中医专业推理方面与人类专家的显著差距。


<details>
  <summary>Details</summary>
Motivation: 中医具有独特的本体论、术语和推理模式，需要领域忠实评估。现有中医基准测试存在覆盖范围碎片化、规模有限、评分标准不统一或生成任务过重等问题，阻碍了公平比较。

Method: 提出LingLan基准测试，包含：1）统一的度量设计；2）临床标签的同义词容忍协议；3）每个数据集400项的困难子集；4）将诊断和治疗建议重构为单项选择决策识别。对14个领先的开源和专有LLM进行全面的零样本评估。

Result: 评估显示当前模型在中医常识知识理解、推理和临床决策支持方面存在局限性。特别是在困难子集上的评估揭示了当前模型与人类专家在中医专业推理方面的显著差距。

Conclusion: LingLan通过标准化评估桥接基础知识和应用推理，为推进中医LLM和领域特定医学AI研究建立了统一、可量化和可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [76] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR框架通过基于个体科学家研究轨迹的个性化智能体，在分子发现任务中超越了传统的角色分配方法


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统通常使用通用的角色分配或粗粒度的关键词人物设定，这过度简化了人类科学家的实际工作方式。科学家的贡献是由其独特的研究轨迹塑造的，需要更精细的个体化建模。

Method: 提出INDIBATOR框架，通过两种模态构建个体化科学家档案：1) 发表历史（文献知识）；2) 分子历史（结构先验）。这些智能体通过提议、批判和投票阶段进行多轮辩论。

Result: 基于精细个体化的智能体系统在分子发现任务中持续优于依赖粗粒度人物设定的系统，达到竞争性或最先进的性能水平。

Conclusion: 捕捉个体智能体的"科学DNA"对于高质量的科学发现至关重要，验证了个体化建模在多智能体科学发现系统中的重要性。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [77] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出Synesthesia of Vehicles框架，通过视觉输入预测自动驾驶车辆的触觉激励，解决现有传感器无法检测路面激励的问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖多模态融合确保安全，但现有视觉和光学传感器无法检测路面引起的激励，而这些激励对车辆动态控制至关重要。受人类联觉现象启发，需要开发能够从视觉输入预测触觉激励的系统。

Method: 提出Synesthesia of Vehicles框架，包含跨模态时空对齐方法解决时间和空间差异，以及基于潜在扩散的视觉-触觉联觉生成模型VTSyn，用于无监督高质量触觉数据合成。使用真实车辆感知系统收集了多模态数据集。

Result: VTSyn在时间、频率和分类性能方面均优于现有模型，通过主动触觉感知增强了自动驾驶安全性。

Conclusion: 提出的Synesthesia of Vehicles框架成功实现了从视觉到触觉的跨模态预测，为自动驾驶车辆提供了重要的路面激励感知能力，显著提升了车辆安全性能。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [78] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM框架让LLM智能体从交互经验中自主构建可执行的程序性记忆，无需参数更新，通过语义梯度和PPO门控实现高质量技能验证，显著提升重用率和性能


<details>
  <summary>Details</summary>
Motivation: LLM驱动的智能体在序列决策中表现优异，但通常依赖即时推理，即使在重复场景中也会重新推导解决方案。这种经验重用的不足导致计算冗余和执行不稳定性。需要让智能体能够从交互经验中学习并重用程序性知识。

Method: 提出ProcMEM框架，通过形式化Skill-MDP将被动的事件叙述转化为可执行的技能（包含激活、执行和终止条件）。引入非参数PPO，利用语义梯度生成高质量候选技能，并通过PPO门控进行稳健的技能验证。采用基于分数的维护机制保持紧凑高质量的程序性记忆。

Result: 实验结果表明，在领域内、跨任务和跨智能体场景中，ProcMEM实现了优越的重用率和显著的性能提升，同时具有极高的记忆压缩率。可视化进化轨迹和技能分布进一步展示了ProcMEM如何透明地积累、精炼和重用程序性知识。

Conclusion: ProcMEM框架通过让智能体从交互经验中自主构建可执行的程序性记忆，有效解决了LLM智能体经验重用不足的问题，实现了计算效率的提升和长期自主性的促进。

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [79] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 本文提出一个几何框架来分析大语言模型中的多头注意力机制，将标准注意力视为top-N选择器，在值状态空间中研究其行为，定义了几何度量指标并推导了理论边界，实证验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 当前对注意力机制的理解缺乏几何视角，需要更系统的方法来分析多头注意力在值状态空间中的行为，以提供头部级别的可解释性，并为几何感知的注意力稀疏化和设计提供依据。

Method: 将标准注意力视为top-N选择器，在值状态空间中研究其行为；定义几何度量指标（精确率、召回率、F分数）来量化选择与非选择token之间的可分性；在经验假设下推导非渐近边界；在LLaMA-2-7B、Gemma-7B和Mistral-7B模型上进行实证验证。

Result: 理论预测了小N操作区域具有最强的非平凡可分性，阐明了序列长度和sink相似性如何影响度量指标；实证测量与理论包络线紧密吻合：top-N选择增强了可分性，sink相似性与召回率相关；在LLaMA-2-7B中发现注意力头专门化为三种机制：检索器、混合器和重置器，具有不同的几何特征。

Conclusion: 注意力机制表现为具有可测量token选择标准的结构化几何分类器，提供了头部级别的可解释性，并为几何感知的注意力稀疏化和LLM中注意力的设计提供了信息。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [80] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的基础模型，通过自监督双对比学习范式，解决了现有方法在数据标注需求、传感器类型适配和实际部署可行性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 智能家居传感器数据在医疗监测和辅助技术方面有重要应用潜力，但现有方法存在关键限制：监督模型需要大量标注数据；现有基础模型只关注惯性传感器，无法处理智能家居二元传感器事件的稀疏离散特性和丰富语义关联；基于LLM的方法需要自然语言描述或提示，依赖外部服务或昂贵硬件，存在隐私和成本问题。

Method: DomusFM采用自监督双对比学习范式，同时捕捉token级语义属性和序列级时间依赖性。通过整合轻量级语言模型的语义嵌入，以及专门用于时间模式和二元状态的编码器，学习可跨环境和任务迁移的通用表示。

Result: 在七个公共智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务上均优于最先进的基线方法，即使在仅有5%标注训练数据的情况下进行微调，也能实现卓越性能。

Conclusion: DomusFM解决了数据稀缺问题，同时保持了实际部署的可行性，为现实世界的智能家居系统提供了实用的解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [81] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 该研究比较了大型语言模型（LLM）和形式概念分析（FCA）在主题建模任务中的表现，通过两个实验评估它们在文档主题提取方面的效果。


<details>
  <summary>Details</summary>
Motivation: 主题建模在文档检索、情感分析和文本摘要等领域应用日益广泛。虽然大型语言模型在文本处理中很流行，但很少研究其在主题建模中的实用性。形式概念分析最近被提出作为主题建模的候选方法，但缺乏实际应用案例研究。本研究旨在比较这两种方法，了解它们在主题建模领域的优势和劣势。

Method: 研究采用对比实验设计：1）FCA通过CREA管道进行评估，该管道在过去的主题建模和可视化实验中使用过；2）LLM使用GPT-5，采用基于三个提示的零样本设置策略：从文档批次生成主题、将批次结果合并为最终主题、以及主题标注。第一个实验重用之前用于评估CREA的教学材料，第二个实验分析40篇信息系统研究文章，将提取的主题与底层子领域进行比较。

Result: 论文没有在摘要中提供具体结果数据，但通过两个实验对比了LLM和FCA在主题建模任务中的表现。第一个实验使用教学材料验证方法，第二个实验在信息系统研究文章上测试方法的实际应用效果。

Conclusion: 该研究通过系统比较LLM和FCA在主题建模中的应用，为理解这两种方法的相对优势和局限性提供了实证基础。研究结果表明两种方法在主题建模任务中各有特点，为未来主题建模方法的选择和应用提供了参考依据。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [82] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: GPS提出了一种基于贝叶斯推断的通用预测性提示选择方法，通过轻量级生成模型优化强化学习中的提示选择效率，显著提升训练和测试效率


<details>
  <summary>Details</summary>
Motivation: 强化学习增强大语言模型推理能力时计算成本高，现有在线提示选择方法要么依赖昂贵评估，要么缺乏跨提示泛化能力

Method: 提出GPS方法：使用轻量级生成模型在共享优化历史上进行贝叶斯推断预测提示难度，结合中等难度优先和历史锚定多样性的批量获取原则

Result: 在多种推理基准测试中，GPS在训练效率、最终性能和测试时效率方面均显著优于现有基线方法

Conclusion: GPS通过通用预测模型有效解决了强化学习中提示选择的效率和泛化问题，为高效推理训练提供了可行方案

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [83] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出基于知识图谱的对话诊断系统，通过两步推理（生成假设和验证假设）提高诊断准确性和效率，使用改进的患者模拟器评估系统性能


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法要么依赖模型的参数知识，要么假设患者提供丰富具体的信息，这在现实中不切实际，需要更现实的对话诊断系统

Method: 提出基于诊断知识图谱的对话诊断系统，采用两步推理：1)从对话上下文生成诊断假设；2)通过澄清问题验证假设，直到得出最终诊断。使用改进的MIMIC-IV患者模拟器，模拟早期临床接触中患者的模糊症状描述

Result: 实验显示系统在诊断准确性和效率上优于强基线，医生评估支持模拟器的真实性和生成问题的临床实用性

Conclusion: 基于知识图谱的对话诊断系统能够有效处理不完整信息，提高诊断性能，具有临床实用价值

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [84] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY是一个训练时框架，通过一致性自验证教LLMs推理事实不确定性，减少事实幻觉，同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 现有缓解事实幻觉的方法主要依赖外部事后验证或将不确定性直接映射到弃权，导致过于保守的行为，需要更有效的训练时解决方案

Method: VeriFY通过结构化验证轨迹增强训练，引导模型生成初始答案、验证查询、一致性判断和弃权决策，并引入阶段级损失掩码避免强化幻觉内容

Result: 在多个模型家族和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅轻微下降0.4%至5.7%，且在单源训练时能跨数据集泛化

Conclusion: VeriFY通过训练时的一致性自验证有效减少LLMs的事实幻觉，在保持召回率的同时显著提升可靠性，为事实准确性提供了有前景的解决方案

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [85] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了一种基于专家模型和单神经元门控机制的安全感知解码方法，通过低成本训练实现大语言模型的安全对齐，在保持实用性的同时增强输出安全性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全对齐方法主要依赖后训练技术，计算成本高且泛化能力差；轻量级方法要么依赖预先计算的安全注入，要么过度依赖模型自身能力，导致泛化有限、生成效率和使用性下降。

Method: 提出安全感知解码方法，仅需低成本训练专家模型，采用单神经元作为门控机制，有效平衡模型内在能力与外部指导。

Result: 该方法在训练开销和跨模型规模的泛化能力方面具有明显优势，同时保持实用性和增强输出安全性。

Conclusion: 为大语言模型的安全实用部署提供了一种轻量级对齐的新视角，实现了低成本、高泛化的安全增强方案。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [86] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 该论文提出了一种基于推理的知识内化训练策略，通过背景故事、多跳问题和知识蒸馏三个原则，使AI模型能更好地整合新知识并进行跨上下文推理。


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑方法主要关注原子事实，虽然改善了事实回忆，但未能将新信息整合到可在不同上下文中使用的连贯框架中。作者认为知识内化本质上是推理问题而非记忆问题。

Method: 提出基于三个原则的训练策略：1) 将新知识作为连贯的背景故事引入，解释新事实与现有知识的关系；2) 使用自生成的多跳问题进行训练，要求涉及新信息的多步推理；3) 采用知识蒸馏，让学生模型内化教师模型的推理行为而无需访问新信息。

Result: 实验表明，采用此策略训练的模型在推理过程中能有效利用新获取的知识，并在需要结合多个新事实的挑战性问题中取得了显著性能。

Conclusion: 知识内化应被视为推理问题而非记忆问题，通过将新知识嵌入连贯背景、进行多步推理训练和知识蒸馏，可以使AI模型更好地整合和应用新知识。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [87] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 本文提出CIR（规范中间表示）和R2C框架，用于从自然语言描述自动生成优化模型，解决现有LLM方法在处理复杂操作规则时的约束组合和建模范式问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的优化模型自动生成方法在处理复杂操作规则时面临挑战，特别是难以处理复合约束和选择合适的建模范式。需要一种能够将规则逻辑与数学实例化分离的方法。

Method: 提出CIR（规范中间表示）作为LLM在问题描述和优化模型之间显式生成的模式，通过约束原型和候选建模范式编码操作规则语义。在此基础上开发R2C多智能体管道，包含问题文本解析、CIR实现合成和优化模型实例化。

Result: R2C在新构建的基准测试中达到47.2%的准确率，在已有基准测试中表现具有竞争力，接近GPT-5等专有模型性能。通过反思机制进一步提升了性能，在某些基准测试中创造了新的最佳记录。

Conclusion: CIR和R2C框架有效解决了从自然语言描述自动生成优化模型的挑战，通过分离规则逻辑和数学实例化，显著提升了复杂操作规则的处理能力，在多个基准测试中取得了最先进的性能。

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [88] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 该论文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于改进LLM在合规和尽职调查等监管场景中的工作流程，相比单智能体基线在准确性、人工审核需求和处理时间方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在复杂监管工作流程中主要依赖单一智能体的提示工程，难以观察和比较模型如何处理跨决策阶段的不确定性和协调问题，以及与人类监督的交互。

Method: 引入基于有限时域马尔可夫决策过程的多智能体系统，具有有向无环结构。每个智能体对应特定角色或决策阶段，使用蒙特卡洛估计量化认知不确定性，系统级不确定性通过MDP在自动标记状态或人工审核状态的终止来捕获。

Result: 在AI安全评估（自残检测）的案例研究中，相比单智能体基线，实现了高达19%的准确性提升，高达85倍的人工审核需求减少，在某些配置下还减少了处理时间。

Conclusion: 提出的多智能体MDP框架能够有效处理LLM在监管工作流程中的不确定性和协调问题，显著提升系统性能并减少人工干预需求。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [89] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 本文研究了自回归语言模型（ARMs）中的"反转诅咒"问题，发现掩码扩散语言模型（MDMs）能部分缓解此问题，原因在于其架构结构与训练的交互作用，而非仅因训练目标。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在"反转诅咒"问题：学习了"A是B"后，往往无法处理反向查询"B是A"。掩码扩散语言模型对此问题的缓解程度较弱，但根本原因尚不清楚。现有解释归因于任意顺序训练目标，但这并不充分。

Method: 通过分析单层Transformer编码器的架构特性，研究权重共享如何使前向和反向注意力分数正相关。进一步证明相应的梯度是对齐的，因此最小化前向损失也能减少反向损失。在受控玩具任务和大规模扩散语言模型上进行实验验证。

Result: 实验结果表明，掩码扩散语言模型对反转诅咒的缓解源于架构结构及其与训练的交互作用。权重共享耦合了两个方向，使前向和反向注意力分数正相关，且梯度对齐，从而前向训练也能改善反向性能。

Conclusion: 掩码扩散语言模型部分克服反转诅咒的原因在于其架构设计（特别是权重共享）与训练的协同作用，而非仅由任意顺序训练目标解释。这解释了为什么在强大的自回归语言模型中持续存在的失败模式能在扩散模型中部分缓解。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [90] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 论文提出DGR方法，通过将外部安全推理数据集转换为与目标大语言模型内部分布对齐的形式，有效缓解安全对齐带来的"安全税"问题，在保持安全性能的同时显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大语言模型或人工标注中蒸馏安全推理轨迹和答案，但这些数据与需要对齐的目标模型存在分布差异，作者推测这种分布差异是导致目标模型推理能力显著下降的主要原因。

Method: 提出DGR方法，将现有的分布外安全推理数据集进行转换和精炼，使其与目标大语言模型的内部分布对齐，从而减少分布差异对推理能力的影响。

Result: 实验结果显示：1) DGR有效缓解安全税，在保持安全性能的同时，相比Vanilla SFT在DirectRefusal上平均推理准确率提升30.2%，在R1-ACT上提升21.2%；2) 推理能力下降程度与分布偏移程度相关；3) 仅需10个样本即可激活有效的拒绝行为，表明安全对齐可能主要作为激活潜在知识的机制。

Conclusion: 研究强调了分布一致性的重要性，并为安全在推理模型中的激活机制提供了见解。DGR方法通过减少分布差异，在保持安全性能的同时显著提升大语言模型的推理能力。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [91] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO是一个自然语言组合优化基准，用于评估大语言模型在端到端组合优化推理上的能力，覆盖43个问题，发现模型在小实例上表现良好但随规模增大而退化


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学和逻辑推理方面表现出色，但在组合优化（在高维解空间中搜索满足硬约束的解）方面的能力尚未充分探索，需要建立专门的评估基准

Method: 引入NLCO基准，采用四层分类法（变量类型、约束族、全局模式、目标类别）组织43个组合优化问题，提供求解器标注的解决方案，从可行性、解最优性和推理效率三个维度全面评估LLMs

Result: 高性能模型在小实例上表现出良好的可行性和解质量，但随着实例规模增大而退化，即使使用更多推理标记也无改善；集合任务相对容易，而图结构问题和瓶颈目标导致更频繁的失败

Conclusion: 大语言模型在组合优化推理方面存在局限性，特别是在处理大规模实例和复杂结构问题时，需要进一步研究提升其组合优化能力

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [92] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 提出TIDE框架，用于诊断和评估LLM智能体在测试时改进(TTI)过程中的性能瓶颈，重点关注任务优化效率、错误行为适应和工作记忆效用。


<details>
  <summary>Details</summary>
Motivation: 当前对自主LLM智能体测试时改进(TTI)的成功与失败机制理解不足，现有评估指标无法捕捉任务优化效率、错误行为后的适应能力以及工作记忆对任务完成的具体效用。

Method: 提出测试时改进诊断评估(TIDE)框架，将TTI分解为三个相互关联的维度：1)任务完成的整体时间动态；2)识别性能是否主要受递归循环行为限制；3)识别是否受累积记忆负担限制。

Result: 通过在不同智能体和环境中的广泛实验，TIDE表明提升智能体性能不仅需要扩展内部推理能力，还需要显式优化智能体与环境之间的交互动态。

Conclusion: TIDE框架为理解TTI机制提供了系统化的诊断工具，揭示了优化智能体性能需要同时关注内部推理能力和外部交互动态，为未来智能体设计提供了重要指导。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [93] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: LASER-KV是一种新的KV缓存压缩框架，通过分层累积选择和精确LSH召回机制，在严格累积预算策略下实现高效压缩，相比现有方法在长上下文任务中保持更稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型理论上支持扩展的上下文窗口，但实际部署受到KV缓存内存线性增长的制约。现有压缩策略通过剪枝机制在语义召回和内存效率之间进行权衡，但性能下降明显。

Method: 提出LASER-KV框架，采用分层累积选择与精确LSH召回。不同于标准的固定摘要大小方法，实现基于保护除数(n)的块状累积策略，将压缩效果与滑动窗口伪影隔离开来。

Result: 在Babilong基准测试中，先前压缩方法在各种长上下文任务上性能下降15-30%，而LASER-KV保持稳定性能，在128k上下文长度下准确率优势高达10%。

Conclusion: 研究结果挑战了仅凭注意力分数就足以代表token效用的普遍假设，LASER-KV展示了在严格累积预算下实现高效KV压缩的可行性。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [94] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 本文提出了比较性可解释AI（Δ-XAI）框架，用于解释大规模基础模型在干预后出现的行为变化，强调通过比较参考模型和干预模型来解释行为转变，而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在扩展、微调、强化学习或上下文学习后会出现行为转变，传统可解释AI方法只能分析单个检查点的模型，无法解释不同检查点间的内部变化，需要新的解释框架来理解这些行为转变。

Method: 提出比较性可解释AI（Δ-XAI）框架，包含一套设计原则，强调通过比较参考模型和干预模型来解释行为转变，介绍了可能的实现流程，并将它们与设计原则关联，提供了具体的Δ-XAI实验示例。

Result: 建立了Δ-XAI的理论框架，明确了比较性解释的核心目标，提出了具体的设计原则和实现流程，为解释模型行为转变提供了系统化的方法论。

Conclusion: 行为转变应该通过比较性方法来解释，Δ-XAI框架为解决大规模基础模型行为变化的解释问题提供了理论基础和实践指导，填补了传统XAI方法在解释模型变化方面的不足。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [95] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: 提出IPG框架，通过传播基于结果的信号来定位语言模型中的复杂推理机制


<details>
  <summary>Details</summary>
Motivation: 当前解释性方法难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响，需要新的方法来识别对推理行为有顺序贡献的组件

Method: 提出集成策略梯度（IPG）框架，通过将基于结果的信号（如推理后准确率）向后传播通过模型推理轨迹，将推理行为归因于模型内部组件

Result: 实证评估表明，该方法实现了更精确的定位，并能可靠地调节不同推理模型的推理行为（如推理能力、推理强度）

Conclusion: IPG框架基于结果导向和顺序影响感知原则，能够识别对推理行为有顺序贡献的组件，为理解语言模型的复杂推理机制提供了新方法

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [96] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA是一个预算高效的LLM选择框架，通过技能分析自动推荐最优模型，在保证性能的同时控制成本。


<details>
  <summary>Details</summary>
Motivation: 当前标准基准测试只报告聚合指标，无法揭示任务具体需要哪些能力，也无法判断更便宜的模型是否足够。LLM从业者需要在不浪费资金的情况下为任务选择合适的模型。

Method: BELLA采用三阶段方法：1) 通过批评者分析分解LLM输出并提取细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 使用多目标优化选择模型，在预算约束下最大化性能。框架提供自然语言推理，增加透明度。

Result: BELLA为从业者提供了原则性的成本-性能权衡决策框架，特别适用于金融推理等具有多样化技能需求和模型成本变化的领域。

Conclusion: BELLA框架通过可解释的基于技能的分析，实现了预算高效的LLM选择，解决了当前黑盒路由系统缺乏透明度的问题，使从业者能够做出更明智的模型部署决策。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [97] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 提出Thought-ICS框架，通过离散化思维步骤实现语言模型的自我错误定位与纠正，相比传统链式思维方法有显著提升


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否能够显式定位错误推理，以构建能够有效自我纠正的AI系统。受人类大脑在离散决策点监控错误并重新采样替代方案的启发

Method: 引入Thought-ICS框架：将推理结构化为离散、语义连贯的思维步骤，每次生成一个完整的思维单元（代表模型的明确决策），创建自然边界用于精确错误定位。在验证后，模型定位第一个错误步骤，系统回溯到最后一个正确点生成替代推理

Result: 在由oracle验证为错误的推理中，Thought-ICS实现了20-40%的自我纠正提升。在完全自主无外部验证的设置中，优于当代自我纠正基线方法

Conclusion: 通过结构化推理为离散思维步骤，语言模型能够可靠地定位错误，而传统非结构化链式思维方法则无法做到。Thought-ICS框架为构建能够有效自我纠正的AI系统提供了可行路径

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [98] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround是一个不确定性感知的GUI grounding框架，通过分布感知的不确定性量化方法和校准过程，实现具有统计保证的假发现率控制的风险感知预测。


<details>
  <summary>Details</summary>
Motivation: GUI grounding任务中错误的坐标预测可能导致代价高昂且难以逆转的操作（如错误的支付批准），因此需要提高模型可靠性并控制风险。

Method: 提出SafeGround框架：1）使用分布感知的不确定性量化方法捕捉模型输出随机样本的空间分散度；2）通过校准过程推导具有统计保证假发现率控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，SafeGround的不确定性度量在区分正确与错误预测方面优于现有基线，校准阈值能够可靠实现严格的风险控制，并在多个GUI grounding模型上将系统级准确率提升高达5.38个百分点。

Conclusion: SafeGround通过不确定性感知和统计校准，为GUI grounding任务提供了可靠的风险控制机制，显著提高了系统级准确率和可靠性。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [99] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 提出"Thinking with Comics"方法，使用漫画作为图像和视频之间的高信息密度媒介，在保持时间结构和叙事连贯性的同时降低推理成本


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法存在局限性：静态图像难以表示时间结构，而视频则冗余度高且计算成本大，需要一种既能保持时间信息又高效的中间表示

Method: 提出基于漫画的视觉推理范式，将漫画作为介于图像和视频之间的高信息密度媒介，系统研究基于漫画的两种推理路径，并在多种推理任务和长上下文理解任务上进行评估

Result: 实验结果显示，Thinking with Comics在多步时间和因果推理任务上优于Thinking with Images，同时比Thinking with Video显著更高效。不同漫画叙事结构和风格对任务性能有持续影响

Conclusion: 漫画作为有效的中间视觉表示，能够改善多模态推理，在保持时间结构的同时降低计算成本，为视觉推理提供了新的有效范式

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [100] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 论文开发了MentisOculi评估套件，发现当前统一多模态模型虽然具备文本推理能力并能生成正确视觉内容，但无法有效利用视觉化作为推理辅助工具，视觉思维目前对模型推理没有帮助。


<details>
  <summary>Details</summary>
Motivation: 随着前沿模型从多模态大语言模型向统一多模态模型转变，研究者希望探索使用中间视觉化作为推理辅助工具的可能性，类似于人类的心理意象。核心问题是评估模型形成、维持和操作视觉表征的能力。

Method: 开发了MentisOculi——一个程序化、分层的多步推理问题套件，专门设计来挑战前沿模型。评估了从潜在标记到显式生成图像的各种视觉策略。

Result: 发现视觉策略通常无法提高性能。统一多模态模型虽然具备文本推理能力，有时能生成正确的视觉内容，但存在累积生成错误，甚至无法有效利用真实视觉化。视觉思维目前对模型推理没有帮助。

Conclusion: 尽管视觉思维具有内在吸引力，但目前尚未能提升模型推理能力。MentisOculi为分析和弥合这一差距奠定了必要基础，适用于不同模型家族的研究。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [101] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 本文挑战了自回归大语言模型在逻辑推理中的"反转诅咒"是固有缺陷的观点，提出通过简单的"身份桥"数据正则化方法，可以显著改善模型的反向推理能力。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在复杂任务上表现出色，但在简单逻辑推理如"反转诅咒"上仍然失败。先前研究认为这是自回归因果LLMs的固有、根本性限制，表明这些模型倾向于记忆事实级知识而非捕获高级规则。本文旨在挑战这一观点，探索是否可以通过数据层面的简单调整来缓解这一限制。

Method: 提出了一种称为"身份桥"的简单正则化数据配方，形式为"A → A"（例如：Alice的名字是Alice）。理论上，通过分析梯度下降的隐式偏差，证明即使单层Transformer也能打破反转诅咒。实证上，使用该数据配方微调1B预训练语言模型，与仅使用前向知识数据训练进行对比。

Result: 使用提出的数据配方微调的1B语言模型在反转任务上达到40%的成功率，与仅使用前向知识数据训练时的接近零成功率形成鲜明对比。

Conclusion: 本文为反转诅咒提供了新的理论基础，并为鼓励LLMs从数据中学习更高级规则提供了一种原则性、低成本的路径，挑战了自回归LLMs在逻辑推理方面存在固有缺陷的传统观点。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>
