<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 该论文研究大型语言模型在零样本推荐中的不确定性和公平性问题，提出新的评估方法并发现Gemini 1.5 Flash存在系统性不公平，同时引入人格感知的公平性基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够利用广泛上下文知识实现强大的零样本推荐，但其预测不确定性和内嵌偏见威胁着推荐的可靠性和公平性。需要研究不确定性和公平性评估如何影响LLM生成推荐的准确性、一致性和可信度。

Method: 1. 引入包含精心设计指标的基准和数据集，数据集标注了8个人口统计属性（31个分类值），涵盖电影和音乐两个领域；2. 通过深度案例研究量化预测不确定性（通过熵）；3. 分析Google DeepMind的Gemini 1.5 Flash对敏感属性的系统性不公平；4. 在提示扰动（如拼写错误和多语言输入）下测试差异持续性；5. 将人格感知公平性集成到RecLLM评估流程中；6. 提出新颖的不确定性感知评估方法。

Result: 1. Gemini 1.5 Flash对某些敏感属性表现出系统性不公平，测量的基于相似性的差距为SNSR 0.1363和SNSV 0.0507；2. 这些差异在提示扰动下持续存在；3. 揭示了人格相关的偏见模式；4. 暴露了个性化与群体公平性之间的权衡；5. 建立了更安全、更可解释的RecLLM基础。

Conclusion: 该研究为更安全、更可解释的推荐LLM奠定了基础，提出了不确定性感知评估方法和人格档案知情公平性基准，推动了LLM推荐的可解释性和公平性，并激励未来在多模型基准和自适应校准方面的研究，以实现可信部署。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [2] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估指标NSG来衡量大语言模型自我解释的忠实度，发现自我解释能显著提升对模型行为的预测能力，但也存在5-15%的严重误导性解释。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型自我解释忠实度评估方法存在严重局限性，通常依赖对抗性提示或检测推理错误，忽视了解释的预测价值。需要一种更通用、可扩展的指标来评估解释是否真实反映模型的真实推理过程。

Method: 提出了归一化可模拟增益（NSG）指标，基于"忠实解释应能让观察者学习模型的决策标准，从而更好地预测其在相关输入上的行为"的理念。在7,000个来自健康、商业和伦理领域的反事实数据上评估了18个前沿专有和开源模型。

Result: 自我解释显著提高了对模型行为的预测能力（11-37% NSG增益）。自我解释比外部模型生成的解释提供更多预测信息，即使外部模型更强大，这表明自我知识带来的优势是外部解释方法无法复制的。同时发现5-15%的自我解释存在严重误导性。

Conclusion: 尽管存在不完美之处，但自我解释确实编码了有助于预测模型行为的信息，为自我解释的积极应用提供了实证支持。NSG指标为评估解释忠实度提供了一个通用且可扩展的框架。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [3] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS框架通过预算感知规划、模块化构建和比较反思记忆三大支柱，优化了AI研究的自动化过程，在MLE-Bench上达到开源框架中的最先进性能。


<details>
  <summary>Details</summary>
Motivation: AI研究与一般软件工程不同，涉及计算昂贵的评估（如模型训练）和性能归因不透明。当前基于LLM的智能体在这方面表现不佳，常生成忽略执行成本和因果因素的单体脚本。

Method: MARS框架包含三大支柱：1) 预算感知规划：通过成本约束的蒙特卡洛树搜索平衡性能与执行成本；2) 模块化构建：采用"设计-分解-实现"流程管理复杂研究仓库；3) 比较反思记忆：通过分析解决方案差异来提取高信号洞察，解决信用分配问题。

Result: 在可比设置下，MARS在MLE-Bench上达到开源框架中的最先进性能，与全球排行榜的顶级方法保持竞争力。63%的已使用经验来自跨分支转移，表明智能体能有效跨搜索路径泛化洞察。

Conclusion: MARS框架通过系统性方法解决了AI研究自动化的关键挑战，实现了性能与成本的平衡，并能有效跨搜索路径学习和泛化洞察。

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [4] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 本文提出了一种动态混合精度路由框架，用于在长时程决策任务中自适应选择高精度和低精度LLM，以平衡任务成功率和推理成本。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在长时程决策任务中表现优异，但使用大型LLM进行多步交互会产生高昂的推理成本。传统观点认为更高的任务成功率需要使用更大更强的LLM模型，但作者观察到不同交互步骤对精度的敏感性存在差异，因此探索使用低精度量化LLM来降低成本。

Method: 提出动态混合精度路由框架，在每个决策步骤自适应选择高精度或低精度LLM。路由器通过两阶段管道训练：1）基于KL散度的监督学习识别精度敏感步骤；2）使用组相对策略优化（GRPO）进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在准确率-成本权衡方面相比单精度基线和启发式路由方法有显著改进。

Conclusion: 通过动态混合精度路由框架，可以在保持任务成功率的同时显著降低长时程决策任务的推理成本，为LLM在实际应用中的部署提供了有效的成本优化方案。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [5] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish是一个统一的全原子LLM，通过自适应缩放结构补丁和几何接地适配器，在几何线索上建立语言推理，减少结构幻觉，实现异构结构接地推理的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理生物分子结构时存在局限性：要么是模态特定的，要么通过序列标记化或固定长度查询连接器压缩结构输入，导致几何基础缺失或模态融合瓶颈，阻碍了通用全原子推理的实现。

Method: 1. 缩放感知补丁：使用指令条件门控机制在结构图上生成可变大小的补丁，根据结构复杂度自适应缩放查询令牌预算，缓解固定长度连接器瓶颈。2. 几何接地适配器：通过跨注意力机制将自适应令牌与模态嵌入进行细化，并将生成的模态令牌注入LLM，暴露明确的几何线索以减少结构幻觉。

Result: 在多样化的全原子基准测试中，Cuttlefish在异构结构接地推理方面实现了优越性能。

Conclusion: Cuttlefish通过自适应缩放结构补丁和几何接地适配器，成功在几何线索上建立语言推理，减少了结构幻觉，为通用全原子推理提供了有效的解决方案。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [6] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer是一个基于大型语言模型的反射式元优化框架，用于解决模拟混合信号集成电路的晶体管尺寸优化问题，通过两阶段优化循环和自适应搜索空间构建，显著提升了优化效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号集成电路设计严重依赖专家知识，晶体管尺寸优化面临非线性行为、高维设计空间和严格性能约束等挑战。现有EDA方法通常将尺寸优化视为静态黑盒优化，导致效率低下且鲁棒性差。虽然大型语言模型具备强大的推理能力，但不适合AMS尺寸的精确数值优化。

Method: 提出AutoSizer框架，采用反射式LLM驱动的元优化方法，将电路理解、自适应搜索空间构建和优化编排统一在闭环中。采用两阶段优化框架：内循环负责电路尺寸优化，外循环分析优化动态和约束，根据仿真反馈迭代优化搜索空间。同时建立了AMS-SizingBench基准测试集。

Result: AutoSizer在实验中表现出更高的解决方案质量、更快的收敛速度和更高的成功率，在不同电路难度下均优于传统优化方法和现有的LLM智能体。建立了包含24个多样化AMS电路的开放基准测试集。

Conclusion: AutoSizer成功地将LLM的推理能力与数值优化相结合，通过反射式元优化框架有效解决了AMS电路尺寸优化问题，为EDA领域提供了新的解决方案。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [7] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER框架通过进化集成优化实现可调控的LLM决策，在临床分诊等序数决策任务中提供单一可解释的风险控制参数，避免模式崩溃并保持领域能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在平均正确性训练下容易出现模式崩溃，特别是在需要权衡特异性和敏感性的序数决策场景（如临床分诊）中，标准对齐方法丧失了基于上下文约束调整ROC操作点的能力。

Method: STEER通过离线约束质量多样性搜索构建自然语言角色群体，在保证最低安全、推理和稳定性阈值的同时促进行为覆盖。推理时通过单一可解释的控制参数将用户指定的风险百分位数映射到选定角色，实现决策保守性的单调调整。

Result: 在两个临床分诊基准测试中，STEER相比基于温度的采样和静态角色集成实现了更广泛的行为覆盖。与代表性后训练方法相比，在明确紧急情况下保持显著更高的准确性，同时在模糊决策上提供可比较的控制能力。

Conclusion: STEER作为一种保持安全性的风险控制范式，能够在不影响领域能力的情况下引导行为，为LLM在需要可调控决策的场景中提供了有效的解决方案。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [8] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 提出BenchAlign方法，通过有限的实际部署信息自动更新离线基准测试，使其能更好地预测模型在真实场景中的性能偏好


<details>
  <summary>Details</summary>
Motivation: 当前语言模型基准测试虽然计算高效，但往往无法准确预测模型在真实世界中的实际效用，需要建立基准测试与实际性能之间的桥梁

Method: 提出BenchAlign方法，利用模型在基准测试问题上的表现和部署期间收集的模型排序对，学习与偏好对齐的基准问题权重，生成能根据偏好对新模型进行排序的新基准

Result: 实验表明，对齐后的基准测试能准确根据人类偏好模型对未见过的模型进行排序，且在不同规模模型上均有效，同时保持可解释性

Conclusion: 这项工作为基准测试与实用人类偏好的对齐提供了见解，有望加速模型开发向实际效用方向发展

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [9] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文研究了思维链推理所需的最小推理标记数量，证明了三个典型任务需要Ω(n)个推理标记，并通过实验验证了理论下界。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能提升大语言模型性能，但带来了显著的延迟和计算成本。本文旨在从理论上探究随着输入规模增长，解决问题所需的最小推理标记数量。

Method: 扩展了有界注意力前缀预言机模型，用于量化解决任务所需的信息流。针对三个BAPO-hard任务（二进制多数、三元组匹配、图可达性）证明了推理标记的下界，并通过显式构造提供了匹配或接近匹配的上界。

Result: 证明了三个任务都需要Ω(n)个推理标记（输入规模为n时）。实验显示前沿推理模型在这些任务上表现出近似线性的推理标记扩展，当限制在较小的推理预算时会失败，与理论下界一致。

Conclusion: 研究识别了通过思维链进行推理计算的基本瓶颈，为分析最优推理长度提供了原则性工具。结果表明线性推理标记扩展是这些任务的固有要求。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [10] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: UAT-LITE：一种推理时框架，通过蒙特卡洛dropout使预训练Transformer分类器的自注意力具有不确定性感知能力，无需修改权重或训练目标，显著改善校准误差和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 神经NLP模型通常校准不佳，对错误预测赋予高置信度，这影响了选择性预测和高风险部署。现有的后处理校准方法只调整输出概率而不改变内部计算，而集成和贝叶斯方法虽然改善不确定性但训练或存储成本高昂。

Method: 提出UAT-LITE推理时框架，通过蒙特卡洛dropout进行近似贝叶斯推断，使预训练Transformer分类器的自注意力具有不确定性感知能力。从随机前向传递中估计token级认知不确定性，并在上下文化过程中调制自注意力，无需修改预训练权重或训练目标。还引入了层间方差分解来诊断预测不确定性如何在Transformer深度中累积。

Result: 在SQuAD 2.0可回答性、MNLI和SST-2数据集上，UAT-LITE相对于微调的BERT-base基线，将预期校准误差平均降低约20%，同时保持任务准确性，并改善了选择性预测和分布偏移下的鲁棒性。

Conclusion: UAT-LITE提供了一种轻量级、无需训练的方法来改善Transformer模型的不确定性估计和校准性能，为高风险NLP应用提供了更可靠的置信度评估。

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [11] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: 本文提出GCR-RL（几何一致性正则化强化学习），通过序理论视角将价值函数估计重新构建为学习期望的偏序集，利用超偏序集精化序列确保价值函数底层偏序集的几何一致性，显著提升样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 几何性质可用于稳定和加速强化学习，现有方法包括编码对称结构、几何感知数据增强和施加结构限制。本文从序理论的新视角重新审视RL，将价值函数估计重新构建为学习期望的偏序集。

Method: 提出GCR-RL方法，通过计算超偏序集精化序列——通过精化先前步骤中的偏序集并从时序差分信号中学习额外的序关系——确保支撑学习价值函数的偏序集序列的几何一致性。开发了基于Q学习和actor-critic的两种新算法来实现这些超偏序集精化。

Result: 分析了算法的理论性质和收敛速率。在多种任务中实证评估GCR-RL，相比强基线方法，在样本效率和稳定性能方面表现出显著改进。

Conclusion: 通过序理论视角重新构建强化学习，提出的GCR-RL方法利用几何一致性正则化，通过超偏序集精化序列有效提升学习效率和稳定性，为强化学习提供了新的几何框架。

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [12] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs在因果推理任务中表现出比人类更规则化的推理策略，较少受到人类典型偏见的影响，但可能在不确定性场景中失效


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在需要因果推理的领域应用增多，需要了解其推理是基于规范因果计算、人类式捷径还是脆弱的模式匹配

Method: 在11个基于碰撞器结构(C₁→E←C₂)的因果判断任务上，将20多个LLMs与匹配的人类基线进行对比，使用可解释模型压缩LLMs的因果判断，并测试其在语义抽象和提示过载下的鲁棒性

Result: LLMs表现出比人类更规则化的推理策略，较少受到人类典型的碰撞器偏见（弱解释消除和马尔可夫违反）影响；思维链(CoT)能提高许多LLMs的鲁棒性

Conclusion: LLMs可以在已知偏见不受欢迎时补充人类判断，但其规则化推理在不确定性场景中可能失效，需要表征LLMs的推理策略以确保安全有效部署

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [13] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: LLMs在训练中获得了序列级规划能力，但在推理时表现出短视和不一致的规划行为，研究者提出贝叶斯解释：自生成上下文驱动规划偏移，导致看似受损的规划表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在训练中获得了序列级规划能力，但在推理时却表现出短视和不一致的规划行为，这种能力与表现之间的差距需要理论解释。

Method: 提出贝叶斯解释框架，将规划行为置于演化的生成上下文中；通过两个受控实验验证：随机生成任务展示人类提示下的受限规划以及自生成上下文积累时规划强度的增加；高斯采样任务显示在条件化自生成序列时初始偏见的减少。

Result: 实验验证了提出的模型：随机生成任务显示随着自生成上下文积累，规划强度增加；高斯采样任务显示在条件化自生成序列时初始偏见减少，支持了自生成上下文驱动规划偏移的理论。

Conclusion: 研究为LLMs在推理时如何提前规划提供了理论解释和实证证据，表明自生成上下文在驱动规划偏移中起关键作用，解释了训练获得的能力与推理表现之间的差距。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [14] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文综述了可微分社会选择这一新兴范式，将投票规则、机制和聚合过程构建为可从数据中优化的可学习、可微分模型，连接了机器学习、经济学和民主理论。


<details>
  <summary>Details</summary>
Motivation: 社会选择已从政治理论和经济学的边缘领域转变为现代机器学习系统的基础组成部分。从拍卖和资源分配到联邦学习、参与式治理和大语言模型对齐，机器学习流程越来越多地将异质偏好、激励和判断聚合为集体决策。许多当代机器学习系统已经实现了社会选择机制，但通常是隐式的且缺乏明确的规范审查。

Method: 采用可微分社会选择范式，将投票规则、机制和聚合过程构建为可学习、可微分的模型，通过数据驱动的方式进行优化。综述了拍卖、投票、预算编制、流动民主、去中心化聚合和逆向机制学习等领域的工作。

Result: 展示了经典公理和不可能性定理如何重新表现为目标、约束和优化权衡。通过整合不同领域的研究，为机器学习、经济学和民主理论的交叉领域提供了系统性框架。

Conclusion: 识别了36个开放问题，定义了机器学习、经济学和民主理论交叉领域的新研究议程。可微分社会选择为设计和分析机器学习系统中的集体决策机制提供了新的理论和方法论基础。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [15] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP是一个推理感知的主动蒸馏框架，通过将教师模型的决策过程外部化为有向无环图，并用模块化概念预测器在学生模型中镜像该图，从而提高样本效率、训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在判别性任务部署中存在推理延迟、计算和API成本高的问题。主动蒸馏虽然能降低成本，但大多数方法只蒸馏最终标签，丢弃了中间推理信号，且缺乏对缺失推理内容和错误来源的诊断能力。

Method: 提出Graph of Concept Predictors (GCP)框架：1）将教师模型的决策过程外部化为有向无环图；2）在学生模型中使用模块化概念预测器镜像该图结构；3）采用图感知的获取策略，针对关键推理节点的不确定性和分歧进行采样；4）实施有针对性的子模块重训练，将下游损失归因于特定概念预测器，只更新最有影响的模块。

Result: 在八个NLP分类基准测试上的实验表明，GCP在有限标注预算下提高了性能，同时产生了更可解释和可控的训练动态。

Conclusion: GCP框架通过外部化教师模型的推理过程并采用模块化设计，不仅提高了主动蒸馏的样本效率和性能，还增强了训练过程的可解释性和可控性，为部署高效的判别性模型提供了新思路。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [16] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC-GRPO方法解决多轮工具调用中奖励稀疏和探索成本高的问题，通过引入奖励条件令牌改善组内多样性，在BFCLv4基准上超越基线方法


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对大型语言模型具有挑战性，因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化低时（如多数rollout获得全0或全1奖励）会停滞，导致组归一化优势信息不足，更新消失

Method: 提出RC-GRPO（奖励条件组相对策略优化），将探索视为通过离散奖励令牌的可控引导问题。首先在混合质量轨迹上微调奖励条件轨迹策略（RCTP），在提示中注入奖励目标特殊令牌（如<|high_reward|>, <|low_reward|>），使模型能够按需生成不同质量的轨迹。然后在RL过程中，在每个GRPO组内采样多样化的奖励令牌，并将rollout条件化于采样的令牌，以提高组内多样性

Result: 在Berkeley Function Calling Leaderboard v4（BFCLv4）多轮基准测试中，该方法相比基线方法获得持续改进的性能，Qwen-2.5-7B-Instruct模型的性能甚至超过所有闭源API模型

Conclusion: RC-GRPO通过奖励条件令牌有效解决了多轮工具调用中组内多样性不足的问题，显著提升了模型性能，为稀疏奖励环境下的强化学习提供了新思路

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [17] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS：基于工具驱动的多智能体系统，通过分析器-推理器-执行器范式，结合视觉推理和潜在重构，实现通用时间序列任务处理


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法在整合直观视觉推理和跨任务泛化方面存在局限，缺乏自适应工具使用能力

Method: 提出MAS4TS系统，采用分析器-推理器-执行器范式，结合视觉语言模型进行时间序列图的视觉推理，在潜在空间重构预测轨迹，通过三个专门智能体协调工作，路由器选择任务特定工具链

Result: 在多个基准测试中达到最先进性能，展现出强大的泛化能力和高效推理

Conclusion: MAS4TS通过工具驱动的多智能体系统成功解决了时间序列分析中视觉推理和跨任务泛化的挑战

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [18] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: 提出KANFIS（Kolmogorov-Arnold神经模糊推理系统），一种紧凑的神经符号架构，通过加性函数分解统一模糊推理，解决传统ANFIS在高维空间中的规则爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 传统ANFIS架构存在结构复杂性问题，基于乘积的推理机制在高维空间中会导致规则数量指数级爆炸，限制了其在实际应用中的可扩展性和可解释性。

Method: 提出KANFIS架构，采用加性聚合机制而非乘积推理，使模型参数和规则复杂度随输入维度线性而非指数增长；兼容Type-1和Interval Type-2模糊逻辑系统；通过稀疏掩码机制生成紧凑结构化规则集。

Result: KANFIS在保持可解释性的同时，实现了与代表性神经和神经模糊基线模型相竞争的性能表现。

Conclusion: KANFIS通过加性函数分解和稀疏掩码机制，成功解决了传统ANFIS的规则爆炸问题，提供了一种紧凑、可扩展且内在可解释的神经模糊推理系统。

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [19] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 该研究系统评估了多智能体系统中过程验证的有效性，发现过程级验证并不能持续提升性能，且存在高方差问题，表明可靠的MAS过程验证仍是一个开放挑战。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在推理轨迹上表现出高方差，过程验证在一般推理场景中显示潜力，但其在MAS中的实际效果尚不明确，需要系统性实证研究填补这一空白。

Method: 提出了MAS-ProVe框架，系统研究三种验证范式（LLM-as-a-Judge、奖励模型、过程奖励模型），在两个验证粒度（智能体级和迭代级）上评估，考察五种代表性验证器和四种上下文管理策略，在六个不同MAS框架和多个推理基准上进行实验。

Result: 过程级验证不能持续提升性能且经常表现出高方差；LLM-as-a-Judge通常优于基于奖励的方法；训练过的法官优于通用LLM；LLM作为法官与作为单智能体之间存在小的性能差距；验证中存在上下文长度与性能的权衡。

Conclusion: 多智能体系统的有效且鲁棒的过程验证仍然是一个开放挑战，需要超越当前范式的进一步进展。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [20] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench是一个系统化的智能体安全评估框架，通过领域无关的安全原则和上下文感知的安全标准，在真实部署场景下评估智能体的安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前智能体安全评估存在局限性：依赖特定场景的风险导向任务，安全风险覆盖有限；无法评估智能体在复杂真实部署中长期交互任务执行中的安全行为；对特定智能体设置的专门化限制了跨不同配置的适应性。

Method: 提出Risky-Bench框架：围绕领域无关的安全原则组织评估；推导上下文感知的安全标准来界定安全空间；在不同威胁假设下通过真实任务执行系统评估安全风险。

Result: 在生活辅助智能体设置中应用Risky-Bench，发现在真实执行条件下最先进的智能体存在重大安全风险。该框架具有良好的结构化评估流程，可适应其他部署场景构建环境特定的安全评估。

Conclusion: Risky-Bench提供了一个可扩展的智能体安全评估方法学，能够系统评估智能体在真实世界部署中的安全风险，超越了现有评估方法的局限性。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [21] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: 本文介绍了MAFBench，一个用于评估多智能体LLM框架的统一评测套件，通过标准化执行管道集成现有基准测试，揭示了框架架构选择对系统性能的巨大影响。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架广泛用于加速基于大语言模型的智能体系统开发，但这些框架采用不同的架构结构来管理智能体交互、信息存储和任务协调。然而，这些架构选择对系统性能的影响尚不清楚，而架构选择本身可能导致延迟和吞吐量数量级的差异，以及准确性和可扩展性的显著变化。现有基准测试主要关注单个能力，缺乏标准化的框架级评估。

Method: 1) 引入一个架构分类法，用于系统比较多智能体LLM框架的基本维度；2) 开发MAFBench，一个统一评估套件，将现有基准测试集成到标准化执行管道中；3) 使用MAFBench对多个广泛使用的框架进行受控实证研究。

Result: 研究发现框架级设计选择单独就能导致延迟增加超过100倍，规划准确性降低高达30%，协调成功率从90%以上降至30%以下。这些结果揭示了架构选择对系统性能的显著影响。

Conclusion: 研究将发现转化为具体的架构设计原则和框架选择指导，并概述了有前景的未来研究方向。MAFBench为系统比较多智能体LLM框架提供了标准化评估方法，有助于开发者和研究人员做出更明智的架构选择。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [22] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 论文扩展了先前关于智能体必须包含环境模型的理论，从确定性、完全可观测环境扩展到随机性、部分可观测环境，证明随机智能体也无法避免学习环境模型。


<details>
  <summary>Details</summary>
Motivation: 先前研究证明了在确定性智能体和完全可观测环境的特定框架下，几乎最优的通用智能体必然包含足够的环境知识。本研究旨在移除这两个限制条件，探索在更现实场景下的智能体建模能力。

Method: 将原定理扩展到随机智能体和部分可观测环境，通过数学证明展示随机智能体也无法避免学习环境模型。同时通过弱化"通用性"概念，证明能力较弱的智能体也包含其操作环境的世界模型。

Result: 成功证明了随机智能体在部分可观测环境中仍然必须学习环境模型，随机化无法避免这一要求。同时证明了更弱的智能体（即通用性要求降低）也包含其环境的世界模型。

Conclusion: 随机智能体在部分可观测环境中也无法避免学习环境模型，这一发现扩展了智能体建模理论的基础，为理解智能体能力与限制提供了更全面的理论框架。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [23] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 提出了一种通用的缺失模态恢复策略，通过增强扩散模型作为可插拔模块来恢复缺失特征，在零样本评估中优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在模态缺失时性能急剧下降，现有方法存在两个困境：基于提示的方法难以恢复缺失的关键特征并损害模型泛化能力；基于插补的方法缺乏有效指导，容易生成语义无关的噪声。恢复精确语义同时保持VLM泛化能力仍然具有挑战性。

Method: 提出通用缺失模态恢复策略，引入增强扩散模型作为可插拔的中阶段训练模块。包含两个关键创新：(1) 动态模态门控，自适应利用条件特征引导生成语义一致的特征；(2) 跨模态互学习机制，桥接双编码器的语义空间实现双向对齐。

Result: 在基准数据集上的零样本评估表明，该方法优于现有基线方法。大量实验和消融研究证实该模型在缺失模态场景下是VLM的鲁棒且可扩展的扩展，确保在不同缺失率和环境下的可靠性。

Conclusion: 提出的通用缺失模态恢复策略通过增强扩散模型有效解决了VLM在模态缺失时的性能下降问题，为VLM在现实世界不完整数据场景下的应用提供了可靠解决方案。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [24] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW是一个统一框架，用于提取、评估和以校准强度控制大语言模型的价值对齐，包含分层价值嵌入空间、价值强度数据库和基于锚点的评估器。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型与人类价值对齐存在挑战：基于偏好的方法难以捕捉深层动机原则，基于价值的方法虽然更有原则性，但在提取时忽略层次结构、评估时只检测存在而非校准强度、以及可控强度下的可操控性理解不足。

Method: 提出VALUEFLOW框架，包含三个组件：1) HIVES分层价值嵌入空间，捕捉理论内和跨理论的价值结构；2) VIDB价值强度数据库，大规模价值标注文本资源，基于排名聚合获得强度估计；3) 基于锚点的评估器，通过将模型输出与VIDB面板排名来产生一致的强度分数。

Result: 在10个模型和4个价值理论上进行了大规模研究，识别了可操控性的不对称性和多价值控制的组合规律，建立了评估和控制价值强度的可扩展基础设施。

Conclusion: VALUEFLOW为评估和控制价值强度建立了可扩展的基础设施，推进了大语言模型的多元对齐。

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [25] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 论文提出TAME框架解决智能体在任务演化过程中记忆错误演化导致的安全对齐问题，通过双记忆演化机制在提升任务性能的同时保持可信度。


<details>
  <summary>Details</summary>
Motivation: 智能体在测试时通过记忆演化积累经验是实现AGI的关键范式，但即使在良性任务演化过程中，智能体的安全对齐仍然脆弱，这种现象被称为"智能体记忆错误演化"。需要评估和解决这一现象。

Method: 提出TAME双记忆演化框架：1) 执行器记忆演化以提升任务性能，通过提炼可泛化的方法论；2) 评估器记忆演化以基于历史反馈精化对安全性和任务效用的评估。通过记忆过滤、草稿生成、可信度精化、执行和双轨记忆更新的闭环流程。

Result: 构建了Trust-Memevo基准来评估良性任务演化中的多维度可信度，发现各种任务域和评估设置中可信度普遍下降。TAME实验表明能够缓解记忆错误演化，在可信度和任务性能上实现联合提升。

Conclusion: TAME框架通过分离演化执行器记忆和评估器记忆，在保持可信度的同时不牺牲任务效用，为解决智能体记忆错误演化问题提供了有效方案。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [26] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 论文指出当前LLM智能体评估存在标准化不足的问题，提出需要统一的评估框架来解决系统提示、工具配置、环境动态等混杂因素带来的评估偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，通用智能体取得了根本性进步，但评估这些智能体面临独特挑战。当前智能体基准测试受到系统提示、工具集配置、环境动态等外部因素的严重干扰，现有评估依赖碎片化、研究者特定的框架，使得难以将性能提升归因于模型本身。

Method: 论文提出标准化智能体评估的提案，旨在建立一个统一的评估框架，解决当前评估中的标准化不足问题。

Result: 论文识别了当前智能体评估中的关键问题：1）提示工程差异导致难以比较模型性能；2）缺乏标准化环境数据导致错误不可追溯和结果不可复现；3）标准化不足引入显著的不公平性和不透明性。

Conclusion: 统一的评估框架对于智能体评估的严格推进至关重要，论文提出的标准化提案旨在解决当前评估中的混杂因素和可复现性问题，促进该领域的公平和透明发展。

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [27] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking框架让大语言模型学会通过动态总结来自我调节推理步骤的粒度，实现推理上下文的压缩，在保持准确性的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有的长思维链方法虽然能显著提升推理能力，但由于KV缓存的线性增长和注意力机制的二次方复杂度，在实际应用中面临计算和内存限制

Method: 提出Accordion-Thinking端到端框架，让LLMs通过动态总结学习自我调节推理步骤粒度；引入Fold推理模式，模型定期总结思维过程并丢弃历史标记；使用强化学习进一步激励这种能力

Result: 模型学会了将关键推理信息编码到紧凑的总结中，实现了推理上下文的有效压缩；在48GB GPU内存配置下实现了3倍吞吐量同时保持准确性；Fold模式与完整Unfold模式之间的准确率差距在训练过程中逐渐缩小直至消失

Conclusion: 通过学习的自我压缩机制，LLMs能够以最小的依赖标记开销处理复杂推理任务而不影响解决方案质量，同时结构化的步骤总结提供了人类可读的推理过程记录

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [28] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: 本文介绍了CSR-Bench基准测试，用于评估多模态大语言模型在跨模态可靠性方面的表现，发现模型存在系统性跨模态对齐差距，安全感知薄弱，语言主导性强，且存在安全性与过度拒绝之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的安全行为可能由单模态捷径驱动而非真正的联合意图理解，需要评估模型在跨模态可靠性方面的表现，诊断模态引起的行为变化。

Method: 提出CSR-Bench基准测试，包含安全、过度拒绝、偏见和幻觉四个压力测试交互模式，涵盖61种细粒度类型。每个实例都需要图像-文本集成解释，并提供配对的纯文本控制组以诊断模态引起的行为变化。评估了16个最先进的多模态大语言模型。

Result: 观察到系统性跨模态对齐差距：模型安全感知薄弱，干扰下语言主导性强，从纯文本控制组到多模态输入存在一致的性能下降。发现减少过度拒绝与保持安全、非歧视行为之间存在明显权衡，表明一些表面上的安全改进可能来自拒绝导向的启发式方法而非稳健的意图理解。

Conclusion: 多模态大语言模型在跨模态可靠性方面存在系统性缺陷，需要更深入的研究来确保模型基于真正的联合意图理解而非单模态捷径做出安全决策。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [29] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出Agentic Proposing框架，通过智能体驱动的顺序决策过程合成高质量、可验证的复杂推理训练数据，显著提升下游模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前复杂推理任务中，高质量可验证数据集依赖昂贵的人工标注，难以扩展。现有合成方法面临结构有效性与问题复杂性之间的权衡：保持结构有效性会限制问题复杂度，而增加难度又会导致不一致或不可解实例。

Method: 提出Agentic Proposing框架，将问题合成建模为目标驱动的顺序决策过程，使用专门智能体动态选择和组合模块化推理技能。通过内部反思和工具使用的迭代工作流程，使用多粒度策略优化（MGPO）开发Agentic-Proposer-4B模型，生成数学、编程和科学领域的高精度可验证训练轨迹。

Result: 基于智能体合成数据训练的下游求解器显著优于领先基线，并展现出强大的跨领域泛化能力。仅使用11,000个合成轨迹训练的30B求解器在AIME25上达到91.6%的SOTA准确率，媲美GPT-5等前沿专有模型。

Conclusion: 少量高质量合成信号可以有效替代大规模人工标注数据集，证明了智能体驱动合成框架在生成复杂推理训练数据方面的有效性，为大规模语言模型的复杂推理能力提升提供了可扩展的解决方案。

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [30] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 本文提出了一种受哲学和语言学启发的新型计算论证方法，引入了结构化双极论证框架，允许基于怀疑拒绝论证，并提供句子层面的语义扩展。


<details>
  <summary>Details</summary>
Motivation: 现有计算论证方法存在两个局限：1）要求接受所有被辩护的论证，不允许基于怀疑的理性拒绝；2）主要关注论证层面，而非更自然的句子/主张层面。本文旨在将哲学和语言学中的这些洞见融入计算模型。

Method: 首先定义结构化双极论证框架，其中论证由句子构成，包含攻击和支持关系。然后提出新的语义学，具有两个特征：1）不强制接受所有被辩护的论证；2）除了论证扩展外，还提供语言扩展来指定可接受的句子集合。

Result: 提出的语义位于抽象论证的可接受语义和完全语义之间，代表辩论中合理的立场。该方法为现有方法提供了新视角：可以指定忽略论证间支持的条件，并证明演绎支持语义是本文方法的特例。

Conclusion: 本文开发了一种更符合人类实际推理的计算论证方法，允许基于怀疑拒绝论证，并在句子层面提供语义，为计算论证领域提供了更灵活和自然的框架。

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [31] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora是一个平衡抽象与具体性的谐波记忆系统，通过主抽象索引具体记忆值和整合相关更新，同时使用线索锚点扩展检索访问，在记忆扩展时保持更好的检索相关性和推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统需要在容纳持续增长信息的同时支持高效、上下文感知的检索。抽象对于扩展记忆规模至关重要，但通常以牺牲具体性为代价，模糊了有效推理所需的细粒度细节。

Method: 引入Memora谐波记忆表示，通过主抽象索引具体记忆值并整合相关更新为统一记忆条目，使用线索锚点扩展跨不同方面的检索访问并连接相关记忆，采用主动利用这些记忆连接的检索策略来获取超越直接语义相似性的相关信息。

Result: 理论上证明标准检索增强生成(RAG)和知识图谱(KG)记忆系统是本框架的特例。在LoCoMo和LongMemEval基准测试中建立了新的最先进水平，在记忆扩展时展示了更好的检索相关性和推理有效性。

Conclusion: Memora通过谐波记忆表示结构性地平衡了抽象与具体性，解决了记忆扩展中的关键挑战，为智能体记忆系统提供了更有效的解决方案。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [32] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: 该研究提出了首个面向真实临床环境的障碍级精神病诊断基准MentalDx Bench，并开发了专门用于精神病诊断的LLM模型MentalSeek-Dx，通过监督轨迹构建和课程强化学习实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在精神病评估中虽有潜力，但缺乏生态效度和细粒度诊断监督的基准限制了其临床应用。需要建立真实临床环境下的障碍级精神病诊断基准来弥合这一差距。

Method: 1. 构建MentalDx Bench基准：包含712份去识别电子健康记录，由认证精神病医生根据ICD-11指南标注，涵盖16个诊断类别的76种障碍；2. 开发MentalSeek-Dx模型：通过监督轨迹构建和课程强化学习训练医学专用LLM，使其内化临床推理过程。

Result: 评估18个LLM发现存在范式错位：在粗粒度诊断分类上表现良好，但在障碍级诊断上系统性失败。MentalSeek-Dx仅用14B参数就在MentalDx Bench上实现了最先进的性能。

Conclusion: 研究揭示了LLM在精神病诊断中的范式错位问题，并提出了临床推理框架MentalSeek-Dx，为可靠的精神病诊断建立了临床基础。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [33] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: 提出Risk Awareness Injection (RAI)框架，通过构建不安全原型子空间和调制高风险视觉token，恢复VLM的LLM式风险识别能力，实现轻量级、无需训练的安全校准。


<details>
  <summary>Details</summary>
Motivation: 现有VLM对多模态越狱攻击高度脆弱，现有防御方法要么需要大量安全微调成本，要么通过激进token操作显著降低模型效用。研究发现LLM本身能识别文本不安全内容，但VLM中视觉输入会稀释风险信号。

Method: RAI从语言嵌入构建不安全原型子空间，对选定的高风险视觉token进行针对性调制，在跨模态特征空间中显式激活安全关键信号，恢复模型的LLM式风险检测能力，同时保持原始token的语义完整性。

Result: 在多个越狱攻击和效用基准测试上的广泛实验表明，RAI显著降低了攻击成功率，同时不损害任务性能。

Conclusion: RAI提供了一种轻量级、无需训练的安全校准框架，通过恢复VLM的LLM式风险识别能力，有效防御多模态越狱攻击，同时保持模型效用。

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [34] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于直觉模糊偏好的三支冲突分析模型，通过更细粒度的偏好表示来克服传统偏好模型中仅使用三种定性关系的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的冲突分析模型仅使用偏好、逆偏好和无关三种定性关系来描述智能体对议题对的态度，这种粗糙的表示方式严重限制了捕捉冲突本质的能力。

Method: 引入直觉模糊偏好冲突情境概念，建立直觉模糊偏好冲突度量框架，构建三支冲突分析模型来三分智能体对集合、智能体集合和议题集合，并基于相对损失函数计算阈值。

Result: 提出了基于调整机制的可行策略，同时考虑调整幅度和冲突程度，并提供了构建此类可行策略的算法，通过示例验证了模型的有效性。

Conclusion: 直觉模糊偏好冲突分析模型能够更精细地捕捉智能体态度，为冲突分析提供了更强大的理论框架和实用工具。

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [35] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM是一个训练大语言模型帮助用户形成和发现意图的新框架，通过模拟用户认知状态和意图层次结构，让模型学会在意图不明确时探索选项，在意图明确时细化实现，显著提升任务性能和对话效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理模糊和开放式请求时，通常只是询问用户澄清问题，但当用户自己也不知道想要什么时，这种方法就会失效。用户之所以模糊，往往是因为他们还没有形成明确的意图，需要通过观察和探索结果来发现自己真正想要的东西。

Method: 提出DiscoverLLM框架，核心是一个新颖的用户模拟器，该模拟器用层次化的意图结构来建模认知状态，意图随着模型提供相关选项而逐步具体化。意图具体化的程度作为奖励信号，模型可以被训练来优化这个信号。模型学会与用户协作，在意图不明确时自适应地发散探索选项，在意图具体化时收敛细化和实现。

Result: 在创意写作、技术写作和SVG绘图等交互基准测试中，DiscoverLLM实现了超过10%的任务性能提升，同时将对话长度减少了高达40%。在75名人类参与者的用户研究中，DiscoverLLM相比基线方法提高了对话满意度和效率。

Conclusion: DiscoverLLM提供了一种通用框架，训练大语言模型帮助用户形成和发现意图，通过建模用户认知状态和意图层次结构，使模型能够自适应地在探索和细化之间切换，显著提升了交互性能和用户体验。

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [36] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: 论文提出将本体编译为工具接口的机制，使大语言模型能够与形式化领域知识结合，通过可执行的本体语义约束指导LLM行为，减少手动模式工程


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型与形式化领域知识耦合的问题，通过本体到工具的编译机制，在生成过程中强制执行语义约束，而非事后验证

Method: 在The World Avatar框架中，将本体规范编译为可执行工具接口，结合MCP协议和智能体，实现本体感知工具的创建和应用，从非结构化科学文本中提取、验证和修复结构化知识

Result: 以金属有机多面体合成文献为例，展示了可执行本体语义如何指导LLM行为，减少手动模式和提示工程，为将形式化知识嵌入生成系统建立了通用范式

Conclusion: 本体到工具的编译机制为LLM与形式化领域知识的耦合提供了原理验证，建立了将语义约束嵌入生成系统的通用范式，减少了手动工程需求

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [37] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA框架通过理论边界和双评论家架构解决VLA模型持续强化学习中的稳定性-可塑性权衡问题


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，终身学习对具身智能体至关重要。持续强化学习（CRL）是将VLA模型部署到终身机器人场景中的有前景途径，但现有方法在平衡稳定性（保留旧技能）和可塑性（学习新技能）方面面临巨大挑战。

Method: 提出CRL-VLA框架，通过非对称调节机制：约束先前任务的优势幅度，同时允许新任务上的受控增长。采用简单有效的双评论家架构，包含新颖的目标条件价值公式（GCVF），其中冻结评论家锚定语义一致性，可训练估计器驱动适应。

Result: 在LIBERO基准测试中，CRL-VLA有效协调了这些冲突目标，在抗遗忘和向前适应方面均优于基线方法。

Conclusion: CRL-VLA为VLA模型的持续后训练提供了一个具有严格理论边界的框架，成功解决了持续强化学习中的稳定性-可塑性权衡问题，为终身机器人学习提供了有效解决方案。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [38] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 该研究探讨如何通过形式化抽象（移除和聚类）改进符号AI解释的可理解性，实验表明聚类提升理解，移除降低认知负荷


<details>
  <summary>Details</summary>
Motivation: 解释对人类认知至关重要，但AI系统常产生难以理解的输出。虽然符号AI为可解释性提供了透明基础，但原始逻辑追踪往往带来较高的外在认知负荷。研究者希望探索形式化抽象如何影响人类推理表现和认知努力。

Method: 使用答案集编程（ASP）作为形式框架，定义需要抽象的无关细节概念以获得简化解释。通过认知实验，让参与者在不同领域中基于答案集程序生成的解释对刺激进行分类。

Result: 认知实验显示，细节聚类显著改善参与者的理解，而细节移除显著降低认知努力，支持抽象能增强以人为中心的符号解释的假设。

Conclusion: 形式化抽象（特别是移除和聚类）能有效增强符号AI解释的人类可理解性，聚类提升理解质量，移除减少认知负荷，为构建更人性化的AI解释系统提供了实证支持。

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [39] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 论文发现现有LLM路由系统存在"路由崩溃"现象：随着成本预算增加，路由器会系统性地选择最强大但最昂贵的模型，即使更便宜的模型已经足够，导致小模型利用率不足。作者提出EquiRouter直接学习模型排名来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由系统存在一个普遍但未被充分探索的失败模式：随着用户成本预算增加，路由器会系统性地默认选择最强大、最昂贵的模型，即使更便宜的模型已经足够。这种现象被称为"路由崩溃"，导致小模型利用率不足，浪费计算资源和金钱成本，违背了路由的核心承诺。

Method: 作者提出EquiRouter，一种决策感知的路由器。现有路由器通常训练来预测标量性能分数，而路由决策最终依赖于候选模型之间的离散比较。EquiRouter直接学习模型排名，而不是标量分数，从而弥合了目标与决策之间的不匹配。

Result: 在RouterBench基准测试中，EquiRouter在达到GPT-4级别性能时，相比先前最强的路由器减少了约17%的成本。这表明EquiRouter能够有效恢复小模型的作用并缓解路由崩溃问题。

Conclusion: 路由崩溃是现有LLM路由系统的一个普遍问题，源于目标-决策不匹配。EquiRouter通过直接学习模型排名来解决这一问题，在保持高质量的同时显著降低成本，实现了更好的质量-成本权衡。

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [40] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 研究AI使用对人类文化演化的长期影响，发现替代型AI用户（AI-substitute）在个体层面占优但会减少文化多样性，而互补型AI用户（AI-complement）在群体选择中有利，能维持文化创新所需的变化。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛使用可能降低文化多样性和创新，导致模型崩溃和幻觉等问题。研究旨在探索AI使用对人类文化演化的长期后果，特别是"文化崩溃"的风险。

Method: 使用基于主体的建模和演化博弈论，比较两种AI使用策略：互补型（寻求建议但保持主要生产者角色）和替代型（最小输入，依赖AI产生大部分输出）。研究这些策略在演化动态中的竞争和传播。

Result: 替代型AI用户在个体层面选择中占优势，但会显著减少文化变化；互补型AI用户能维持探索所需的变化，在群体边界强时可通过文化群体选择获得优势。

Conclusion: AI使用对文化演化有深远影响，替代型策略可能主导个体层面但损害文化多样性，而互补型策略在群体层面有利。研究结果为制定缓解风险的策略提供了依据。

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [41] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: LLMs在静态医疗推理任务表现良好，但作为动态医疗世界模型模拟疾病进展和治疗结果时，在连续干预下难以保持一致的病人状态，导致长期模拟误差累积。研究者提出EHRWorld模型和EHRWorld-110K数据集来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 世界模型为干预下的未来状态模拟提供了原则性框架，但在复杂高风险的医疗领域实现这样的模型仍然具有挑战性。虽然LLMs在静态医疗推理任务上表现良好，但能否作为动态医疗世界模型来模拟疾病进展和治疗结果仍是一个开放性问题。

Method: 提出EHRWorld，一个基于因果序列范式训练的患者中心医疗世界模型，同时构建了EHRWorld-110K，一个从真实世界电子健康记录中提取的大规模纵向临床数据集。

Result: EHRWorld显著优于基于LLM的基线方法，实现了更稳定的长期模拟、对临床敏感事件的改进建模以及有利的推理效率。这表明基于因果基础、时间演化的临床数据进行训练对于可靠和稳健的医疗世界建模是必要的。

Conclusion: 仅依靠医学知识的LLMs难以在连续干预下保持一致的病人状态，导致长期临床模拟中的误差累积。通过因果序列范式训练的患者中心医疗世界模型EHRWorld能够显著提升医疗世界建模的性能和可靠性。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [42] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm：一种任务导向的动态通信算法，通过策略梯度优化多轮LLM多智能体系统中的动态通信拓扑，以适应角色变化和时变约束


<details>
  <summary>Details</summary>
Motivation: 现有多轮LLM多智能体系统使用固定的通信拓扑结构，无法适应现实应用中智能体角色可能因动态对抗、任务进展或时变约束（如通信带宽）而跨轮次变化的需求

Method: 提出TodyComm算法，通过策略梯度优化生成行为驱动的协作拓扑，使通信结构能够适应每轮的动态变化，从而优化任务效用

Result: 在五个基准测试上的实验表明，在动态对抗和通信预算约束下，TodyComm在任务有效性方面表现优异，同时保持了令牌效率和可扩展性

Conclusion: TodyComm通过动态调整通信拓扑有效解决了多轮LLM多智能体系统中角色变化的挑战，为现实应用提供了更灵活的协作框架

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [43] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra是一个基于统一智能体抽象（指令、上下文、工具、模型四元组）的编排系统，通过动态创建专用执行器来解决复杂长时任务，实现了框架无关的智能体协同


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体系统在解决复杂长时任务时缺乏动态抽象视图，导致适应性不足。子智能体作为工具的范式虽然有所发展，但仍需要更灵活的智能体抽象来提升系统适应性

Method: 提出统一的框架无关智能体抽象：将任何智能体建模为（指令、上下文、工具、模型）四元组。基于此构建AOrchestra系统，其中编排器在每一步具体化该四元组：策划任务相关上下文、选择工具和模型，通过即时自动创建智能体来委托执行

Result: 在三个具有挑战性的基准测试（GAIA、SWE-Bench、Terminal-Bench）中，AOrchestra与Gemini-3-Flash配对时，相对于最强基线实现了16.28%的相对改进。系统实现了可控的性能-成本权衡，接近帕累托效率

Conclusion: AOrchestra通过统一的智能体抽象和动态编排机制，显著提升了语言智能体在复杂长时任务中的适应性，减少了人工工程工作量，并支持框架无关的即插即用智能体集成

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [44] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 研究发现LLM多智能体系统中，单纯增加同质智能体数量存在收益递减，而异质智能体（不同模型、提示词或工具）能持续带来显著性能提升。研究提出信息论框架，证明系统性能受限于任务固有不确定性而非智能体数量，并引入有效通道数K*量化异质性。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统在处理复杂任务方面显示出潜力，但实践中发现单纯增加同质智能体数量存在收益递减现象，而异质智能体配置却能持续提升性能。这引发了一个根本问题：什么限制了规模扩展？为什么多样性有帮助？

Method: 提出信息论框架分析多智能体系统性能边界，证明性能受限于任务固有不确定性而非智能体数量。引入有效通道数K*量化系统访问的有效通道数量，无需真实标签即可评估异质性。通过实验验证异质配置优于同质扩展。

Result: 实验表明异质配置始终优于同质扩展：2个异质智能体的性能可匹配甚至超过16个同质智能体。研究提供了基于多样性的高效稳健多智能体系统设计原则。

Conclusion: 多智能体系统性能受任务固有不确定性限制而非智能体数量，同质智能体因输出高度相关而早期饱和，异质智能体提供互补证据。通过多样性感知设计可构建更高效稳健的多智能体系统。

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [45] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 本文提出了一种基于风险控制的自适应推理框架，通过上下阈值机制动态调整LLM推理的token预算，在保证错误率不超目标风险的前提下最小化计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时存在计算成本与准确率的权衡问题：增加token预算可以提高准确率，但计算成本也随之增加。现有方法难以在保证特定错误率限制的同时最小化计算开销，需要一种系统化的风险控制方法。

Method: 提出基于风险控制的框架，包含两个阈值：上阈值在模型置信度高时停止推理（可能输出错误结果），下阈值在问题不可解时提前停止（可能过早停止）。使用无分布风险控制方法，根据目标风险和验证集优化设置这些停止机制。对于多预算控制标准的情况，引入效率损失函数选择最计算高效的退出机制。

Result: 在多种推理任务和模型上的实验结果表明，该风险控制方法有效实现了计算效率提升，下阈值和集成停止机制都能显著减少计算开销，同时严格遵守用户指定的风险目标。

Conclusion: 该框架为LLM推理提供了实用的风险控制解决方案，能够在保证特定错误率限制的同时最小化计算成本，实现了计算效率与可靠性的平衡。

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [46] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench是首个大规模科学插图生成基准，包含3300个高质量文本-插图对；AutoFigure是首个基于长文本自动生成高质量科学插图的智能体框架，通过思考、重组和验证实现结构完整且美观的插图生成。


<details>
  <summary>Details</summary>
Motivation: 高质量科学插图对于有效传达复杂科技概念至关重要，但手动创建插图在学术界和工业界都是公认的瓶颈，需要自动化解决方案来提升效率。

Method: 提出了FigureBench基准数据集（3300个高质量文本-插图对）和AutoFigure智能体框架，该框架在最终渲染前通过广泛的思考、重组和验证来生成结构合理且美观的布局。

Result: 实验结果表明，AutoFigure在所有基线方法中表现最优，能够生成可直接用于发表的科学插图，代码、数据集和HuggingFace空间已开源。

Conclusion: FigureBench为科学插图生成提供了首个大规模基准，AutoFigure框架通过智能体方法成功实现了从长文本自动生成高质量科学插图的目标，解决了手动创建的瓶颈问题。

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>
