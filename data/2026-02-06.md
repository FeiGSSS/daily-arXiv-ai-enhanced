<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架显著提升大语言模型在规划任务中的推理能力，特别是在符号化任务上准确率从31.5%提升至97.3%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理和规划任务上存在不足，现有提示技术如思维链也受到质疑。研究者希望探索认知科学中的任务-方法-知识框架是否能改善LLM的推理能力，特别是其独特的因果、目的论和层次化推理结构

Method: 采用任务-方法-知识框架构建提示，在PlanBench基准的Blocksworld领域进行实验，测试TMK结构化提示是否能帮助语言模型更好地将复杂规划问题分解为可管理的子任务

Result: TMK提示显著提升了推理模型的性能，在不透明的符号化任务上准确率从31.5%提升至97.3%，表现出明显的性能反转，表明TMK能引导模型从默认语言模式转向形式化代码执行路径

Conclusion: TMK框架不仅能提供上下文，还能作为机制引导推理模型远离默认语言模式，激活形式化代码执行路径，有助于弥合语义近似与符号操作之间的差距

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [2] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构造方法，通过结合程序执行反馈和LLM的思维链能力，提升数学推理的准确性和可修正性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理方面仍缺乏可靠可修正的推理过程表示，要么采用僵化的顺序流程无法修正早期错误，要么依赖启发式自评估可能无法识别和修复错误，同时程序化上下文可能分散语言模型的注意力并降低准确性。

Method: IIPC（迭代改进的程序构造）方法迭代地精炼程序化推理链，将程序执行反馈与基础LLM的原生思维链能力相结合，以保持高层次上下文关注。

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法。

Conclusion: IIPC通过迭代改进程序构造，结合执行反馈和思维链能力，有效提升了数学推理的可靠性和准确性，所有代码和实现已开源发布。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [3] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk框架将多智能体系统的动态蒸馏到单个模型的权重中，将显式的测试时交互转化为隐式的模型能力，使单个智能体具备多智能体系统的智能同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多智能体系统通过迭代辩论实现了优越的推理性能，但实际部署受到高计算成本和错误传播的限制。需要一种方法既能保留多智能体系统的优势，又能降低计算开销。

Method: 提出AgentArk框架，采用三种分层蒸馏策略：推理增强微调、基于轨迹的增强和过程感知蒸馏。这些策略将多智能体动态蒸馏到单个模型的权重中，将计算负担从推理阶段转移到训练阶段。

Result: 蒸馏后的模型在保持单个智能体效率的同时，展现出多智能体的强大推理和自我纠正性能。这些模型在多样化的推理任务中表现出增强的鲁棒性和泛化能力。

Conclusion: AgentArk框架通过将多智能体动态蒸馏到单个模型中，实现了高效且鲁棒的多智能体开发，为未来高效多智能体系统研究提供了新思路。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [4] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出了一种状态级选择性验证框架，在验证成本受限的设置下，通过智能分配验证资源来提高大语言模型推理效率，相比传统方法减少44%验证调用。


<details>
  <summary>Details</summary>
Motivation: 测试时计算已成为大语言模型推理进展的主要驱动力，但昂贵的验证过程成为瓶颈。许多推理系统中，大量验证调用被浪费在冗余或无希望的中间假设上。研究在验证成本受限设置下的推理问题，探索如何在中间状态间分配验证努力。

Method: 提出状态级选择性验证框架，包含三个核心组件：(1) 结构化移动接口上的确定性可行性门控；(2) 使用学习的状态距离和残差评分混合的预验证排序；(3) 基于局部不确定性的自适应验证调用分配。与解决方案级的最佳N选择或均匀中间验证不同，该方法将验证资源分配到信息量最大的地方。

Result: 在MATH基准测试上，该方法相比最佳N选择、多数投票和波束搜索实现了更高的准确率，同时减少了44%的验证调用。

Conclusion: 通过智能分配验证资源的状态级选择性验证框架，能够在验证成本受限的设置下显著提高大语言模型推理效率，为推理系统设计提供了新的优化方向。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [5] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 研究发现，在强化学习可验证奖励（RLVR）训练中，思维链（CoT）的可监控性提升并非普遍现象，而是高度依赖于数据多样性和指令遵循数据，且与模型能力正交。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型（LRMs）的部署增加，审计其思维链（CoT）轨迹的安全性变得至关重要。先前研究发现，在强化学习可验证奖励（RLVR）的早期阶段，可监控性（CoT忠实且信息丰富地反映内部计算的程度）可能表现为"免费赠品"，本研究旨在系统验证这一现象。

Method: 通过跨模型家族和训练领域的系统评估，研究RLVR训练中可监控性的变化。采用机制分析，重点关注响应分布锐化（熵减少）和提示关注度的变化，而非推理轨迹的因果依赖。同时控制训练和评估难度来观察可监控性动态变化。

Result: 可监控性提升并非普遍现象，而是强烈依赖于数据特性，特别是数据多样性和指令遵循数据。可监控性与模型能力正交，推理性能提升并不必然增加透明度。机制分析显示，可监控性增益主要来自响应分布锐化和对提示关注度的增加。

Conclusion: 研究提供了RLVR下可监控性出现的整体视图，阐明了何时可能获得可监控性增益以及何时不会。强调了数据质量（特别是多样性和指令遵循）在实现可监控性中的关键作用，并指出可监控性与模型能力是独立维度。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [6] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 本文提出对抗性解释攻击(AEAs)，通过操纵LLM生成解释的框架来调节用户对错误输出的信任，揭示了AI与用户之间认知通道的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统越来越多地在人类决策循环中运行，用户依赖模型推荐并解释其输出。LLM生成的自然语言解释会影响用户对AI输出的感知和信任，这揭示了一个新的攻击面：AI与用户之间的认知通信通道。现有对抗威胁主要针对模型计算行为，而非依赖模型的用户。

Method: 提出对抗性解释攻击(AEAs)框架，通过操纵解释的四个维度来调节人类信任：推理模式、证据类型、沟通风格和呈现格式。引入信任校准差距指标来衡量在对抗性解释下正确与错误输出之间的人类信任差异。通过受控实验(n=205)系统性地测试不同解释框架对用户信任的影响。

Result: 用户对对抗性和良性解释报告几乎相同的信任度，对抗性解释在错误情况下仍能保留大部分良性信任。最脆弱的场景是当AEAs模仿专家沟通风格时，结合权威证据、中性语气和领域适当的推理。在困难任务、事实驱动领域以及教育程度较低、较年轻或高度信任AI的参与者中，脆弱性最高。

Conclusion: 这是首个将解释视为对抗性认知通道并量化其对AI辅助决策中人类信任影响的系统性安全研究。研究表明，通过操纵解释框架可以显著影响用户对错误AI输出的信任，揭示了LLM解释作为认知攻击面的安全威胁。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [7] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文提出一个反事实解释的axiomatic框架，通过公理体系定义了五种不同类型的反事实解释，包括局部和全局解释，并建立了公理子集与解释器家族的一一对应关系。


<details>
  <summary>Details</summary>
Motivation: 当前反事实解释研究存在两个主要问题：1) 大多数解释器只关注单一类型的反事实；2) 主要局限于局部解释（针对单个实例），缺乏对全局解释（揭示系统整体推理过程）的系统研究。

Method: 构建基于一组理想性质（公理）的反事实解释器axiomatic框架，证明不可能性定理，建立表示定理，揭示五种不同类型的反事实解释与特定公理子集的一一对应关系。

Result: 发现了五种根本不同类型的反事实解释，其中一些对应局部解释，另一些对应全局解释；建立了现有解释器在该分类体系中的位置，形式化描述了它们的行为，并分析了生成此类解释的计算复杂度。

Conclusion: 该框架填补了反事实解释研究的两个重要空白，为理解和设计不同类型的反事实解释提供了系统化的理论基础，有助于提高对自主智能系统决策的信任。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [8] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: ORBIT框架通过多任务多回合元强化学习训练LLMs，使其能够在上下文中从交互中学习，显著提升了未见环境中的在线学习能力，性能媲美GPT-5.2并大幅超越标准RL微调。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在静态预测和指令跟随任务中表现良好，但在需要实时交互、延迟反馈、平衡信息收集与利用的在线决策任务中表现不佳。虽然上下文学习允许无需权重更新的适应，但现有LLMs难以可靠利用上下文交互经验。

Method: 提出ORBIT框架：一个多任务、多回合的元强化学习框架，专门训练LLMs在上下文中从交互中学习。该框架通过元训练使模型能够适应未见环境。

Result: 经过元训练后，相对较小的开源模型(Qwen3-14B)在完全未见环境中表现出显著改进的上下文在线学习能力，性能与GPT-5.2相当，并大幅超越标准RL微调方法。扩展实验显示模型规模越大性能提升越一致。

Conclusion: 通过专门训练，LLMs可以在上下文中有效学习交互经验，显著提升在线决策能力。模型规模扩展带来持续增益，表明学习型推理决策智能体仍有很大发展空间。

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [9] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将现代LLM应用视为上下文构建与执行问题的系统，通过异构DNN堆栈、上下文构建层和动作层，结合小型模型和工具链处理复杂任务，仅将精炼上下文传递给用户选择的LLM生成最终响应。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖单一大型模型处理所有任务，计算成本高且不够灵活。Interfaze旨在通过将LLM应用重新定义为上下文构建与执行问题，结合小型专用模型和工具链，在保持竞争力的同时将计算负担从昂贵的大型模型中转移出来。

Method: 系统包含三个核心组件：(1) 异构DNN堆栈与小型语言模型作为感知模块，处理复杂PDF、图表、多语言ASR等；(2) 上下文构建层，爬取、索引和解析外部资源为结构化状态；(3) 动作层，支持浏览、检索、沙箱代码执行和驱动无头浏览器。顶层控制器决定运行哪些小型模型和动作，并将精炼上下文传递给用户选择的LLM生成最终响应。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%。多模态任务上：MMMU(val) 77.3%、AI2D 91.5%、ChartQA 90.9%、Common Voice v16 90.8%。大多数查询主要由小型模型和工具链处理，大型LLM仅操作精炼上下文，实现了竞争性准确率同时将主要计算从最昂贵的模型中转移。

Conclusion: Interfaze展示了通过将LLM应用重新定义为上下文构建与执行问题，结合小型专用模型和工具链，可以在保持高性能的同时显著降低对昂贵大型模型的依赖，为更高效、可扩展的LLM应用架构提供了新方向。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [10] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: OMG-Agent是一个新型的多模态生成框架，通过模拟"思考-行动"的认知过程，将任务解耦为语义规划、证据检索和执行合成三个阶段，解决了现有方法中的语义-细节纠缠问题，显著提升了数据不完整情况下的生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态系统面临数据不完整性的严重挑战。传统参数化/生成模型容易产生幻觉，检索增强框架存在检索僵化问题，且端到端架构受限于语义-细节纠缠的结构性冲突，影响了生成保真度。

Method: 提出OMG-Agent框架，采用动态粗到细的智能体工作流程：1) MLLM驱动的语义规划器通过渐进上下文推理解决输入歧义；2) 非参数化证据检索器将抽象语义锚定到外部知识；3) 检索注入执行器利用检索证据作为灵活特征提示来合成高保真细节。

Result: 在多个基准测试中，OMG-Agent始终优于最先进方法，在极端缺失情况下保持鲁棒性，如在CMU-MOSI数据集上70%缺失率时获得2.6分的提升。

Conclusion: OMG-Agent通过解耦语义规划和细节合成，有效解决了语义-细节纠缠问题，为数据不完整情况下的多模态生成提供了更可靠的解决方案。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [11] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 论文提出可扩展交互监督框架，通过将复杂意图分解为递归决策树来增强人类监督，使非专家能生成专家级产品需求文档，对齐度提升54%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在复杂长时任务（如vibe coding）中日益自动化，出现了监督缺口。模型擅长执行，但用户因领域专业知识不足、难以精确表达意图、无法可靠验证复杂输出而难以有效指导模型，这构成了可扩展监督的关键挑战。

Method: 提出可扩展交互监督框架，将复杂意图分解为递归决策树，在节点处获取低负担反馈，递归聚合这些信号形成精确全局指导，而非依赖开放式提示。框架可通过仅使用在线用户反馈的强化学习进行优化。

Result: 在网页开发任务中验证，该框架使非专家能生成专家级产品需求文档，实现了54%的对齐度提升。证明框架可通过仅使用在线用户反馈的强化学习进行优化。

Conclusion: 可扩展交互监督框架为解决AI规模化过程中的人类控制问题提供了实用途径，通过递归分解复杂意图和低负担反馈机制，使人类能有效监督超越自身能力的AI任务。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [12] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: Agent-Omit：一个训练框架，让LLM智能体能够自适应地省略冗余的思考和观察，在保持性能的同时提升效率


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理多轮智能体-环境交互时，将整个交互轨迹同等对待，忽视了不同轮次中思考必要性和观察效用的差异。这导致智能体效率低下，存在冗余的思考和观察。

Method: 1. 首先定量研究思考和观察如何影响智能体的效果和效率；2. 提出Agent-Omit框架，包括：a) 合成少量冷启动数据（单轮和多轮省略场景）微调智能体；b) 引入省略感知的智能体强化学习方法，包含双重采样机制和定制的省略奖励。

Result: 在五个智能体基准测试中，构建的Agent-Omit-8B模型性能可与七个前沿LLM智能体相媲美，并且在效果-效率权衡方面优于七个高效LLM智能体方法。理论证明省略策略的偏差受KL散度上界约束。

Conclusion: Agent-Omit框架通过自适应省略冗余思考和观察，有效提升了LLM智能体的效率，同时保持了性能，实现了更好的效果-效率平衡。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [13] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架将LLM推理中的隐含假设转化为结构化决策树，通过评估场景可能性、目标收益和执行成本来指导行动选择，减少多智能体环境中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 在多智能体、部分可观察、去中心化环境中，智能体需要处理隐藏对象和合作者意图的不确定性。现有LLM方法主要依赖频繁的智能体间通信来缓解不确定性，但这会产生高昂的token和时间成本，并可能破坏涉及人类伙伴的工作流程。

Method: 提出PCE（Planner-Composer-Evaluator）框架：1）将LLM推理轨迹中的碎片化假设转化为结构化决策树；2）内部节点编码环境假设，叶子节点映射到行动；3）通过评估场景可能性、目标导向收益和执行成本对每条路径进行评分，指导理性行动选择。

Result: 在两个多智能体基准测试（C-WAH和TDW-MAT）和三个不同LLM骨干上，PCE在成功率和任务效率方面持续优于以通信为中心的基线方法，同时保持相当的token使用量。消融实验表明PCE的性能提升在不同模型容量和推理深度下都保持有效。用户研究显示人类伙伴认为PCE产生的通信模式更高效和可信。

Conclusion: PCE框架为将LLM的隐含假设转化为可靠的不确定性感知规划策略提供了原则性途径，结构化不确定性处理与模型容量和推理深度扩展形成互补，能够减少通信开销同时提高规划质量。

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [14] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出了一种模块化、可互操作的解决方案，通过最小化配置和解耦数字孪生与AI组件角色，实现AI管道在信息物理系统中的无缝集成，并在微工厂场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 工业领域信息物理系统日益复杂，物联网和工业物联网技术的碎片化（表现为多样的通信协议、数据格式和设备能力）在底层物理层与高层智能功能之间造成了巨大鸿沟。现有数字孪生方法往往孤立且紧耦合，限制了AI功能的可扩展性和重用性。

Method: 提出模块化、可互操作的解决方案，引入零配置AI管道概念，其中数字孪生负责数据管理和智能增强，最小化配置需求并解耦数字孪生与AI组件的角色。

Result: 在微工厂场景中验证了该方法，展示了其对并发机器学习模型和动态数据处理的支持，有效加速了复杂工业环境中智能服务的部署。

Conclusion: 该解决方案通过零配置AI管道和角色解耦，解决了信息物理系统中AI集成面临的碎片化和紧耦合问题，为复杂工业环境中的智能服务部署提供了有效途径。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [15] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构动态分配计算资源，在专家级科学推理任务上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在专家级科学推理（如Humanity's Last Exam基准）上存在挑战，传统工具管道僵化、多智能体协调脆弱、测试时扩展效率低，限制了性能提升

Method: 提出ReThinker框架：1) Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算；2) 自适应工具调用、引导式多维度反思、鲁棒的置信度加权选择；3) 无需人工标注的反向数据合成管道和自适应轨迹回收策略，将成功推理轨迹转化为高质量监督数据

Result: 在HLE、GAIA和XBench基准测试中，ReThinker一致性地超越了现有最先进的基础模型（带工具）和深度研究系统，在专家级推理任务上取得了最先进的结果

Conclusion: ReThinker通过置信度感知的动态计算分配和创新的无监督训练策略，有效解决了专家级科学推理中的关键挑战，为智能体框架设计提供了新思路

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [16] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 研究提出一个框架来解决生成式AI系统与问答论坛之间的悖论关系：AI系统依赖论坛数据提升性能，但同时又分流用户。通过顺序交互框架和真实数据模拟，展示了激励错配问题，但证明双方仍能获得理想情境下约一半的效用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统从问答论坛获取数据来提升性能，但同时又将用户从这些论坛分流出去，形成了依赖与竞争并存的悖论关系。需要探索AI系统与人类知识平台之间可持续的协作模式。

Method: 提出顺序交互框架，让生成式AI系统向论坛提出问题，论坛可以选择发布部分问题。该框架考虑了非货币交换、信息不对称和激励错配等复杂因素。使用真实的Stack Exchange数据和常用LLM进行全面的数据驱动模拟。

Result: 实证展示了激励错配问题，但证明参与方仍能获得理想完全信息情境下大约一半的效用。模拟结果表明存在可持续协作的潜力。

Conclusion: 研究展示了生成式AI系统与人类知识平台之间可持续协作的可能性，能够保持有效的知识共享，解决AI依赖论坛数据却又分流用户的悖论问题。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [17] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 论文提出Vibe AIGC新范式，通过智能体编排解决当前生成式AI的意图-执行差距问题，将用户从提示工程师转变为提供高层"氛围"的指挥官，由元规划器分解为可执行的智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI存在"意图-执行差距"问题，即用户的高层意图与模型的随机黑盒性质之间存在根本性差异。尽管视觉保真度显著提升，但模型中心范式遇到了"可用性天花板"，限制了复杂数字资产的创作。

Method: 提出Vibe AIGC范式，受Vibe Coding启发，通过智能体编排实现内容生成。用户作为指挥官提供高层"氛围"（包含审美偏好、功能逻辑等），中央元规划器作为系统架构师，将氛围分解为可执行、可验证、自适应的智能体管道。

Result: 该范式从随机推理转向逻辑编排，弥合了人类想象力与机器执行之间的差距，将AI从脆弱的推理引擎转变为强大的系统工程伙伴。

Conclusion: Vibe AIGC将重新定义人机协作经济，民主化复杂、长视野数字资产的创作，代表了生成式AI从模型中心范式向智能体编排范式的根本性转变。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [18] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: WideSeek-R1是一个通过多智能体强化学习训练的领导-子智能体框架，通过宽度扩展（增加并行子智能体）而非深度扩展（增加单个智能体能力）来解决广泛信息搜索任务，4B参数模型在WideSearch基准上达到40.0%的F1分数，性能可与671B参数的单个智能体模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 随着任务范围扩大，关键瓶颈从个体能力转向组织能力。现有多智能体系统依赖手工设计的工作流程和轮流交互，无法有效并行化工作。需要探索宽度扩展的补充维度来解决广泛信息搜索问题。

Method: 提出WideSeek-R1框架：领导智能体-子智能体架构，通过多智能体强化学习训练，利用共享LLM但隔离上下文和专用工具，在20k个广泛信息搜索任务的精选数据集上联合优化领导智能体和并行子智能体。

Result: WideSeek-R1-4B在WideSearch基准上达到40.0%的F1分数，性能与单智能体DeepSeek-R1-671B相当。随着并行子智能体数量增加，性能持续提升，证明了宽度扩展的有效性。

Conclusion: 宽度扩展是多智能体系统解决广泛信息搜索任务的有效方法，WideSeek-R1框架通过并行执行和可扩展编排实现了与深度扩展相当的性能，但参数规模小得多。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [19] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 该论文提出了一个七维分类法来系统评估医疗领域LLM智能体的能力，分析了49项研究，揭示了能力实现的不均衡性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM智能体的研究缺乏统一的分析框架，现有文献要么是宽泛的综述，要么只关注单一能力，无法为医疗工作提供共同参考框架。

Method: 开发了包含7个维度（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务）和29个子维度的分类法，对49项研究进行系统评估，使用明确的标准和标签（完全实现、部分实现、未实现）。

Result: 分析揭示了明显的不对称性：外部知识整合常见（76%完全实现），而事件触发激活（92%未实现）和漂移检测与缓解（98%未实现）罕见；多智能体设计是主导模式（82%完全实现）；信息中心能力领先，但治疗规划等行动导向领域仍有较大差距（59%未实现）。

Conclusion: 该分类法为医疗LLM智能体研究提供了系统分析框架，揭示了当前能力实现的分布模式，指出了未来需要重点发展的方向，特别是行动导向和自适应能力。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [20] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 本文反驳了METR报告中关于AI能力呈指数增长的论点，认为数据不支持这一结论，并提出了更复杂的模型表明AI能力拐点已经或即将到来。


<details>
  <summary>Details</summary>
Motivation: 针对METR报告声称AI能力自2019年以来呈指数增长的观点，本文旨在质疑这种预测的可靠性，指出现有数据并不支持指数增长假设，强调现有预测的脆弱性。

Method: 1) 对METR现有数据拟合S型曲线，发现拐点已经过去；2) 提出更复杂的分解模型，将AI能力分为基础和推理能力，分别考察其改进速率；3) 通过模型证明AI能力将在近期出现拐点。

Result: 分析显示：1) METR数据拟合的S型曲线拐点已经过去，而非遥远的未来；2) 提出的分解模型支持AI能力将在近期出现拐点的假设；3) 现有关于AI指数增长的预测缺乏数据支持。

Conclusion: AI能力并未呈现指数增长，现有预测过于乐观且脆弱。本文旨在警示对AI能力增长的过度预测，而非建立自己的严格预测模型。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [21] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 该研究对QwQ-32B模型的推理机制进行了解剖，发现其在处理抽象结构信息时会动态优化内部表征，形成专注于结构而非具体动作名称的抽象编码，这种"流体推理表征"是推理模型性能提升的关键因素之一。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在抽象问题上表现优异，但其内部机制尚不明确。研究者希望理解QwQ-32B这类专门训练用于生成扩展推理轨迹的模型如何处理抽象结构信息，揭示其超越非推理模型的内部工作机制。

Method: 在语义混淆的规划领域Mystery Blocksworld上，对QwQ-32B进行机制分析。通过转向实验建立因果关系：从成功轨迹中注入精炼表征来提升准确性，同时用符号表征替换混淆编码来验证机制。分析模型在推理过程中内部表征的动态变化。

Result: 研究发现QwQ-32B在推理过程中逐步改进对动作和概念的内部表征，发展出专注于结构而非具体动作名称的抽象编码。转向实验提供了因果证据：注入精炼表征能提升问题解决准确性，而符号表征替换混淆编码仅导致最小性能损失。

Conclusion: 推理模型性能的关键驱动因素之一是上下文中的表征精炼过程，即"流体推理表征"。模型在推理过程中动态优化内部表征，形成抽象结构编码，这种适应性改进是推理能力提升的重要机制。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>
