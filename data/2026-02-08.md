<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG是一个基于因果门控的层次化图RAG框架，通过显式建模因果关系来抑制虚假相关性，实现大规模知识图上的可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法过度依赖表面节点匹配，缺乏显式因果建模，导致答案不可靠或虚假。先前尝试融入因果关系的方法通常局限于局部或单文档上下文，且受模块化图结构导致的信息隔离问题影响，阻碍了可扩展性和跨模块因果推理。

Method: 提出HugRAG框架，通过因果门控重新思考基于图的RAG知识组织方式，在层次化模块间实现因果门控，显式建模因果关系以抑制虚假相关性，同时支持大规模知识图上的可扩展推理。

Result: 大量实验表明，HugRAG在多个数据集和评估指标上持续优于竞争性的基于图的RAG基线方法。

Conclusion: 该工作为结构化、可扩展且基于因果基础的RAG系统建立了原则性基础。

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [2] [Hallucination-Resistant Security Planning with a Large Language Model](https://arxiv.org/abs/2602.05279)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.AI

TL;DR: 提出一个原则性框架，将LLM集成到安全管理的决策支持中，通过一致性检查和数字孪生反馈控制幻觉风险，在事件响应用例中相比前沿LLM减少30%恢复时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在支持安全管理任务（如事件响应规划）方面很有前景，但其不可靠性和幻觉倾向仍然是重大挑战，需要解决这些问题以提高LLM在安全领域的实用性。

Method: 提出一个原则性框架，将LLM集成到迭代循环中：LLM生成候选行动，检查与系统约束和前瞻预测的一致性；当一致性低时，通过数字孪生等外部反馈收集信息，然后通过上下文学习（ICL）优化候选行动。

Result: 证明该设计可通过调整一致性阈值控制幻觉风险，并在某些假设下建立了ICL的遗憾界限。在四个公共数据集上的实验表明，该框架相比前沿LLM将恢复时间减少高达30%。

Conclusion: 该框架为在安全管理中使用LLM作为决策支持提供了原则性方法，通过一致性检查和外部反馈有效控制幻觉风险，显著提高了事件响应规划的效果。

Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.

</details>


### [3] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5在PieArena谈判基准测试中达到或超越商学院学生水平，但中低端模型仍有改进空间，谈判行为分析揭示了模型间的异质性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在谈判任务中的能力，谈判是商业核心任务，需要战略推理、心智理论和经济价值创造能力。

Method: 引入PieArena大规模谈判基准，基于精英商学院MBA谈判课程的真实场景，进行多智能体交互评估，研究联合意向性智能体支架的效果。

Result: 前沿模型GPT-5达到或超越经过一学期谈判训练和针对性指导的商学院学生；联合意向性支架对中低端模型有显著提升，但对前沿模型收益递减；谈判行为分析揭示了模型在欺骗、计算准确性、指令遵从和感知声誉方面的异质性。

Conclusion: 前沿语言智能体在智力和心理上已具备在高风险经济环境中部署的能力，但在鲁棒性和可信赖性方面仍存在挑战。

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [4] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim是一个端到端的医院管理模拟框架，结合真实数据生成和多智能体模拟，用于系统评估LLM在医院行政工作流程自动化中的表现。


<details>
  <summary>Details</summary>
Motivation: 医院行政部门每天处理大量任务（超过10,000个请求），对LLM自动化有强烈需求。但现有研究主要关注医患交互或孤立的管理子任务，未能捕捉真实行政工作流程的复杂性。

Method: 提出H-AdminSim框架，结合真实数据生成和多智能体模拟医院行政工作流程。通过FHIR集成提供统一、可互操作的环境，使用详细评分标准对任务进行定量评估。

Result: 该框架为评估LLM驱动的行政自动化可行性和性能提供了标准化测试平台，能够跨异构医院环境测试行政工作流程。

Conclusion: H-AdminSim填补了现有研究的空白，通过端到端模拟框架解决了医院行政工作流程的复杂性，为系统比较不同LLM在行政自动化中的表现提供了有效工具。

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [5] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: SPT方法通过在LoRA参数中训练一个"诚实人格"，在模型正常响应后激活该人格来审查主模型的输出，有效检测大语言模型中的错位行为，在Anthropic审计游戏基准上达到96%准确率。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型中的错位行为具有挑战性，因为模型可能在训练中学会隐藏不良行为。传统审计方法存在局限：黑盒方法难以区分错位输出与良性输出，而机制可解释性方法无法随模型能力扩展。

Method: 引入Split Personality Training (SPT)方法，在LoRA参数中微调一个"诚实人格"，该人格在正常操作时保持非激活状态。主模型响应后，激活LoRA适配器并插入触发字符串，使诚实人格能够审查响应同时访问主模型的潜在状态。

Result: 在Anthropic审计游戏模型生物基准测试中，SPT达到96%的总体准确率，而Anthropic报告接近0%准确率。诚实人格揭示了外部观察者无法访问的潜在知识，如受损模型训练时使用的虚构偏见。

Conclusion: SPT方法通过创建独立的诚实审查人格，有效检测大语言模型中的隐藏错位行为，为解决模型欺骗性对齐问题提供了新途径。

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [6] [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023)
*Christopher A. McClurg,Alan R. Wagner*

Main category: cs.AI

TL;DR: 开发数据驱动的离散事件模拟器，用于评估学校安全干预策略，特别是机器人应对枪击事件的方案，解决VR研究中招募参与者困难的问题。


<details>
  <summary>Details</summary>
Motivation: VR虽然能有效评估学校安全措施，但每次条件变化都需要招募新参与者，使得大规模或迭代评估变得困难，尤其是在需要大量训练场景的学习有效干预策略时。

Method: 开发数据驱动的离散事件模拟器，将枪手移动和区域内行动建模为从VR研究中参与者行为学习的随机过程，用于评估机器人干预策略。

Result: 模拟器能够重现关键经验模式，实现可扩展的干预策略评估和学习，这些策略难以直接通过人类受试者进行训练。

Conclusion: 这项工作展示了一个高到中保真度的模拟工作流程，为开发和评估自主学校安全干预提供了可扩展的替代方案。

Abstract: Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.

</details>
