<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样减少rollout模型与策略分布不匹配，实现LLM强化学习中rollout生成与策略优化的高效解耦


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习成本高昂，主要因为rollout生成过程昂贵。将rollout生成与策略优化解耦（如使用更高效模型进行rollout）可能显著提升效率，但这会引入严重的分布不匹配问题，导致学习不稳定。

Method: 提出Jackpot框架，采用最优预算拒绝采样直接减少rollout模型与演化策略之间的分布差异。框架包括：1）原则性的OBRS过程；2）联合更新策略和rollout模型的统一训练目标；3）通过top-k概率估计和批次级偏差校正实现的高效系统实现。

Result: 理论分析表明OBRS在可控接受预算下持续使rollout分布更接近目标分布。实验显示相比重要性采样基线，Jackpot显著提升训练稳定性，在Qwen3-8B-Base模型上训练300步（批次大小64）时达到与在线RL相当的性能。

Conclusion: 基于OBRS的对齐方法使RL for LLMs中rollout生成与策略优化的实际有效解耦更近一步，为实现实用高效的LLM强化学习提供了有前景的路径。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [2] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文首次对大型语言模型（LLMs）的推理失败进行了全面调查，提出了新的分类框架，将推理分为具身和非具身类型，并将推理失败分为三类：架构固有的基础性失败、特定领域的应用限制以及鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种任务中表现出卓越的推理能力，但在看似简单的场景中仍然存在显著的推理失败。为了系统性地理解和解决这些缺陷，需要对这些失败进行全面的调查和分类。

Method: 提出了一个新颖的分类框架：将推理分为具身推理和非具身推理，非具身推理进一步细分为非正式（直觉）推理和正式（逻辑）推理。同时，将推理失败分为三类：影响下游任务的基础性架构失败、特定领域表现的应用限制、以及跨微小变化表现不一致的鲁棒性问题。对每种失败都提供了定义、分析现有研究、探索根本原因并提出缓解策略。

Result: 通过整合碎片化的研究工作，该调查为LLM推理的系统性弱点提供了结构化视角，提供了有价值的见解，并指导未来研究构建更强、更可靠、更鲁棒的推理能力。同时发布了GitHub存储库，收集了关于LLM推理失败的研究工作。

Conclusion: 该论文首次对LLM推理失败进行了全面调查，提出了系统的分类框架，为理解和解决LLM推理缺陷提供了结构化方法，有助于推动更可靠、鲁棒的推理能力发展。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [3] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT逻辑框架的非马尔可夫奖励规范方法，用于大规模状态空间的MDP，通过理论片段识别和奖励机器+Hindsight Experience Replay技术解决表达力与计算复杂度的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理大规模状态空间MDP中的复杂任务时存在局限性，需要手动编码谓词且表达能力有限。本文旨在开发一个统一、可重用的框架，能够处理非结构化、异构数据域中的复杂任务规范。

Method: 1) 使用LTLfMT（基于理论的线性时序逻辑）作为逻辑规范语言；2) 从理论上识别LTLfMT的可处理片段；3) 提出基于奖励机器和Hindsight Experience Replay（HER）的实用方法，将一阶逻辑规范转化为可执行的奖励机制并解决奖励稀疏性问题。

Result: 在连续控制环境中使用非线性算术理论进行评估，实验结果表明：1) 该方法能够自然地规范复杂任务；2) 定制的HER实现对于解决具有复杂目标的任务至关重要。

Conclusion: 提出的LTLfMT框架为大规模状态空间MDP中的非马尔可夫奖励规范提供了统一且可重用的解决方案，通过理论片段识别和奖励机器+HER技术平衡了表达力与计算可行性，在复杂任务规范方面表现出色。

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [4] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个基于图算法问题的基准测试，用于评估大型推理模型，揭示了当前模型在长上下文推理中的两大弱点：随着图节点数增加准确率急剧下降，以及过度思考现象。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限性：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要一个新的基准来更全面地评估大型推理模型的推理能力。

Method: 设计了GrAlgoBench基准测试，包含9个图算法任务。图算法问题特别适合评估推理能力：需要长上下文推理、可精细控制难度、支持标准化程序化评估。

Result: 实验揭示了当前大型推理模型的两大弱点：1) 当图节点超过120个时，准确率急剧下降到50%以下，主要由执行错误、弱记忆和冗余推理导致；2) 存在过度思考现象，大量无效的自我验证增加了推理痕迹但未提高正确性。

Conclusion: GrAlgoBench通过暴露大型推理模型的局限性，确立了图算法问题作为严谨、多维且实际相关的测试平台，有助于推进大型推理模型推理能力的研究。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [5] [Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion](https://arxiv.org/abs/2602.06351)
*Longhui Ma,Di Zhao,Siwei Wang,Zhao Lv,Miao Wang*

Main category: cs.AI

TL;DR: Trifuse是一个基于注意力机制的GUI grounding框架，通过整合OCR文本线索和图标级语义，无需任务特定微调就能实现GUI元素的精确定位。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding方法主要依赖大规模数据集微调MLLMs来预测坐标，这种方法数据密集且对未见界面的泛化能力差。基于注意力的替代方案虽然无需微调，但由于缺乏显式的空间锚点而可靠性较低。

Method: Trifuse框架显式整合互补的空间锚点，通过注意力机制、OCR提取的文本线索和图标级语义描述，采用共识-单峰融合策略强制跨模态一致性同时保持尖锐的定位峰值。

Result: 在四个grounding基准测试上的广泛评估表明，Trifuse无需任务特定微调就能实现强大性能，显著减少对昂贵标注数据的依赖。消融研究显示整合OCR和语义线索能持续提升基于注意力的grounding性能。

Conclusion: Trifuse通过显式整合互补空间锚点，为GUI grounding提供了一个无需任务特定微调的通用框架，有效解决了现有方法的泛化性和可靠性问题。

Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its effectiveness as a general framework for GUI grounding.

</details>


### [6] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: 本文提出DEPO框架，通过在线难度评估器动态筛选训练数据，在保持模型性能的同时将推理成本降低2倍，解决了GRPO在简单或复杂问题上梯度信号衰减和计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如DeepSeek-R1）通过GRPO扩展推理时计算具有潜力，但GRPO在问题过于简单或复杂时会出现梯度信号衰减问题。梯度消失使信号易受噪声影响，损害收敛稳定性。现有变体如DAPO虽尝试解决梯度消失，但无法缓解低效用样本上详尽rollout带来的巨大计算开销。

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成在线难度评估器，在rollout阶段前动态评估和筛选训练数据。该机制确保计算资源优先分配给具有高学习潜力的样本，优化推理对齐的效率和鲁棒性。

Result: 实证结果表明，DEPO在不影响模型性能的情况下，将rollout成本降低了高达2倍。该方法显著降低了训练高性能推理模型的计算门槛，为推理扩展提供了更可持续的路径。

Conclusion: DEPO通过动态难度评估和样本筛选，有效解决了GRPO的梯度衰减和计算效率问题，为大型推理模型的训练提供了更高效、更稳健的对齐框架，降低了推理扩展的计算成本。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [7] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token是一种质量感知的分词方法，通过双层优化、强化学习和自适应参数学习，将数据可靠性直接纳入词汇表构建，在基因组学和金融领域显著提升性能，减少15%的token数量。


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法在处理序列数据时没有考虑信号质量，限制了它们在嘈杂的真实世界语料库中的有效性。需要一种能够将数据可靠性直接纳入词汇表构建的方法。

Method: 提出了QA-Token（质量感知分词），包含三个关键贡献：(1) 双层优化公式，联合优化词汇表构建和下游性能；(2) 强化学习方法，通过质量感知奖励学习合并策略，具有收敛保证；(3) 通过Gumbel-Softmax松弛的自适应参数学习机制，实现端到端优化。

Result: 实验评估显示一致改进：基因组学（变体调用中F1分数比BPE提高6.7个百分点），金融（夏普比率提高30%）。在基础模型规模上，分词了包含1.7万亿碱基对的预训练语料库，实现了最先进的病原体检测（94.53 MCC），同时减少了15%的token数量。

Conclusion: QA-Token解锁了嘈杂的真实世界语料库，包括petabases级别的基因组序列和terabytes级别的金融时间序列，用于基础模型训练，且推理开销为零。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [8] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 论文指出自回归大语言模型在长时程推理中存在内在稳定性限制，即使在线性无分支任务中，决策优势也会随执行长度指数衰减，导致性能断崖式下降。


<details>
  <summary>Details</summary>
Motivation: 传统解释将大语言模型在长时程任务中的性能下降归因于任务复杂性（如组合搜索爆炸或长期信用分配），但作者认为这些解释不完整，即使在无语义模糊的线性无分支任务中，自回归执行也存在内在稳定性限制。

Method: 提出将长时程推理重新定义为结构治理问题，推导出定理A证明单路径自回归推理中的决策优势随执行长度指数衰减。通过合成环境和真实TextWorld任务的实证研究验证理论预测。

Result: 实证研究显示性能断崖与理论预测一致，揭示了自回归架构在维持长期连贯性方面的新限制。短时程评估协议可能掩盖结构不稳定性。

Conclusion: 长时程推理的根本约束源于自回归生成的过程级不稳定性而非单纯搜索或任务复杂性。未来推理系统可能需要从规模扩展转向结构化治理，稳定的长时程推理需要离散分段，自然诱导出有向无环图等图状执行结构。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [9] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出了AgentCPM-Explore，一个4B参数的边缘规模智能体模型，通过参数空间模型融合、奖励信号去噪和上下文信息精炼等技术，解决了边缘模型在监督微调中的灾难性遗忘、强化学习中的奖励信号噪声敏感性以及长上下文推理退化等问题，在多个基准测试中达到或超越了更大规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体系统严重依赖大规模模型，而边缘规模模型（4B参数级别）的潜力尚未得到充分探索。本文旨在系统研究训练4B参数规模智能体模型的方法，解决边缘模型面临的三个主要瓶颈问题。

Method: 提出了AgentCPM-Explore，一个紧凑的4B智能体模型，具有高知识密度和强探索能力。采用整体训练框架，包括：1）参数空间模型融合；2）奖励信号去噪；3）上下文信息精炼。通过深度探索技术提升模型性能。

Result: AgentCPM-Explore在4B类模型中达到最先进性能，在四个基准测试中匹配或超越了8B类SOTA模型，在五个基准测试中甚至超越了Claude-4.5-Sonnet或DeepSeek-v3.2等更大规模模型。在GAIA文本任务上达到97.09%的准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈并非其固有的能力上限，而是推理稳定性问题。通过本文建立的训练框架，AgentCPM-Explore有效释放了边缘规模模型被低估的显著潜力，为紧凑型智能体模型的发展提供了有力证据。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [10] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER提出了一种动态扩展-缩减控制策略，用于多路径思维链推理，通过在线控制器平衡探索与利用，在固定计算预算下提升推理准确率同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索-利用权衡上存在局限：树状搜索通过脆弱的扩展规则硬编码探索，干扰后训练推理；并行推理则过度探索冗余假设路径且依赖弱答案选择。研究发现最优平衡是阶段依赖的，正确与错误推理路径往往只在后期才分叉。

Method: 将测试时扩展重新表述为假设池上的动态扩展-缩减控制问题。提出HyPER：1) 在线控制器根据假设池演化从探索转向利用；2) 令牌级细化机制实现高效生成时利用而无需完整路径重采样；3) 长度和置信度感知的聚合策略实现可靠答案时利用。

Result: 在四个专家混合语言模型和多样化推理基准上的实验表明，HyPER始终实现更优的准确率-计算权衡，准确率提升8-10%，同时令牌使用量减少25-40%。

Conclusion: HyPER通过动态控制多路径解码中的探索-利用权衡，在固定计算预算下显著提升推理性能，为测试时计算扩展提供了更灵活有效的解决方案。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [11] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 本文提出SeeUPO算法，解决现有RL算法在多轮交互场景中缺乏收敛保证的问题，通过序列级顺序更新策略优化实现无critic的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在多轮交互的智能体场景中缺乏经过验证的收敛保证，导致训练不稳定和无法收敛到最优策略。主流骨干RL算法无法同时实现无critic和收敛保证。

Method: 提出SeeUPO（序列级顺序更新策略优化），将多轮交互建模为顺序执行的多臂赌博机问题，通过反向执行顺序的逐轮顺序策略更新，确保单调改进并通过反向归纳收敛到全局最优解。

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO相比现有骨干算法取得显著改进：在Qwen3-14B上相对增益43.3%-54.6%，在Qwen2.5-14B上相对增益24.1%-41.9%（跨基准平均），并具有优越的训练稳定性。

Conclusion: SeeUPO是一种具有收敛保证的无critic方法，能够有效解决多轮交互场景中RL算法的收敛问题，为LLM智能体训练提供了更稳定可靠的优化框架。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [12] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 本文提出了一种新的VLM鲁棒性评估框架，发现传统输出层不变性假设不足，揭示了三种失效模式：内部表征漂移、规模不改善鲁棒性、扰动对任务有不同影响。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型（VLM）鲁棒性评估仅关注输出层不变性，隐含假设稳定预测反映稳定的多模态处理。本文认为这一假设不足，需要更深入评估内部表征变化。

Method: 提出表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），同时结合标准基于标签的指标。在SEEDBench、MMMU和POPE数据集上对现代VLM进行评估。

Result: 发现三种失效模式：1）模型预测答案保持不变时内部表征发生显著漂移；2）鲁棒性不随模型规模提升而改善；3）扰动对不同任务有不同影响：破坏推理任务但减少幻觉基准的误报。

Conclusion: 传统输出层鲁棒性评估不足，需要更全面的内部表征分析框架。VLM鲁棒性存在复杂模式，不能简单通过输出稳定性推断内部处理稳定性。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [13] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK提出了一种自回归知识图谱生成模型，将图谱视为三元组序列进行生成，无需显式规则监督即可学习语义约束，在IntelliGraphs基准上达到89.2%-100%的语义有效性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖关系，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕获整个子图的相互依赖关系以产生语义连贯的结构。

Method: 提出ARK（自回归知识图谱生成）模型家族，将图谱视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还引入了SAIL，ARK的变分扩展，通过学习的潜在表示实现可控生成。

Result: 在IntelliGraphs基准上，模型在多样化数据集上达到89.2%到100.0%的语义有效性，同时生成训练中未见的新图谱。分析显示模型容量（隐藏维度≥64）比架构深度更重要，循环架构在保持可比有效性的同时提供显著计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答中具有实际应用价值。模型容量比架构深度更关键，循环架构在效率和有效性之间取得了良好平衡。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [14] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 提出一种基于语义LTL到自动机转换的多任务强化学习方法，利用结构化任务嵌入实现通用策略学习


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中，需要学习能够泛化到任意任务的通用策略。现有方法在处理复杂LTL规范时存在局限性，需要更有效的任务表示方法。

Method: 利用新一代语义LTL到自动机转换技术，构建语义标记的自动机，从中提取结构化任务嵌入来条件化策略，支持完整的LTL规范。

Result: 实验表明该方法在多个领域达到最先进性能，能够扩展到复杂规范，而现有方法在这些情况下会失败。

Conclusion: 基于语义LTL到自动机转换的任务嵌入技术能够有效支持多任务强化学习，处理复杂LTL规范，实现高性能的通用策略学习。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [15] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 研究比较了两种主动概念学习策略：理性主动学习者的期望信息增益策略和人类常用的积极测试策略，发现在简单概念上后者表现更好，这可能是因为"确认偏误"有助于在稀疏假设空间中维持可处理的推理。


<details>
  <summary>Details</summary>
Motivation: 研究人类主动概念学习中的权衡问题，即查询的信息量与学习者生成和评分假设的稳定性之间的平衡。探讨在神经符号贝叶斯学习框架下，不同查询策略的有效性差异。

Method: 采用神经符号贝叶斯学习方法，其中假设是由大型语言模型生成的可执行程序，通过贝叶斯更新重新加权。比较两种策略：理性主动学习者的期望信息增益策略和人类常用的积极测试策略。在经典数字游戏中进行概念学习任务测试。

Result: 期望信息增益策略在需要证伪的复杂概念（如复合规则或包含例外的规则）上有效，但在简单概念上表现不佳。积极测试策略虽然信息次优，但通过选择"安全"查询来维持提案有效性，在简单规则上收敛更快。

Conclusion: "确认偏误"可能不是认知错误，而是在人类思维特有的稀疏、开放式假设空间中维持可处理推理的理性适应策略。积极测试策略通过避免支持不匹配陷阱，在简单概念学习中具有优势。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [16] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通才智能体需要交互式环境进行自我探索，但当前交互环境严重匮乏，现有合成方法在环境多样性和可扩展性方面存在显著局限性。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上展示显著性能提升，表明强大的泛化能力；研究领域数量增加与模型泛化性能的关系，提供经验证据表明扩展环境多样性对稳健智能体学习至关重要。

Conclusion: ScaleEnv通过从零构建交互式环境解决了环境稀缺问题，其程序化测试和任务验证机制确保了环境可靠性和任务可解性，扩展环境多样性对智能体学习具有关键作用。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [17] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种面向大型基础模型的高效在线结构化剪枝框架，通过分区引导的动态剪枝在推理过程中实现上下文感知的稀疏化，显著降低计算开销和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前的结构化剪枝方法在推理时采用固定的剪枝决策，忽略了自回归token生成过程中出现的稀疏模式，无法充分利用上下文信息进行动态优化。

Method: POP将模型通道划分为保留区、候选区和剪枝区：预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估。该方法无需预处理、离线校准、重新训练或学习预测器。

Result: 在多种大型基础模型（LLMs、MoEs、VLMs）上的广泛评估表明，POP相比现有剪枝方法能提供更高的准确性，同时产生更小的计算开销和更低的推理延迟。

Conclusion: POP是一种轻量级即插即用的在线剪枝框架，通过上下文条件化的动态剪枝有效平衡了模型精度和计算效率，为大型基础模型的高效推理提供了新思路。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [18] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 提出自适应差分隐私联邦学习框架，解决异构设备和Non-IID数据下的训练不稳定问题，通过本地压缩模块、自适应梯度裁剪和约束感知聚合机制提升模型性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际部署中面临设备异构性和非独立同分布数据导致的梯度更新不稳定和偏差问题，而差分隐私的固定梯度裁剪和高斯噪声注入会进一步放大梯度扰动，造成训练震荡和性能下降

Method: 1) 客户端引入轻量级本地压缩模块来正则化中间表示并约束梯度变异性；2) 服务器端采用基于历史更新统计的自适应梯度裁剪策略；3) 设计约束感知聚合机制来抑制不可靠或噪声主导的客户端更新

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该方法提高了收敛稳定性和分类准确率

Conclusion: 提出的自适应差分隐私联邦学习框架能够有效解决异构隐私约束环境下的模型效率问题，通过多层次的适应性机制提升了联邦学习的稳定性和性能

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [19] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态预测解释与智能体系统解释的差异，发现传统特征归因方法适用于静态分类但不适用于诊断智能体执行失败，而基于轨迹的诊断方法能有效定位智能体行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，智能体AI系统通过多步决策轨迹展现行为，成功与失败由决策序列而非单一输出决定。现有解释方法主要针对静态预测设计，不清楚这些方法如何适用于行为随时间演化的智能体场景。

Method: 通过比较静态分类任务中的归因解释方法与智能体基准测试（TAU-bench Airline和AssistantBench）中的轨迹诊断方法，明确区分两种解释方法的适用性。

Result: 归因方法在静态设置中能获得稳定的特征排名（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障。基于轨迹的评估方法能一致定位行为故障，发现状态跟踪不一致在失败运行中普遍性高2.7倍，并将成功概率降低49%。

Conclusion: 研究结果表明，在评估和诊断自主AI行为时，需要从静态解释转向轨迹级解释方法，以更好地理解和诊断智能体系统的行为故障。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>
