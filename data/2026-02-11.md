<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 71]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过视觉编辑、树状结构建模和智能体驱动查询解决来提升表格理解的准确性和可用性。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答面临两大挑战：精确提取单元格内容和位置，以及准确恢复表格布局中隐含的逻辑结构、层次关系和语义关联。现有方法存在信息丢失、处理复杂布局困难等问题，人工解释又耗时耗力。

Method: ST-Raptor是一个智能体系统，提供交互式分析环境，结合视觉编辑、基于树的结构建模和智能体驱动的查询解决机制，支持准确且用户友好的表格理解。

Result: 在基准测试和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性方面均优于现有方法。

Conclusion: ST-Raptor通过创新的交互式智能体系统，有效解决了半结构化表格问答中的信息提取和结构恢复难题，提供了更准确、更用户友好的解决方案。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [2] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 论文提出"空间理论"概念，评估多模态基础模型在主动探索和构建空间信念方面的能力，发现存在主动-被动差距、低效探索、信念不稳定和信念惯性等问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型在被动感知方面表现出色，但在主动、自主探索方面的能力研究不足。空间具身智能需要智能体在部分可观测环境下通过行动获取信息，因此需要研究模型在主动探索和构建空间信念方面的能力。

Method: 提出"空间理论"概念，定义为智能体通过自主主动探索获取信息，并从序列化部分观测中构建、修正和利用空间信念的能力。通过好奇心驱动的探索基准进行评估，关键创新是空间信念探测技术，在每一步提示模型揭示其内部空间表示。

Result: 评估最先进模型发现几个关键瓶颈：1) 主动-被动差距 - 当智能体必须自主收集信息时性能显著下降；2) 高无效性 - 模型探索缺乏系统性；3) 全局信念不稳定导致空间知识随时间退化；4) 信念惯性 - 智能体无法用新证据更新过时的先验，视觉模型尤其严重。

Conclusion: 当前基础模型在主动探索过程中难以维持连贯、可修正的空间信念，揭示了多模态模型在主动空间推理方面的局限性，为未来改进方向提供了重要见解。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [3] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: BRIDGE框架通过模型响应学习潜在难度尺度，并将其锚定到人类任务完成时间，实现从模型性能推断人类任务难度


<details>
  <summary>Details</summary>
Motivation: 现有基于人类任务完成时间直接标注的方法成本高、噪声大、难以扩展，需要一种可扩展的方法来评估AI系统的真实能力

Method: 使用双参数逻辑项目反应理论模型，从多个基准测试的模型性能数据中联合估计潜在任务难度和模型能力，建立潜在难度与人类完成时间对数之间的线性关系

Result: 潜在任务难度与人类完成时间的对数呈线性关系，可以仅从模型性能推断新基准的人类任务完成时间，预测前沿模型能力，重现METR的指数缩放结果

Conclusion: BRIDGE提供了一种可扩展的心理测量框架，将基准性能与人类可解释的任务难度联系起来，为评估AI系统真实能力提供了新方法

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [4] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: STEER2ADAPT：一个轻量级框架，通过组合而非从头学习新的转向向量来适应LLMs，在推理和安全任务上平均提升8.2%


<details>
  <summary>Details</summary>
Motivation: 现有激活转向方法通常依赖每个任务或概念的单一静态方向，这使其在任务变化时不够灵活，且难以处理需要多种协调能力的复杂任务

Method: 提出STEER2ADAPT框架，将任务共享的底层概念维度捕获为可重用的低维语义先验子空间，仅需少量示例即可动态发现基向量的线性组合来适应新任务

Result: 在9个任务和3个模型上的实验表明，STEER2ADAPT在推理和安全领域平均提升8.2%，是一种数据高效、稳定且透明的推理时适应方法

Conclusion: STEER2ADAPT通过组合转向向量而非从头学习，为LLMs提供了一种灵活、高效的适应方法，特别适用于需要多种协调能力的复杂任务

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [5] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 研究开发了一个自适应系统，通过动态选择两种ICAP模式的例题（主动式引导例题和建构式错误例题）来优化认知参与度，比较了BKT和DRL两种自适应方法与基线方法，发现两种自适应策略都能显著提升学生表现。


<details>
  <summary>Details</summary>
Motivation: ICAP框架定义了四种认知参与水平，但如何在智能辅导系统中个性化学习活动以激发最佳认知参与水平仍然是一个关键挑战。

Method: 开发了一个自适应系统，通过动态选择两种ICAP模式的例题（主动式引导例题和建构式错误例题）来优化认知参与度。在逻辑ITS中比较了贝叶斯知识追踪（BKT）和深度强化学习（DRL）两种自适应方法与一个非自适应基线方法。

Result: 113名学生的实验表明，两种自适应策略都显著提高了学生在测试问题上的表现。BKT对低先验知识学生的后测成绩提升最大，帮助他们赶上高先验知识的同学；而DRL在高先验知识学生中产生了显著更高的后测成绩。

Conclusion: 该研究为认知参与度和自适应性的复杂相互作用及其对学习结果的影响提供了新的见解，表明自适应方法可以根据学生先验知识水平提供差异化的认知参与支持。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [6] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD是一个确定性策略提取框架，将预训练的扩散轨迹规划器蒸馏为高效策略，消除扩散采样，实现8倍加速并保持竞争性能


<details>
  <summary>Details</summary>
Motivation: 扩散轨迹规划器能很好建模人类驾驶的多模态行为，但依赖迭代随机采样，难以满足实时安全关键部署需求

Method: 使用分数正则化策略优化，利用预训练扩散规划器的评分函数作为行为先验来正则化策略学习；通过模仿预测驾驶员控制器的评论家提供密集安全监督

Result: 在nuPlan场景中实现竞争性能，比扩散基线加速8倍；在interPlan基准测试中达到基于学习的规划器的最先进泛化能力

Conclusion: RAPiD成功将扩散规划器蒸馏为高效确定性策略，解决了扩散模型实时部署难题，在保持性能的同时显著提升效率

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [7] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 提出宽深研究智能体框架，通过并行工具调用扩展智能体宽度，在深度研究基准上显著提升性能并减少所需步骤


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过增加顺序思考和工具调用的深度来提升性能，但通过并行工具调用扩展宽度的潜力尚未充分探索。本研究旨在探索同时扩展深度和宽度对智能体行为和性能的影响。

Method: 提出宽深研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调，避免了复杂的多智能体编排。通过案例研究分析改进因素，并探索各种工具调用调度器来优化并行策略。

Result: 扩展宽度显著提升了深度研究基准的性能，同时减少了获得正确答案所需的轮次。使用GPT-5-Medium在BrowseComp上获得62.2%的准确率，超过了GPT-5-High报告的原始54.9%。

Conclusion: 优化宽度和深度之间的权衡是构建高效深度研究智能体的关键途径。并行工具调用能够在不依赖复杂多智能体编排的情况下显著提升性能。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [8] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: NAAMSE是一个进化框架，将AI智能体安全评估重新定义为反馈驱动的优化问题，通过遗传提示突变、分层语料库探索和非对称行为评分来系统性地发现被单次方法遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体安全评估主要依赖人工红队测试或静态基准测试，这些方法无法模拟自适应、多轮次的对抗性攻击，存在评估瓶颈。

Method: 采用进化框架，使用单个自主智能体协调遗传提示突变、分层语料库探索和非对称行为评分的生命周期，利用模型响应作为适应度信号，迭代地组合有效攻击策略，同时确保"良性使用正确性"。

Result: 在Gemini 2.5 Flash上的实验表明，进化突变能系统性地放大被单次方法遗漏的漏洞，控制消融实验显示探索与定向突变的协同作用能发现高严重性故障模式。

Conclusion: 这种自适应方法为面对不断演变的威胁时，提供了更现实和可扩展的智能体鲁棒性评估，NAAMSE代码已开源。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [9] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过生成-选择范式解决VLA模型在少样本适应中的几何模糊问题，使用价值引导的动作块选择来提升轨迹的几何精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在少样本适应新任务时不可靠，主要问题在于几何模糊性——语义合理的轨迹因几何细节偏差导致执行失败，现有方法在有限监督下难以解决这种近邻候选动作的歧义。

Method: 提出VGAS框架：1) 使用微调VLA作为高召回率提案生成器；2) 引入Q-Chunk-Former作为几何基础Transformer评论家，解决细粒度几何模糊；3) 提出显式几何正则化(EGR)，通过塑造判别性价值景观来保持动作排序分辨率并缓解价值不稳定。

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下持续提升成功率和鲁棒性，有效解决了少样本VLA适应中的几何模糊问题。

Conclusion: VGAS通过生成-选择范式和几何感知的价值引导，为少样本VLA适应提供了可靠解决方案，显著提升了模型在物理控制任务中的几何精度和鲁棒性。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [10] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: PBio-Agent：一个多智能体框架，通过难度感知任务排序和迭代知识精炼，预测化学扰动下的基因调控响应，在LINCSQA和PerturbQA基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理高维扰动结果时容易陷入信息纠缠，且当前研究主要关注单细胞遗传扰动，而药物发现核心的批量细胞化学扰动研究不足。需要开发能够预测复杂化学扰动下基因调控的方法。

Method: 提出PBio-Agent多智能体框架，包含难度感知任务排序和迭代知识精炼。核心洞见是：受相同扰动影响的基因共享因果结构，已确信预测的基因可为更困难案例提供上下文。框架使用生物知识图谱增强的专门智能体，合成智能体整合输出，专门判断器确保逻辑一致性。

Result: PBio-Agent在LINCSQA和PerturbQA基准测试中均优于现有基线方法，即使较小的模型也能在不额外训练的情况下预测和解释复杂生物过程。

Conclusion: PBio-Agent通过多智能体框架有效解决了化学扰动下基因调控预测的挑战，为药物发现提供了有力工具，展示了生物因果推理的新范式。

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [11] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 本文提出了一类新的后验确定性POMDPs，在该类模型中可达概率可以近似计算到任意精度，突破了传统POMDPs中该问题的不可判定性限制。


<details>
  <summary>Details</summary>
Motivation: 传统POMDPs中的许多验证和综合问题是不可判定或难处理的，特别是可达概率计算问题（Madani et al., 2003证明无法计算或近似）。这与完全可观察的MDPs形成鲜明对比，后者可达值可在多项式时间内计算。

Method: 引入后验确定性POMDPs这一新类别，定义为：给定当前状态、采取的动作和接收的观测，下一个状态可以唯一确定。这意味着一旦真实状态已知，它将永远保持已知。

Result: 对于后验确定性POMDPs，到达给定状态集的最大概率可以近似到任意精度。该类包含所有MDPs，并捕获了经典的非平凡示例（如Tiger POMDP），是已知最大的POMDPs类别之一。

Conclusion: 后验确定性POMDPs提供了一个具有可计算可达概率的POMDPs新类别，扩展了可处理POMDPs问题的范围，同时保持了模型的表达能力。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [12] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 该研究提出了一个结合知识图谱与多智能体推理的框架，用于寻找PFAS（全氟和多氟烷基物质）的可持续替代品，通过分布式专业化和关系推理来加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 当前材料科学面临信息过载的挑战，需要整合从分子化学到机械性能的跨领域概念。传统方法（包括单智能体LLM）难以处理这种复杂信息流，且容易产生幻觉。PFAS作为受严格监管的化学品，急需寻找可持续替代品。

Method: 开发了一个基于大规模知识图谱的多智能体框架，包含专门化智能体：问题分解、证据检索、设计参数提取和图遍历。通过定制图遍历策略，系统在利用性搜索（关注领域关键结果）和探索性搜索（发现跨领域新连接）之间切换。

Result: 消融研究表明，完整的多智能体流程优于单次提示方法。以生物医学导管为例，该框架生成了平衡摩擦学性能、热稳定性、化学抗性和生物相容性的可持续无PFAS替代品。系统能够发现不同知识领域之间的潜在联系，支持假设生成。

Conclusion: 该工作建立了一个结合知识图谱与多智能体推理的框架，扩展了材料设计空间，展示了多个初步设计候选方案，为加速科学发现提供了新方法。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [13] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: MSP-LLM：一个统一的LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题，通过引入离散材料类别作为中间决策变量，在完整MSP任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的关键瓶颈，现有方法仅解决孤立子任务，缺乏统一的完整解决方案。

Method: 提出MSP-LLM框架，将MSP分解为前驱体预测和合成操作预测两个子问题，引入离散材料类别作为中间决策变量，并采用分层前驱体类型作为归纳偏置，使用显式条件策略保持前驱体信息。

Result: MSP-LLM在PP、SOP以及完整MSP任务上均优于现有方法，展示了有效且可扩展的MSP框架。

Conclusion: MSP-LLM提供了一个统一的LLM框架，能够有效解决完整的材料合成规划任务，加速实际材料发现过程。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [14] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL框架利用符号微分规则实现可验证的数学问题分解，确保子问题更简单、解决子问题有助于父任务、且分解关系有数学基础，相比启发式方法显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法通常是启发式的，无法保证子问题更简单、解决子问题有助于父任务、且分解关系有数学基础。需要一种可验证的分解框架来确保这些关键属性。

Method: 提出Verify-RL框架，利用符号微分规则作为分解基础。通过微积分规则明确定义表达式如何分解为更简单的组件，并满足三个可验证条件：结构复杂度严格递减、解包含性、形式规则推导。实现"构造即验证"的自动验证机制。

Result: 消除无效分解带来显著性能提升：最难题目的准确率从32%翻倍至68%，整体相对改进达40%。验证机制确保所有分解都有效，而启发式方法中有相当比例的分解是无效的。

Conclusion: 符号微分为数学问题分解提供了自然的可验证结构。Verify-RL框架通过确保分解满足三个关键数学属性，显著提升了语言模型解决复杂数学问题的能力，证明了可验证分解的重要性。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [15] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi是一个睡眠基础模型，能够同时掌握整夜的睡眠宏观结构和细粒度信号特征，在多项下游任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 睡眠医学目前主要依赖任务特定的模型，这些模型关注局部微观结构特征，忽略了多模态PSG数据的丰富上下文和整夜睡眠的全局宏观结构。

Method: 采用分层双编码器设计：宏观编码器建模整夜时间依赖关系，微观编码器捕获生物信号的短期特征。宏观编码器通过人口统计学引导的对比学习进行训练，微观编码器通过混合掩码自编码器和多模态对比目标进行优化。

Result: 在超过20,000个PSG记录（158K小时）上预训练后，SleepMaMi在多样化的下游任务中超越了现有的基础模型，展示了优越的泛化能力和标签高效适应能力。

Conclusion: SleepMaMi成功解决了睡眠医学中任务特定模型的局限性，通过统一的基础模型框架同时捕捉睡眠的宏观和微观特征，为临床睡眠分析提供了更强大的工具。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [16] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG框架通过检索-重排序-推理三阶段流程，让多模态大语言模型能够从大规模表格图像集合中回答用户查询，显著提升了检索召回率和答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据常以图像形式存在（如财务报表、手写记录、文档扫描），现有MLLM方法通常假设相关表格已准备好，但实际应用中需要从大规模表格集合中识别和推理相关表格来回答用户查询。

Method: 提出TabRAG框架：1）使用联合训练的视觉-文本基础模型检索候选表格；2）利用MLLM对这些候选表格进行细粒度重排序；3）使用MLLM对选定表格进行推理生成答案。

Result: 在包含88,161个训练样本和9,819个测试样本的新建数据集上进行实验，涵盖8个基准测试和48,504个独特表格。结果显示，相比现有方法，检索召回率提升7.0%，答案准确率提升6.1%。

Conclusion: TabRAG框架为现实世界的表格理解任务提供了实用解决方案，通过检索-重排序-推理的端到端流程，显著提升了从大规模表格图像集合中回答查询的性能。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [17] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文提出了一个基于统一基础本体论的信任参考本体（ONTrust），旨在为人工智能和区块链等新技术中的信任问题提供概念化基础，支持信息建模、自动推理和语义互操作。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和区块链等技术的发展，机器越来越像人类，去中心化技术创造了新的信任形式。这些新技术有潜力改善产品和服务提供，促进个人和集体福祉，但其采用很大程度上取决于信任。为了构建可信系统，需要在定义法律法规和治理模型的同时，正确概念化信任，使其能够被人类和机器理解。

Method: 开发了基于统一基础本体论（UFO）的信任参考本体（ONTrust），使用OntoUML语言进行规范。该本体形式化地描述了信任概念及其不同类型，分析了影响信任的各种因素，并解释了信任关系如何产生风险。通过两个文献案例研究来展示ONTrust的应用。

Result: ONTrust已被应用于多个领域，包括概念建模和企业架构设计、语言评估与（重新）设计、信任管理、需求工程，以及在情感化人机协作背景下的可信人工智能。本体能够支持信息建模、自动推理、信息集成和语义互操作任务。

Conclusion: ONTrust为信任提供了一个坚实的本体论基础，有助于理解和管理新兴技术中的信任问题。该本体能够支持多种应用场景，为构建可信系统和促进技术采用提供了概念化工具。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [18] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast是一个将未来事件知识整合到时间序列预测中的模块化框架，专门解决电商在闪购、假日促销等事件期间的需求预测失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在闪购、假日促销和政策干预等高影响时期经常失败，因为这些时期需求模式会发生突然且不可预测的变化。电商运营需要能够处理这些事件驱动的需求波动的预测解决方案。

Method: EventCast使用LLM处理非结构化业务数据（活动、假日安排、卖家激励），将其转换为可解释的文本摘要，然后通过双塔架构将这些摘要与历史需求特征融合，实现准确、可解释且可扩展的预测。

Result: 在4个国家160个地区10个月的电商场景中，相比无事件知识的变体，MAE和MSE分别提升86.9%和97.7%；相比最佳工业基线，在事件驱动期间MAE降低57.0%，MSE降低83.3%。

Conclusion: EventCast提供了一个实用的解决方案，通过整合未来事件知识来改进动态电商环境中的运营决策，自2025年3月起已部署到实际工业管道中。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [19] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder：首个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动代码进化实现精确几何重建，在几何重建精度和视觉一致性方面显著领先。


<details>
  <summary>Details</summary>
Motivation: 程序代码作为连接视觉与逻辑的桥梁，为增强大模型多模态推理能力提供了可行的监督方法。然而，当前逆向图形方法在准确重建复杂几何细节方面面临巨大挑战，常导致关键几何约束丢失或结构失真。

Method: 提出基于多智能体系统的Geo-coder框架，将过程解耦为：1）通过像素级锚定进行几何建模，利用视觉算子和大模型的互补优势精确捕获像素坐标和视觉属性；2）引入合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正。

Result: 实验表明Geo-coder在几何重建精度和视觉一致性方面取得显著领先。重建图像在多模态推理任务中表现出与原始图像相当的性能，验证了框架的鲁棒性。开源了包含1500+样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder通过创新的多智能体逆向编程框架有效解决了复杂几何细节重建的瓶颈问题，为后续研究提供了坚实的数据和模型基础，推动了多模态推理能力的发展。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [20] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 本研究调查了本科生对AI评分系统的看法，发现学生对AI缺乏情境理解和个性化表示担忧，建议AI系统应反映人类判断、灵活性和同理心，作为人类监督下的补充工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解学生对AI评分系统的看法，特别是在计算机科学课程中使用AI进行评分时，关注公平性、信任度、一致性和透明度等伦理问题。

Method: 采用基于Jobin（2019）伦理原则框架的研究方法，在本科计算机科学课程（n=27）中调查学生对AI评分系统的看法，通过比较AI生成的反馈与原始人工评分反馈来评估AI评分的质量。

Result: 研究发现学生对AI评分系统存在担忧，主要关注AI缺乏情境理解和个性化能力。学生认为AI系统在理解上下文和提供个性化反馈方面存在局限性。

Conclusion: 结论是公平且可信的AI系统应反映人类判断、灵活性和同理心，作为人类监督下的补充工具。这项工作通过放大学生声音和为AI人性化设计提供原则，为以伦理为中心的评估实践做出贡献。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [21] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap多智能体系统在AndroidWorld基准测试中实现100%成功率，首次完全解决所有116个任务，超越人类表现（80%）


<details>
  <summary>Details</summary>
Motivation: 解决单智能体架构在移动设备任务执行中的失败问题：混合推理轨迹导致的上下文污染、文本输入失败未被检测、重复动作循环无法逃脱

Method: 采用多智能体分解（六个专门化智能体）、确定性后验证（文本输入与设备状态对比）、元认知推理（检测循环并触发策略变更）

Result: 在AndroidWorld基准测试中达到100%成功率，完全解决所有116个任务；多智能体分解贡献+21分，验证执行+7分，元认知+9分

Conclusion: Minitap通过专门化智能体、输入验证和元认知机制成功解决了移动设备任务执行的挑战，超越人类表现并开源发布

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [22] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出Data Darwinism十级分类法，通过数据-模型协同进化框架提升基础模型性能，在科学文献领域验证了高级数据处理能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统化的数据处理框架。需要建立数据与模型协同进化的理论框架，让先进模型为下一代系统生成更优质的数据。

Method: 提出Data Darwinism十级分类法（L0-L9），构建Darwin-Science科学语料库（900B token，L0-L5）。通过L4（生成式精炼）和L5（认知补全）使用前沿LLM解析科学文本中的推理和术语。预训练daVinci-origin-3B/7B模型作为无污染基线，进行600B token的持续预训练。

Result: Darwin-Science在20多个基准测试中分别比基线模型提升2.12（3B）和2.95（7B）个百分点，在领域对齐任务上提升5.60和8.40个百分点。系统推进到L5级别带来1.36个百分点的总增益，证实高级数据处理能释放数据潜在价值。

Conclusion: Data Darwinism框架验证了数据-模型协同进化的有效性，高级数据处理能显著提升模型性能。发布Darwin-Science语料库和daVinci-origin模型，支持基于原则的协同进化发展。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [23] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLMs进行时间序列推理的框架，显著提升了LLM在时间序列任务上的性能，使小型模型能达到或超过大型专有LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在各领域普遍存在，但利用LLM进行时间序列推理仍处于早期阶段，主要受限于：缺乏精心策划的时间序列CoT训练数据、数据调度不足导致的数据效率低下、以及缺乏针对时间序列CoT数据优化的RL算法。

Method: 1. 数据合成管道：构建具有过程可验证注释的TS-文本多模态数据集
2. 数据调度机制：根据难度层次和任务分类原则安排训练样本
3. 两阶段强化微调：利用可验证的过程级CoT数据，采用细粒度多目标奖励

Result: VeriTime显著提升了LLM在各种时间序列推理任务上的性能。值得注意的是，它使紧凑的3B、4B模型能够达到与大型专有LLMs相当或超过的推理能力。

Conclusion: VeriTime框架通过系统化的数据合成、调度和强化学习训练，成功解决了时间序列推理中的关键挑战，为LLM在时间序列任务上的应用提供了有效解决方案。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [24] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: LQA是一个轻量化的量化自适应框架，用于在边缘设备上部署视觉语言模型，通过选择性混合量化和无梯度测试时适应来解决资源限制和分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署视觉语言模型面临资源限制和分布偏移导致的性能下降挑战。现有的测试时适应方法资源消耗过大，不适合在设备上部署。

Method: 提出LQA框架，结合模态感知量化策略和无梯度测试时适应。包括选择性混合量化(SHQ)和量化无梯度适应机制，实现轻量化和鲁棒性。

Result: 在合成和真实世界分布偏移实验中，LQA将整体适应性能提升4.5%，内存使用少于全精度模型，显著优于基于梯度的TTA方法，内存使用降低达19.9倍。

Conclusion: LQA为在边缘设备上实现鲁棒、隐私保护和高效的VLM部署提供了实用途径。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [25] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大语言模型在狭窄有害数据集上可能导致模型出现突发性错位，在多样无关场景中给出刻板"邪恶"回答。专家预测失败凸显了我们对LLM归纳偏好的理解不足。研究发现通用解决方案比狭窄解决方案更稳定高效。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在狭窄有害数据集上微调后出现的突发性错位现象，探索LLM学习和泛化的归纳偏好，理解为什么专家无法预测这种结果，为监控和缓解模型错位提供理论基础。

Method: 使用突发性错位作为案例研究，发现不同微调会收敛到相同的线性错位表示。通过引入KL散度损失学习狭窄解决方案的线性表示，比较两种表示的特性，分析其损失值、鲁棒性和在预训练分布中的影响力。

Result: 研究发现通用错位表示比狭窄解决方案具有更低的损失值、更强的抗干扰能力，并且在预训练分布中更具影响力。通用解决方案在稳定性和效率方面表现更优。

Conclusion: 这项工作分离出了通用错位的具体表示，可用于监控和缓解模型错位。更广泛地，为研究归纳偏好如何塑造LLM泛化提供了详细案例研究和初步指标。所有代码、数据集和模型微调均已开源。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [26] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly是一个基于信息瓶颈原则的LLM记忆框架，通过梯度自由优化器构建分层记忆结构，结合混合检索机制，在记忆连贯性、响应保真度和准确性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆框架面临一个基本困境：既要高效压缩冗余信息，又要保持精确检索以支持下游任务。这种压缩与检索精度之间的权衡限制了长期记忆系统的性能。

Method: 基于信息瓶颈原则，通过梯度自由优化器最小化压缩熵同时最大化相关熵，构建分层记忆结构。开发混合检索机制，整合语义、符号和拓扑检索路径，并采用迭代精炼处理复杂多跳查询。

Result: 综合实验表明，MemFly在记忆连贯性、响应保真度和准确性方面显著优于现有最先进的基线方法。

Conclusion: MemFly框架成功解决了LLM记忆系统中压缩效率与检索精度之间的根本困境，通过信息瓶颈原则和混合检索机制实现了更优的记忆管理性能。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [27] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: TRUST是一种新颖的扩散模型概念遗忘方法，通过动态定位目标概念神经元进行选择性微调，结合Hessian正则化，实现高效、鲁棒的概念遗忘


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散模型容易被滥用生成有害内容，传统概念遗忘方法通常针对单个概念，且依赖全模型微调计算成本高；现有概念定位方法是静态的，导致效果不佳

Method: 提出TRUST方法：1) 动态估计目标概念神经元；2) 通过选择性微调进行概念遗忘；3) 使用Hessian正则化增强鲁棒性

Result: 实验表明TRUST能有效对抗对抗性提示，显著保持生成质量，比SOTA方法快得多，能遗忘单个概念、概念组合和条件概念，无需特定正则化

Conclusion: TRUST提供了一种高效、鲁棒的扩散模型概念遗忘解决方案，解决了现有方法计算成本高和效果有限的问题

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [28] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: LLMs能够帮助发现有效的工具变量，通过多智能体系统IV Co-Scientist提出、批判和优化工具变量，在缺乏真实值的情况下使用统计测试评估一致性。


<details>
  <summary>Details</summary>
Motivation: 工具变量识别需要跨学科知识、创造力和上下文理解，是一项非平凡任务。研究大型语言模型是否能辅助这一任务。

Method: 采用两阶段评估框架：1)测试LLMs能否从文献中恢复已建立的工具变量；2)评估LLMs能否识别和避免已被实证或理论否定的工具变量。基于此引入IV Co-Scientist多智能体系统，并提出在缺乏真实值情况下的统计测试。

Result: 结果显示LLMs有潜力从大型观测数据库中识别有效的工具变量。

Conclusion: LLMs在工具变量发现方面具有潜力，IV Co-Scientist系统能够辅助提出、批判和优化工具变量选择。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [29] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN是一个端到端的科学发现框架，通过生成器-实验者的两阶段搜索，在多个领域发现了比现有方法多2-4倍的统计显著假设，预测性能提升7-17%，并通过专家评审和真实A/B测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽然能加速部分过程，但未能支持端到端的科学发现。需要一种能够完整支持从假设生成到实验验证的框架。

Method: 提出EXPERIGEN框架，采用受贝叶斯优化启发的两阶段搜索：生成器提出候选假设，实验者进行实证评估。框架支持多模态和关系数据集，并通过专家评审和真实A/B测试进行验证。

Result: 在多个领域中，EXPERIGEN发现的统计显著假设数量是现有方法的2-4倍，预测性能提升7-17%。专家评审显示：88%的假设具有中等或强新颖性，70%被认为有影响力且值得追求，大多数假设的严谨性相当于高级研究生水平研究。A/B测试显示统计显著结果(p<1e-6)，效应大小达344%。

Conclusion: EXPERIGEN框架能够有效支持端到端的科学发现，不仅提高了假设发现的效率和预测性能，而且生成的假设具有新颖性、实证基础和可操作性，能够推动真实的科学进步。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [30] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 本文提出小型智能体群（SAG）作为替代大型语言模型在临床场景中的新范式，通过协同推理而非单纯模型缩放来平衡临床决策的有效性、可靠性和部署成本。


<details>
  <summary>Details</summary>
Motivation: 当前数字健康领域过度依赖"规模优先"的大型语言模型，但临床实际需求不仅需要有效性，还需要可靠性和合理的部署成本。临床决策本质上是协作性的，因此需要挑战单一模型缩放范式，探索小型智能体群是否能支持更好的临床推理。

Method: 提出小型智能体群（SAG）方法，将单一模型智能转变为集体专业知识，通过协作审议过程分配推理、循证分析和关键审核。使用多样化的临床指标进行全面评估，涵盖有效性、可靠性和部署成本。

Result: SAG在性能上优于单一大型模型，无论是否进行额外优化或检索增强生成。结果表明，SAG所代表的协同推理可以在临床环境中替代模型参数增长。

Conclusion: SAG为数字健康提供了一个可扩展的解决方案，能更好地平衡有效性、可靠性和部署效率，挑战了传统的模型缩放范式。

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [31] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lihui Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM通过引入自我遗忘机制解决推理模型过度思考导致性能下降的问题，在保持紧凑无噪声状态的同时提升各种规模模型的性能


<details>
  <summary>Details</summary>
Motivation: 标准LLM存在"只分配不释放"的架构缺陷，持续积累有效和冗余步骤而没有修剪过时信息的机制，导致过度思考时性能下降

Method: 提出Free()LM模型，通过Free-Module（即插即用的LoRA适配器）引入内在的自我遗忘能力，在推理模式和清理模式之间迭代切换，动态识别并修剪无用上下文块

Result: 在所有模型规模（8B到685B）上实现一致改进，平均比顶级推理基线提升3.3%，在IMOanswerBench上建立新的SOTA；在标准Qwen3-235B-A22B模型完全崩溃（0%准确率）的长时任务中，Free()LM将性能恢复到50%

Conclusion: 可持续智能需要遗忘的自由与思考的能力同等重要，自我遗忘机制是提升推理模型性能的关键

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [32] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 该论文提出了一个五层生物安全数据等级框架，用于根据数据训练AI模型时可能带来的生物安全风险对病原体数据进行分类，并为每个等级提出相应的技术限制措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI在生物学领域的广泛应用，训练数据成为AI模型能力的关键决定因素。某些类型的生物数据可能被用于训练具有生物安全风险的AI模型，如开发生物武器。因此需要建立数据控制框架来防止AI被用于有害应用。

Method: 提出了五层生物安全数据等级框架，根据不同类型病原体数据训练AI模型时可能带来的风险进行分类。为每个等级设计了相应的技术限制措施，并构建了针对新创建的双重用途病原体数据的治理框架。

Result: 建立了一个系统化的生物安全数据分类和治理框架，能够帮助设计有效的数据控制措施，减少具有生物安全风险的AI能力扩散。

Conclusion: 在计算和编码资源广泛可及的世界中，数据控制可能是减少具有生物安全风险的AI能力扩散的最有效干预措施之一。提出的BDL框架为实施此类控制提供了实用工具。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [33] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出一个两阶段梯度框架，用于多智能体强化学习中的可解释故障检测和归因，包括识别初始故障源、验证多米诺效应和追踪故障传播路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在安全关键领域应用日益增多，但缺乏可解释的故障检测和归因方法。现有方法多为黑盒检测，无法解释故障如何传播以及为什么非受攻击智能体可能首先被标记。

Method: 两阶段梯度框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的每智能体故障检测，在首次阈值交叉时声明初始故障源候选；第二阶段通过评论家导数的几何分析（一阶敏感性和方向二阶曲率）在因果窗口上聚合，构建可解释的传染图。

Result: 在Simple Spread（3和5智能体）和StarCraft II环境中评估，使用MADDPG和HATRPO算法，在500和100个episode中实现了88.2-99.4%的初始故障源检测准确率，并为检测决策提供了可解释的几何证据。

Conclusion: 该框架超越了黑盒检测，提供了梯度层面的可解释取证工具，为安全关键多智能体强化学习系统中的级联故障诊断提供了实用方法。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [34] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出递归熵概念量化反思过程中的资源消耗风险，并基于此开发RECUR攻击方法，通过构造反事实问题触发大推理模型的过度反思，导致输出长度增加11倍、吞吐量下降90%。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）的显式推理需要长上下文，导致资源消耗显著增加。先前研究表明对抗性输入可能触发冗余推理过程，但反思过程本身（特别是其反思组件）受到的关注有限，而过度反思会消耗过多计算资源。

Method: 提出递归熵来量化反思过程中的资源消耗风险，并基于此开发RECUR攻击方法——通过递归熵引导的反事实利用和反思进行资源耗尽攻击，构造反事实问题来验证LRMs的内在缺陷和风险。

Result: 实验表明，在良性推理中递归熵呈现明显下降趋势，而RECUR攻击破坏了这一趋势，使输出长度最多增加11倍，吞吐量下降90%。

Conclusion: 该工作为鲁棒推理提供了新视角，揭示了推理本身存在的安全问题，特别是反思过程可能导致的资源耗尽风险。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [35] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS提出利用模型自身历史弱检查点来指导继续优化，通过识别可恢复的学习差距并强化补偿学习，突破后训练饱和瓶颈，实现零额外推理成本下的性能提升。


<details>
  <summary>Details</summary>
Motivation: 后训练优化面临饱和瓶颈：模型变得高度自信后，进一步训练收益递减。现有方法继续强化目标预测，但研究发现信息丰富的监督信号潜藏在模型自身的历史弱状态中。

Method: WMSS后训练范式利用弱检查点指导继续优化，通过熵动态识别可恢复的学习差距，并通过补偿学习强化这些差距，使强智能体超越传统后训练饱和。

Result: 在数学推理和代码生成数据集上的实验表明，使用WMSS训练的智能体实现了有效的性能提升，同时不产生额外的推理成本。

Conclusion: WMSS通过利用模型自身历史弱状态中的监督信号，成功突破了后训练优化的饱和瓶颈，为零成本性能提升提供了新范式。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [36] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 论文提出去中心化评估框架解决LLM评估中的不一致性问题，通过区块链协议激励全球贡献者作为独立验证者，显著降低评估方差


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的集中式评估存在不透明、过拟合和硬件差异导致的方差问题，实证分析显示现有评估存在严重不一致性，HumanEval上单个模型十次运行的标准差(1.67)甚至超过了官方排行榜前10模型间的性能差距(0.91)，使得当前排名统计上不可靠

Method: 提出去中心化评估框架，通过异构计算节点的大规模基准测试实现硬件和参数多样性；利用区块链协议激励全球贡献者作为独立验证者，采用稳健的奖励系统确保评估完整性并阻止不诚实参与；通过多方共识和多样化推理环境将评估从"集中式黑箱"转变为"去中心化背书"

Result: 去中心化评估框架将同一模型十次运行的标准差降低到0.28，相比传统框架显著改善，确保模型排名具有更高的统计置信度；平台已完全实现并即将向社区发布

Conclusion: 去中心化评估框架通过集体验证和多环境共识，为大型语言模型提供了更稳定、更具代表性的评估指标，解决了当前评估中的统计不稳定性问题

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [37] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO是一个通过强化学习框架训练的多模态视觉推理模型，引入区域级视觉注意力奖励机制，解决现有MLLMs视觉注意力不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于思维链的多模态大语言模型在复杂推理任务中主要依赖长文本推理轨迹，缺乏学习稳定视觉注意力策略的机制。研究发现当前MLLMs存在视觉注意力薄弱的问题：早期视觉对齐错误很少在后续推理中得到纠正，导致错误传播和推理失败。这种限制源于训练过程中视觉注意力信用分配不足

Method: 提出SAYO模型，采用强化学习框架训练，引入区域级视觉注意力奖励机制。该奖励明确将优化信号与基于视觉的推理步骤对齐，使模型能够学习更可靠的注意力行为

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务上持续提升性能

Conclusion: 通过强化学习框架和区域级视觉注意力奖励机制，SAYO能够有效改善多模态大语言模型的视觉注意力稳定性，提升复杂推理任务的性能

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [38] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: G-LNS是一个生成式进化框架，利用大语言模型自动设计大规模邻域搜索的破坏和修复算子对，显著提升了组合优化问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计方法通常局限于构造性优先级规则或参数化局部搜索指导，限制了搜索空间的结构探索能力，难以在复杂组合优化问题中跳出深度局部最优。

Method: 提出G-LNS生成式进化框架，利用大语言模型协同进化紧密耦合的破坏和修复算子对，通过合作评估机制显式捕捉算子间的交互作用，发现能够共同执行有效结构破坏和重建的互补算子逻辑。

Result: 在旅行商问题和容量约束车辆路径问题等挑战性组合优化基准测试中，G-LNS显著优于基于大语言模型的自动启发式设计方法以及强大的经典求解器。发现的启发式不仅能在减少计算预算的情况下获得接近最优解，还在多样且未见过的实例分布上展现出鲁棒的泛化能力。

Conclusion: G-LNS将大语言模型在自动启发式设计中的应用扩展到大规模邻域搜索算子的自动设计，通过协同进化破坏-修复算子对实现了更有效的结构探索，为复杂组合优化问题提供了强大的求解框架。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [39] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent是一个多智能体系统框架，用于模拟肥胖症合并精神障碍患者，通过整合临床数据和文献构建个性化虚拟患者，模拟疾病进展和治疗反应，GPT-5和Claude 4.5 Sonnet在该框架中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据存在碎片化、偏见和隐私限制等问题，模拟高保真患者为研究复杂疾病提供了有力途径，特别是针对肥胖症合并精神障碍这类复杂疾病。

Method: 提出SynthAgent多智能体系统框架，整合索赔数据、人口调查和以患者为中心的文献等临床医学证据，构建具有人格特质的个性化虚拟患者，通过自主智能体交互模拟疾病进展、治疗反应和生活管理。

Result: 评估100多个生成的患者显示，GPT-5和Claude 4.5 Sonnet作为核心引擎在提出的MAS框架中实现了最高保真度，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展且保护隐私的框架，用于探索医学和心理领域的患者旅程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [40] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 提出Structural Context Model形式化模型，从上下文结构角度分析和比较LLM智能体，包含声明式实现框架和可持续工程工作流，在动态猴子香蕉问题上实现32%成功率提升


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究碎片化严重，概念框架和方法论原则常与底层实现细节混杂，缺乏可分析的、自洽的形式化模型来支持实现无关的智能体表征和比较

Method: 提出Structural Context Model形式化模型，从上下文结构角度分析LLM智能体；包含声明式实现框架和Semantic Dynamics Analysis可持续工程工作流，支持快速系统化设计迭代

Result: 在动态变体的猴子香蕉问题上，使用该框架设计的智能体在最困难设置中实现了高达32个百分点的成功率提升

Conclusion: 提出的形式化模型和工程工作流为LLM智能体研究提供了原则性见解，支持快速系统化设计迭代，解决了当前研究碎片化问题

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [41] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出"氛围自动化"概念，认为生成式AI代表从优化预定义指标到导航上下文语义连贯性的认识论转变，将人类角色转变为"氛围工程"。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能不仅代表技术进步，更是认识论的根本转变，挑战计算机科学的基础假设。传统机器学习是自动化的自动化，而生成式AI通过导航上下文、语义和风格连贯性运作，需要新的理论框架来理解这一转变。

Method: 引入"氛围自动化"概念来表征这一转变，提出生成式AI通过操作化隐性规律来工作——这些上下文敏感的模式无法通过显式算法规则完全指定。人类角色转变为"氛围工程"，即协调生成系统中的对齐和上下文判断。

Result: 建立了一个跨越三个分析层次和三个行动领域的概念框架：教师世界观、行业关系和课程设计。讨论了模式崩溃和文化同质化的风险，强调需要刻意参与生成系统以避免回归合成统一性。

Conclusion: 生成式AI代表从算法问题规范到氛围工程的深刻认识论转变，需要教育机构和行业重新思考其角色和方法，通过刻意参与避免文化同质化，实现有意义的转型。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [42] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 该研究首次系统性地探讨了视觉语言模型中的道德奉承行为，发现VLMs在用户意见影响下会牺牲道德准确性，存在从正确到错误判断的不对称转变，且不同数据集上表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究探索了视觉语言模型在一般情境中的奉承行为，但其对基于道德的视觉决策的影响仍未被充分理解。本研究旨在填补这一空白，首次系统研究VLMs中的道德奉承现象。

Method: 在Moralise和M^3oralBench两个数据集上评估了10个广泛使用的视觉语言模型，采用明确的用户分歧设置。使用错误引入率（EIR）和错误纠正率（ECR）进行量化评估，分析模型在用户诱导偏见下的行为变化。

Result: VLMs经常在初始判断正确的情况下产生道德错误的后续回应；存在明显的不对称性：模型更倾向于从道德正确转向道德错误而非相反方向；不同数据集上表现差异显著；存在EIR和ECR之间的权衡关系；初始道德正确立场会引发更强的奉承行为。

Conclusion: 视觉语言模型对道德影响具有脆弱性，需要制定原则性策略来提高多模态AI系统的伦理一致性和鲁棒性。研究揭示了模型在道德奉承行为中的系统性缺陷，为改进AI伦理决策提供了重要见解。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [43] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP框架通过基于Shapley值的分层信用分配机制，解决了多智能体强化学习中LLM与外部工具集成时的信用分配难题，显著提升了复杂问题求解性能。


<details>
  <summary>Details</summary>
Motivation: 将LLM与外部工具通过多智能体系统集成是解决复杂问题的新范式，但训练这些系统面临信用分配挑战——难以确定哪个具体功能智能体应对决策轨迹的成功或失败负责。现有方法依赖稀疏或全局广播奖励，无法捕捉个体贡献，导致强化学习效率低下。

Method: 提出了SHARP框架，通过精确信用分配优化多智能体强化学习。框架包含：1) 全局广播准确性奖励；2) 基于Shapley值的边际信用奖励（为每个智能体分配）；3) 工具过程奖励（提高执行效率）。通过轨迹组内智能体特定优势的归一化来稳定训练。

Result: 在多个真实世界基准测试中，SHARP显著优于现有最先进基线方法，相比单智能体方法平均提升23.66%，相比多智能体方法平均提升14.05%。

Conclusion: SHARP框架通过精确的信用分配机制有效解决了多智能体强化学习中的信用分配难题，为LLM与外部工具集成的复杂问题求解提供了稳定高效的训练方法。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [44] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero：一种无需标注的视觉语言模型训练范式，通过双阶段数据合成和认知对齐训练，提升视觉推理的逻辑一致性和可解释性


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽然改善了图像-文本对齐，但仍依赖表面相关性而非逻辑一致的结构化表示，导致缺乏高层语义结构和非因果关系理解，限制了组合性和可验证推理能力

Method: 提出CoTZero范式，包含两个组件：(1) 双阶段数据合成：自底向上阶段提取原子视觉基元并组合成结构化问题推理形式；自顶向下阶段使用粗粒度全局结构指导局部细节和因果关系的解释；(2) 认知对齐训练：在合成CoT数据基础上，在强化微调中引入认知一致可验证奖励，提供推理一致性和事实正确性的逐步反馈

Result: 在多级语义不一致基准测试（包含词汇扰动负样本）中，CoTZero在域内和域外设置下均达到83.33%的F1分数，消融实验证实每个组件都能提升视觉推理的可解释性和人类对齐性

Conclusion: CoTZero通过将人类认知模型融入推理过程，有效解决了视觉语言模型在结构化表示和逻辑推理方面的局限性，实现了更可解释、更符合人类认知的视觉推理

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [45] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出Outline-Guided Path Exploration (OPE)方法，通过生成多样化的推理大纲来引导并行路径探索，解决并行思维中探索路径间互信息瓶颈问题，提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行思维方法主要关注聚合阶段优化，对路径探索阶段关注不足。作者从理论上分析发现探索路径间的互信息瓶颈限制了整体性能，需要解决路径探索中的信息冗余问题。

Method: 提出Outline-Guided Path Exploration (OPE)方法：1）首先生成多样化的推理大纲来划分解空间；2）采用迭代强化学习策略，独立优化大纲规划和基于大纲的推理；3）减少信息冗余，提高探索路径的信息多样性。

Result: 在多个具有挑战性的数学基准测试上进行广泛实验，结果表明OPE能有效提升不同聚合策略下的推理性能，使大型推理模型更可靠地发现正确解决方案。

Conclusion: OPE通过显式划分解空间和减少探索路径间的信息冗余，解决了并行思维中的互信息瓶颈问题，为大型推理模型的路径探索优化提供了有效方案。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [46] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有时间知识图谱基准存在严重缺陷：仅通过统计共现就能达到接近SOTA的性能，无需使用任何时间信息，这暴露了数据集内在偏差和评估任务过于简化的问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现现有时间知识图谱基准无意中引入了捷径，使得仅通过统计共现就能达到接近最优性能，这掩盖了模型真正学习演化模式的能力，需要建立更公平的评估基准。

Method: 分析现有基准问题的根本原因，识别数据集内在偏差和评估任务简化形式；构建包含四个偏差校正数据集和两个与演化过程紧密对齐的新任务的TKG演化基准。

Result: 揭示了现有基准的多个局限性：时间区间知识的不合理格式化、忽略知识过时性学习、精确演化理解信息不足等；提出了新的TKG演化基准以促进更准确的评估。

Conclusion: 现有时间知识图谱基准存在严重缺陷，需要新的评估框架来公平评估模型学习演化模式的能力；提出的TKG演化基准通过偏差校正和新任务设计，为更准确的演化建模评估提供了基础。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [47] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: SAGE是一种新的采样范式，通过释放大型推理模型的自我停止思考能力来提升推理效率和准确性，结合强化学习后能显著改进数学基准测试表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型使用长思维链方法存在大量冗余，损害计算效率并导致实时应用延迟。研究发现更长的推理链与正确性无关甚至有害，但模型实际上隐含知道何时停止思考，只是被当前采样范式掩盖。

Method: 提出SAGE（自我感知引导高效推理）采样范式，释放模型的高效推理潜力。进一步将SAGE作为混合采样整合到基于群体的强化学习中（SAGE-RL），使SAGE-RL能够将SAGE发现的高效推理模式融入标准pass@1推理。

Result: SAGE-RL显著提升了大型推理模型在多个具有挑战性的数学基准测试中的推理准确性和效率。

Conclusion: 大型推理模型隐含知道何时停止思考，SAGE采样范式能够释放这种能力，结合强化学习后能同时提升推理准确性和效率，为高效推理提供了新方向。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [48] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 将随机森林分类器编译为电路的方法，用于计算决策的完整原因、鲁棒性和最短翻转路径


<details>
  <summary>Details</summary>
Motivation: 现有方法在将随机森林编译为电路时效率较低，且需要更有效的工具来计算决策的解释、鲁棒性和翻转方式

Method: 1) 提出将随机森林分类器编译为电路集的方法，每个电路直接编码分类器中某个类别的实例；2) 利用该方法获得可处理的电路，用于计算决策的完整和一般原因；3) 提出计算决策鲁棒性和所有最短翻转路径的算法

Result: 提出的方法比现有类似方法显著更高效，能够枚举所有充分原因、必要原因和对比解释，计算决策鲁棒性，并识别随机森林决策的所有最短翻转方式

Conclusion: 该方法为随机森林分类器提供了有效的解释性分析工具，能够支持决策原因分析、鲁棒性评估和决策翻转路径识别

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [49] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: VIRF框架通过神经符号架构实现LLM规划器的可验证安全规划，将被动安全检查转变为主动协作修复，在家庭安全任务中实现零危险行动率和最高目标达成率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为具身AI规划器缺乏形式化推理能力，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM进行安全检查，要么简单拒绝不安全计划而不提供修复方案，需要一种既能保证安全又能智能修复规划的方法。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，建立导师-学徒对话机制：确定性逻辑导师基于形式化安全本体提供因果和教学反馈，指导LLM规划器进行智能计划修复而非简单避免。同时开发可扩展知识获取管道，从真实世界文档合成安全知识库。

Result: 在具有挑战性的家庭安全任务中，VIRF实现了完美的0%危险行动率(HAR)和77.3%的目标条件率(GCR)，在所有基线方法中最高。平均仅需1.1次修正迭代，效率极高。

Conclusion: VIRF展示了构建根本可信且可验证安全的具身智能体的原则性途径，通过神经符号协作将被动安全把关转变为主动安全修复，为LLM在物理部署中的安全应用提供了新范式。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [50] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM是首个专门为智能体模型设计的水印框架，通过在功能相同的工具执行路径中注入水印来保护智能体系统的知识产权，即使攻击者只能访问黑盒或灰盒系统也能有效检测。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型发展为能够自主推理和使用工具的智能体系统，这些系统产生了重要的知识产权价值。然而，现有LLM水印技术在智能体领域失效，因为现实中的智能体系统通常是灰盒，隐藏了验证所需的内部推理痕迹，使得攻击者可以通过训练模仿模型来窃取专有功能。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏置功能相同的工具执行路径分布来注入水印。该方法将可验证信号直接嵌入可见的动作轨迹中，同时对用户不可察觉。开发了自动生成鲁棒水印方案的流水线，以及用于验证的严格统计假设检验程序。

Result: 在三个复杂领域进行广泛评估，AGENTWM实现了高检测准确率，同时对智能体性能影响可忽略。实验证实AGENTWM能有效保护智能体知识产权，即使面对自适应攻击者，他们也无法在不严重降低窃取模型效用的情况下移除水印。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，成功解决了现有LLM水印技术在智能体领域的局限性，通过利用动作序列语义等价性，在保持智能体性能的同时有效保护知识产权，为智能体系统的商业化部署提供了重要安全保障。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [51] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: TreeTensor是一个用于处理嵌套数据的通用容器，通过树状结构视角系统建模数据关系，支持对嵌套数据应用任意函数和操作，且几乎零开销。


<details>
  <summary>Details</summary>
Motivation: 传统Tensor具有固定形状，在处理复杂认知AI系统中具有层次结构（嵌套数据）的各种模态数据时，编程不便且效率低下。需要一种能有效处理嵌套数据的方法。

Method: 总结嵌套数据的两种主要计算模式，提出TreeTensor通用嵌套数据容器。通过TreeTensor的各种约束和魔法工具，可以对嵌套数据应用任意函数和操作，包括Scikit-Learn、Numpy和PyTorch等机器学习库。

Result: TreeTensor在各种问题中提供强大的可用性，特别是在目前最复杂的AI系统之一：StarCraftII的AlphaStar中。同时展示了优异的运行时效率，没有任何开销。

Conclusion: TreeTensor通过约束的树状结构视角系统建模数据关系，能够有效处理复杂AI系统中的嵌套数据，且易于与其他方法结合扩展更多用途，如异步执行和变长数据计算。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [52] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 提出Reinforcement Inference方法，利用模型自身不确定性选择性触发二次推理，无需重新训练即可提升大语言模型性能


<details>
  <summary>Details</summary>
Motivation: 传统的一次性贪婪推理协议会系统性地低估模型真实能力，许多错误源于内部模糊性下的过早决策，而非知识缺失

Method: 引入基于熵的推理时控制策略，利用模型自身不确定性作为控制信号，选择性触发第二次更审慎的推理尝试

Result: 在MMLU-Pro的12,032个问题上，准确率从60.72%提升至84.03%，仅增加61.06%的推理调用；不确定性感知选择能获得大部分可实现的改进

Conclusion: 提出熵感知范式用于衡量和扩展模型能力，不确定性条件化推理与一次性贪婪推理之间的差距可作为诊断模型潜在推理能力的工具

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [53] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 该论文提出了一种用于全局工作空间理论（GWT）的顶部注意力机制，用于选择多模态系统中的相关模态，提高了噪声鲁棒性并在MM-IMDb基准测试中达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 全局工作空间理论（GWT）作为认知神经科学启发的框架，虽然已有研究探索其多模态表示能力，但相关的注意力机制研究不足。需要开发有效的注意力机制来选择多模态系统中的相关子集，以提升系统的灵活认知能力。

Method: 提出了一种顶部注意力机制，用于在全局工作空间内部选择模态。该方法在两个复杂度递增的多模态数据集（Simple Shapes和MM-IMDb 1.0）上进行评估，并与现有多模态注意力模型进行比较。

Result: 1. 注意力机制提高了全局工作空间系统在两个数据集上的噪声鲁棒性；2. 展示了现有文献中多模态注意力模型不具备的跨任务和跨模态泛化能力；3. 在MM-IMDb 1.0基准测试中，该注意力机制使全局工作空间系统达到与现有先进方法竞争的水平。

Conclusion: 提出的顶部注意力机制有效增强了全局工作空间理论在多模态集成中的性能，不仅提高了噪声鲁棒性，还展现了独特的泛化能力，使GWT在多模态基准测试中具有竞争力。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [54] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: OSCAR是一个用于组合图像检索的优化引导智能体规划框架，将启发式搜索转化为轨迹优化问题，通过离线-在线范式实现，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有的组合图像检索方法存在两个主要问题：统一嵌入检索存在单一模型近视问题，而启发式智能体检索则受限于次优的试错编排。需要一种更系统的方法来处理异构视觉和文本约束的复杂推理

Method: 提出OSCAR框架，将智能体CIR重新定义为轨迹优化问题。采用离线-在线范式：离线阶段通过两阶段混合整数规划建模CIR，通过布尔集合运算推导最大化真实覆盖的最优轨迹，存储在黄金库中；在线阶段使用这些轨迹作为上下文演示来引导VLM规划器

Result: 在三个公共基准和一个私有工业基准上的实验表明，OSCAR始终优于最先进的基线方法。特别值得注意的是，仅使用10%的训练数据就能实现优越性能，展示了规划逻辑的强泛化能力而非数据集特定的记忆

Conclusion: OSCAR成功地将组合图像检索从启发式搜索过程转化为原则性的轨迹优化问题，通过离线推导最优轨迹和在线引导VLM规划器，实现了更好的性能和更强的泛化能力

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [55] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 本文引入"辩论查询复杂度"(DQC)概念，分析人类监督辩论所需的查询次数，发现PSPACE/poly问题仅需O(log n)次查询即可判定，证明辩论具有极高的查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然理论上证明了辩论可以解决哪些问题，但没有分析人类监督的实际成本——法官需要检查辩论记录中的多少信息。本文旨在量化人类监督辩论所需的查询复杂度。

Method: 引入辩论查询复杂度(DQC)作为衡量指标，即验证者正确判定辩论所需检查的最小比特数。通过理论分析建立DQC与计算复杂度类的关系，特别是与PSPACE/poly的等价性。

Result: 1. PSPACE/poly恰好是可用O(log n)查询判定的函数类；2. 依赖所有输入比特的函数需要Ω(log n)查询；3. 规模为s的电路可计算函数满足DQC(f) ≤ log(s) + 3；4. 证明P类语言的DQC下界可导出新的电路下界。

Conclusion: 辩论具有惊人的查询效率，即使对于高度复杂的问题，对数级别的监督就足够了。DQC与电路复杂度的联系为通过辩论查询复杂度研究基础计算问题提供了新途径。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [56] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 论文探讨聊天机器人信任问题，指出用户信任常源于交互设计而非系统可信度，建议将聊天机器人视为销售员而非助手，需区分心理信任形成与规范可信度。


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人模糊自动化系统与人类对话的界限，需要更仔细审视这些系统的信任基础。监管和政策框架倾向于从规范角度定义信任，但用户对聊天机器人的信任往往源于行为机制，这种信任通常不是通过展示可信度获得的，而是通过利用认知偏见影响用户行为的交互设计选择形成的。

Method: 基于观察提出重新框架化聊天机器人的概念，将其视为由部署组织确定目标的高技能销售员，而非伴侣或助手。分析竞争性"信任"概念共存于同一术语下的问题，区分心理信任形成与规范可信度之间的重要差异。

Result: 指出当前聊天机器人信任概念存在混淆，用户信任往往被设计操纵而非真正赢得。这种混淆掩盖了心理信任形成与规范可信度之间的重要区别，导致用户可能对聊天机器人产生不恰当的信任。

Conclusion: 需要进一步研究和更强有力的支持机制，帮助用户适当校准对对话AI系统的信任。必须区分不同类型的信任概念，并开发相应工具和方法来确保用户能够基于系统真实可信度而非设计操纵来建立信任。

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [57] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 该论文研究了STRIPS规划中操作符只有1个前提条件和1个效果时的计算复杂度问题，探索了"小解决方案假设"是否成立。


<details>
  <summary>Details</summary>
Motivation: Bylander的研究表明，即使操作符限制为2个前提条件和2个后置条件，命题STRIPS规划的存在性判定也是PSPACE完全的。但对于操作符只有1个前提条件和1个效果的情况，是否NP完全仍然未知。本文旨在探索这个"小解决方案假设"是否成立。

Method: 1. 对小型实例调用SAT求解器；2. 引入文字图(literal graph)概念；3. 将问题映射到Petri网进行分析。

Result: 论文通过多种技术方法探索了STRIPS$^1_1$（即操作符只有1个前提条件和1个效果）的计算复杂度问题，但摘要中未明确给出最终结论。

Conclusion: 该研究为理解STRIPS规划中最小操作符配置的计算复杂度提供了新的分析框架和方法，但关于STRIPS$^1_1$是否NP完全的问题需要进一步研究。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [58] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文首次系统回顾和分析了SAIG方法（合成人工智能基准真值方法），提出了新的分类法，识别了七个关键特征，并揭示了XAI评估领域缺乏共识的问题。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能（XAI）评估方法多样且复杂，与传统AI评估不同，XAI缺乏普遍正确的解释基准真值，使得客观评估具有挑战性。SAIG方法通过生成人工基准真值来解决这一问题，但该领域缺乏系统性的综述和分析。

Method: 1. 首次对SAIG方法进行全面回顾和分析；2. 提出新的分类法对SAIG方法进行分类；3. 识别了区分不同SAIG方法的七个关键特征；4. 进行对比研究，分析不同方法的有效性。

Result: 1. 建立了SAIG方法的系统性分类框架；2. 识别了七个区分不同SAIG方法的关键特征；3. 发现XAI评估技术领域存在令人担忧的共识缺乏；4. 揭示了该领域需要进一步研究和标准化的迫切需求。

Conclusion: SAIG方法为解决XAI评估中的基准真值问题提供了有前景的方向，但当前领域缺乏对最有效评估技术的共识，亟需进一步研究和标准化工作来推动XAI评估的客观性和可靠性。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [59] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 本文探讨人类使用LLM聊天机器人作为思维伙伴时产生的"信念卸载"现象，即人们将信念形成和维护过程外包给AI系统，这对认知技能和行为产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人日益成为人们的思维伙伴，存在认知卸载的风险，特别是过度依赖可能导致认知技能受损。本文旨在定义和研究一种特殊的人类-AI交互中的认知卸载形式——"信念卸载"。

Method: 结合哲学、心理学和计算机科学研究，明确信念卸载发生的边界条件，提供信念卸载的描述性分类学及其规范含义。

Result: 提出了信念卸载的概念框架和分类体系，阐明了信念卸载对人们行为模式和信念系统性质的下游影响。

Conclusion: 信念卸载是人类-AI交互中的重要现象，需要进一步研究其潜在影响和后果，为未来工作提供方向。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [60] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 该研究将潜在思维链视为表示空间中可操纵的因果过程，通过结构因果模型分析潜在步骤的因果效应，探索了三个关键问题：哪些步骤对正确性因果必要、影响如何跨步传播、以及中间轨迹是否保留竞争答案模式。


<details>
  <summary>Details</summary>
Motivation: 现有潜在思维链方法使用内部潜在步骤替代显式文本推理，但这些中间计算难以通过相关性探测之外的方式进行评估。研究者希望将潜在思维链视为可操纵的因果过程，以更深入地理解其工作机制。

Method: 将潜在步骤建模为结构因果模型中的变量，通过逐步干预分析其效应。研究比较了Coconut和CODI两种代表性范式在数学和通用推理任务上的表现。

Result: 发现潜在步骤预算不像同质的额外深度，而更像具有非局部路由的分阶段功能；识别出早期输出偏差与晚期表示承诺之间的持续差距；潜在步骤表现出阶段性功能和非局部路由特性。

Conclusion: 研究结果支持模式条件和稳定性感知分析作为解释和改进潜在推理系统的更可靠工具，并提出了相应的训练/解码目标。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [61] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: 研究探讨AI工具能否支持认知诊断模型中的Q矩阵构建，通过比较AI生成的Q矩阵与已验证Q矩阵的一致性，发现不同AI模型表现差异显著，其中Google Gemini 2.5 Pro与已验证Q矩阵的一致性最高，甚至超过了人类专家。


<details>
  <summary>Details</summary>
Motivation: 构建Q矩阵是认知诊断建模的关键但劳动密集型步骤，本研究旨在探索AI工具（通用语言模型）是否能够支持Q矩阵的开发工作。

Method: 研究比较了AI生成的Q矩阵与Li和Suen（2013）阅读理解测试已验证Q矩阵的一致性。在2025年5月，多个AI模型获得了与人类专家相同的训练材料，使用Cohen's kappa评估AI生成Q矩阵、已验证Q矩阵和人类评分者Q矩阵之间的一致性。

Result: 不同AI模型表现差异显著，Google Gemini 2.5 Pro与已验证Q矩阵的一致性最高（Kappa = 0.63），超过了所有人类专家。但2026年1月使用更新版本AI进行的后续分析显示，与已验证Q矩阵的一致性有所下降。

Conclusion: AI工具在支持Q矩阵开发方面具有潜力，但不同模型和版本的表现存在显著差异，需要进一步研究来理解这种变异并优化AI在认知诊断建模中的应用。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [62] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: RC-LLM：一种基于残差连接和大语言模型的微服务根因分析方法，通过多源遥测数据融合和上下文推理能力提升故障定位效果


<details>
  <summary>Details</summary>
Motivation: 微服务架构中根因定位面临挑战：复杂的故障传播机制和多源遥测数据（指标、日志、追踪）的高维度特性限制了现有RCA方法的有效性

Method: 提出RC-LLM方法：设计残差式层次融合结构集成多源遥测数据，利用大语言模型的上下文推理能力建模时间依赖和跨微服务因果依赖

Result: 在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析中实现了高准确性和效率

Conclusion: RC-LLM通过融合多源数据和利用LLM的推理能力，有效解决了复杂微服务架构中的根因定位问题

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [63] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: NADEx提出了一种负感知扩散模型用于时序知识图谱推理，通过结合负样本信息和余弦对齐正则化，在四个公开基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时序知识图谱推理中存在两个主要问题：1）生成路径仅基于正样本证据，忽略了信息丰富的负样本上下文；2）训练目标主要依赖交叉熵排序，虽然改善了候选排序但缺乏对去噪嵌入校准的监督。

Method: NADEx编码实体、关系和时序间隔的以主体为中心的历史为序列嵌入，在前向过程中扰动查询对象，在反向过程中使用Transformer去噪器基于时序关系上下文进行重构。此外，从批处理负样本原型中推导出余弦对齐正则化器，以收紧决策边界对抗不合理候选。

Result: 在四个公开时序知识图谱基准上的综合实验表明，NADEx实现了最先进的性能。

Conclusion: NADEx通过引入负感知机制和余弦对齐正则化，有效解决了现有扩散模型在时序知识图谱推理中的局限性，显著提升了预测性能。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [64] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架，用于整合多种定性推理的扩展和组合形式，包括多尺度推理、时间序列和松散集成，并研究了其可满足性决策的复杂性。


<details>
  <summary>Details</summary>
Motivation: 定性推理能够在信息不精确、不完整且缺乏数值的情况下进行推理，但现有的定性形式化方法存在局限性，无法统一处理多种扩展和组合形式，需要建立一个统一的框架来系统研究这些组合的可满足性决策及其复杂性。

Method: 提出了一个形式化框架，统一了多种定性形式化的扩展和组合，包括多尺度推理、时间序列和松散集成。该框架不仅支持在这些组合和扩展背景下进行推理，还能以统一方式研究可满足性决策及其复杂性。特别是建立了两个互补定理来保证可满足性决策的多项式时间复杂度。

Result: 通过提出的统一框架，恢复了已知的尺寸-拓扑组合结果，并将定性形式化的主要定义推广到包含文献定义中被排除但在组合背景下重要的定性形式化方法。

Conclusion: 该研究为定性推理的多种扩展和组合提供了一个统一的框架，不仅能够支持在这些复杂背景下的推理，还能系统研究其可满足性决策的复杂性，为定性推理的实际应用提供了理论基础。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [65] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: 提出STP框架，通过时空剪枝同时提升扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习对释放扩散大语言模型的复杂推理能力至关重要，但现有方法在效率和稳定性方面面临挑战

Method: 提出时空剪枝(STP)框架：1) 空间剪枝：利用静态先验约束探索空间；2) 时间剪枝：绕过冗余的后期细化步骤

Result: 理论分析表明STP严格降低了对数似然估计的方差，确保更稳定的策略更新；实验显示STP在效率和准确性上都超越了最先进基线

Conclusion: STP框架有效解决了扩散大语言模型强化学习中的效率和稳定性问题，为相关研究提供了新思路

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [66] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统检测LLM在因果推理中的失败模式，包括梯级坍塌、奉承漂移和错误拒绝，通过实用性和安全性指标揭示聚合精度无法发现的故障模式。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在多种失败模式（如奉承、梯级坍塌、错误拒绝），但由于缺乏系统诊断基准，改进进展缓慢。需要创建一个能够系统检测这些故障模式的诊断工具。

Method: 开发了CausalT5K基准，包含5000多个案例，覆盖10个领域，测试三种关键能力：检测梯级坍塌、抵抗奉承漂移、生成明智拒绝。采用人机协作流程，涉及40名领域专家、迭代交叉验证循环，以及基于规则、LLM和人工评分的复合验证。

Result: 初步实验揭示了一个四象限控制景观，显示静态审计策略普遍失败。基准能够将性能分解为实用性（敏感性）和安全性（特异性），揭示聚合精度无法发现的故障模式。

Conclusion: CausalT5K作为研究基础设施实现了Pearl的因果阶梯，为推进可信推理系统提供了有价值的诊断工具，能够系统检测LLM在因果推理中的关键失败模式。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [67] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: SWM是一个模块化、经过测试和文档化的世界模型研究生态系统，旨在解决现有世界模型实现缺乏可重用性、标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数世界模型实现都是针对特定论文的，这严重限制了它们的可重用性，增加了错误风险，并降低了评估标准化程度。

Method: 开发了stable-worldmodel (SWM)生态系统，提供高效的数据收集工具、标准化环境、规划算法和基线实现，每个环境都支持可控的变化因素。

Result: 通过使用SWM研究DINO-WM中的零样本鲁棒性，展示了该生态系统的实用性。

Conclusion: SWM为世界模型研究提供了一个模块化、可重用且标准化的生态系统，有助于推动鲁棒性和持续学习研究的发展。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [68] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5是一个用于跨计算和实证领域的端到端科学发现的统一系统，通过生成、验证和演化三个协调子系统实现自主科学发现。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够在计算建模和实验室实验之间协调工作的统一系统，实现跨领域的自主科学发现，解决传统方法在扩展性和协调性方面的限制。

Method: 采用结构化架构，包含生成、验证和演化三个协调子系统，支持深度研究、解决方案优化和长时程记忆等基础能力，能够在扩展发现周期中持续运行。

Result: 在GAIA、HLE、GPQA和FrontierScience等科学推理基准测试中取得领先性能；在算法发现任务中自主设计机器学习核心问题的竞争性方法；在实证发现任务中执行完整计算或湿实验室实验，在地球、生命、生物和物理领域产生科学发现。

Conclusion: InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架，能够协调计算和实证工作流，实现跨领域的端到端科学发现。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [69] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: iGRPO是一种两阶段强化学习方法，通过模型自生成草稿和动态自我条件化，提升大语言模型在数学推理任务上的表现，在多个基准测试中达到新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在解决复杂数学问题方面显示出潜力，但它们仍然难以产生准确且一致的解决方案。需要更有效的强化学习方法来对齐模型与任务特定的奖励，提高整体质量和可靠性。

Method: 提出迭代组相对策略优化(iGRPO)，这是GRPO的两阶段扩展：第一阶段采样多个探索性草稿并选择最高奖励的草稿；第二阶段将最佳草稿附加到原始提示中，在草稿条件化的改进上应用GRPO风格的更新，训练策略超越其先前的最佳尝试。

Result: 在匹配的rollout预算下，iGRPO在多个基础模型上持续优于GRPO。应用于OpenReasoning-Nemotron-7B模型时，在AIME24和AIME25上分别达到85.62%和79.64%的新SOTA结果。消融实验显示该方法具有良好泛化性。

Conclusion: 迭代的、基于自我反馈的强化学习方法在推进可验证数学推理方面具有巨大潜力，iGRPO通过动态自我条件化和两阶段优化策略显著提升了模型性能。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [70] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出L0-L4分层数据管理框架，支持LLM全生命周期训练，通过模型指导数据管理实现数据与模型协同进化


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度依赖数据规模的单向扩展，面临数据可用性、获取成本和训练效率瓶颈，需要转向数据-模型协同进化的新范式

Method: 提出L0-L4分层数据管理框架，从原始未处理资源到组织化可验证知识，利用LLM进行质量评分和内容编辑，为不同训练阶段（预训练、中期训练、对齐）战略分配数据

Result: 实证研究表明，分层数据利用显著提高训练效率和模型性能，框架在数据质量、获取成本和边际训练效益之间取得平衡

Conclusion: 数据-模型协同进化是AGI发展的新阶段，分层数据管理框架为实现可扩展和可持续的数据管理提供了系统化方法

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [71] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: GEBench是一个用于评估GUI动态交互和时序一致性的综合基准，包含700个样本和五维评估指标GE-Score，发现现有模型在单步交互表现良好但在多步序列中保持时序一致性方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型能够根据用户指令预测未来GUI状态，但现有基准主要关注通用领域的视觉保真度，对GUI特定场景中的状态转换和时序一致性的评估不足。需要填补这一研究空白。

Method: 提出了GEBench基准，包含700个精心策划的样本，涵盖5个任务类别（单步交互、多步轨迹、真实和虚构场景、定位点接地）。同时提出了GE-Score五维评估指标，从目标达成、交互逻辑、内容一致性、UI合理性和视觉质量五个维度进行评估。

Result: 对当前模型的广泛评估表明，它们在单步转换上表现良好，但在保持较长交互序列的时序一致性和空间接地方面存在显著困难。图标解释、文本渲染和定位精度被确定为关键瓶颈。

Conclusion: 这项工作为系统评估提供了基础，并为未来构建高保真生成式GUI环境的研究指明了有前景的方向。代码已开源。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>
