{"id": "2602.09112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09112", "abs": "https://arxiv.org/abs/2602.09112", "authors": ["Russ Webb", "Jason Ramapuram"], "title": "A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation", "comment": null, "summary": "What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \\$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\\% accuracy while GPT-5 has 95\\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86Cadmus\u7cfb\u7edf\uff0c\u5305\u542b\u6574\u6570\u865a\u62df\u673a\u3001\u591a\u6837\u5316\u771f\u5b9e\u7a0b\u5e8f\u6570\u636e\u96c6\u548c\u4f4e\u6210\u672c\u8bad\u7ec3\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u7814\u7a76\u7a0b\u5e8f\u5408\u6210\u3001\u5206\u5e03\u5916\u8868\u793a\u3001\u5f52\u7eb3\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\uff0c\u76f8\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u66f4\u900f\u660e\u53ef\u63a7\u7684\u7814\u7a76\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u7a0b\u5e8f\u5408\u6210\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b58\u5728\u5206\u5e03\u8303\u56f4\u4e0d\u660e\u786e\u3001\u5fae\u8c03\u6548\u679c\u96be\u7406\u89e3\u3001\u5206\u8bcd\u5f71\u54cd\u4e0d\u6e05\u695a\u3001\u8ba1\u7b97\u5b58\u50a8\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002\u9700\u8981\u5c0f\u578b\u3001\u900f\u660e\u3001\u53ef\u63a7\u7684\u7814\u7a76\u7cfb\u7edf\u6765\u6df1\u5165\u7406\u89e3\u7a0b\u5e8f\u5408\u6210\u7684\u57fa\u672c\u673a\u5236\u3002", "method": "\u5f00\u53d1Cadmus\u7cfb\u7edf\uff1a1) \u6574\u6570\u865a\u62df\u673a\u4f5c\u4e3a\u6267\u884c\u73af\u5883\uff1b2) \u5305\u542b\u591a\u6837\u5316\u4efb\u52a1\u7684\u771f\u5b9e\u7a0b\u5e8f\u6570\u636e\u96c6\uff1b3) \u4f7f\u7528\u81ea\u56de\u5f52Transformer\u6a21\u578b\uff0c\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e200\u7f8e\u5143\u3002\u7cfb\u7edf\u63d0\u4f9b\u5bf9\u8bad\u7ec3\u5206\u5e03\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u6a21\u578b\u68c0\u67e5\u80fd\u529b\u3002", "result": "Cadmus\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u8bed\u8a00\uff08DSL\uff09\u7684\u6574\u6570\u7b97\u672f\u7a0b\u5e8f\u8865\u5168\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-5\uff08100%\u51c6\u786e\u7387 vs 95%\u51c6\u786e\u7387\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u4e0e\u95ee\u9898\u5173\u7cfb\u7684\u900f\u660e\u5ea6\uff0c\u800cGPT-5\u5728\u89e3\u51b3\u76f8\u540c\u4efb\u52a1\u65f6\u5f15\u5165\u4e86\u672a\u77e5\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u6210\u4e3a\u67d0\u4e9b\u7814\u7a76\u7684\u6df7\u6742\u56e0\u7d20\u3002", "conclusion": "\u5c0f\u578b\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u80fd\u591f\u5b9e\u73b0\u5927\u578b\u6a21\u578b\u96be\u4ee5\u8fdb\u884c\u7684\u4eea\u5668\u5316\u548c\u6df1\u5165\u8c03\u67e5\u3002Cadmus\u7cfb\u7edf\u4e3a\u7a0b\u5e8f\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u63a7\u3001\u7ecf\u6d4e\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b8c\u5168\u7406\u89e3\u8bad\u7ec3\u96c6\u4e0e\u4efb\u52a1\u5173\u7cfb\u7684\u7814\u7a76\u573a\u666f\u3002"}}
{"id": "2602.09138", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09138", "abs": "https://arxiv.org/abs/2602.09138", "authors": ["Haitao Jiang", "Lin Ge", "Hengrui Cai", "Rui Song"], "title": "PABU: Progress-Aware Belief Update for Efficient LLM Agents", "comment": null, "summary": "Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.", "AI": {"tldr": "PABU\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u8fdb\u5ea6\u548c\u9009\u62e9\u6027\u4fdd\u7559\u5386\u53f2\u4fe1\u606f\uff0c\u51cf\u5c11LLM\u667a\u80fd\u4f53\u4e2d\u7684\u5197\u4f59\u52a8\u4f5c\u548c\u63a8\u7406\u6210\u672c\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfLLM\u667a\u80fd\u4f53\u57fa\u4e8e\u5b8c\u6574\u52a8\u4f5c-\u89c2\u5bdf\u5386\u53f2\u8fdb\u884c\u51b3\u7b56\uff0c\u8fd9\u5f15\u5165\u4e86\u5927\u91cf\u4efb\u52a1\u65e0\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u5197\u4f59\u52a8\u4f5c\u548c\u66f4\u9ad8\u7684\u63a8\u7406\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7d27\u51d1\u7684\u72b6\u6001\u8868\u793a\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51faProgress-Aware Belief Update (PABU)\u4fe1\u5ff5\u72b6\u6001\u6846\u67b6\uff1a1) \u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u8fdb\u5ea6\uff0c\u9884\u6d4b\u76f8\u5bf9\u8fdb\u5ea6\u53d8\u5316\uff1b2) \u9009\u62e9\u6027\u4fdd\u7559\u8fc7\u53bb\u7684\u52a8\u4f5c\u548c\u89c2\u5bdf\uff1b3) \u57fa\u4e8e\u4fdd\u7559\u7684\u5b50\u96c6\u8fdb\u884c\u672a\u6765\u51b3\u7b56\uff0c\u51cf\u5c11\u65e0\u5173\u4fe1\u606f\u5e72\u6270\u3002", "result": "\u5728AgentGym\u57fa\u51c6\u76848\u4e2a\u73af\u5883\u4e2d\uff0cPABU\u8fbe\u523081.0%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u6bd4\u57fa\u4e8e\u5b8c\u6574\u5386\u53f2\u7684SOTA\u6a21\u578b\u63d0\u534723.9%\u3002\u5e73\u5747\u4ea4\u4e92\u6b65\u9aa4\u51cf\u5c11\u52309.5\u6b65\uff0c\u5bf9\u5e9426.9%\u7684\u51cf\u5c11\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u8fdb\u5ea6\u9884\u6d4b\u548c\u9009\u62e9\u6027\u4fdd\u7559\u5bf9\u6027\u80fd\u63d0\u5347\u90fd\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "PABU\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u8fdb\u5ea6\u548c\u9009\u62e9\u6027\u4fe1\u606f\u4fdd\u7559\uff0c\u6709\u6548\u51cf\u5c11\u4e86LLM\u667a\u80fd\u4f53\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u5904\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.09159", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.09159", "abs": "https://arxiv.org/abs/2602.09159", "authors": ["Yichen Wu", "Yujin Oh", "Sangjoon Park", "Kailong Fan", "Dania Daye", "Hana Farzaneh", "Xiang Li", "Raul Uppot", "Quanzheng Li"], "title": "CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective", "comment": "9 pages, 3 figures", "summary": "Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.", "AI": {"tldr": "CoMMa\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u533b\u7597\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u76ee\u6807\u548c\u786e\u5b9a\u6027\u5d4c\u5165\u6295\u5f71\u5b9e\u73b0\u8d21\u732e\u611f\u77e5\u7684\u4fe1\u7528\u5206\u914d\uff0c\u5728\u80bf\u7624\u5b66\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u5904\u7406\u9700\u8981\u52a8\u6001\u3001\u5f02\u6784\u60a3\u8005\u6570\u636e\u63a8\u7406\u7684\u80bf\u7624\u5b66\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5927\u591a\u6570\u667a\u80fd\u4f53\u67b6\u6784\u4f9d\u8d56\u968f\u673a\u53d9\u4e8b\u63a8\u7406\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u8bc1\u636e\u5f52\u56e0\u548c\u7a33\u5b9a\u7684\u51b3\u7b56\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u8d21\u732e\u611f\u77e5\u533b\u7597\u591a\u667a\u80fd\u4f53(CoMMa)\u6846\u67b6\uff0c\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316LLM\u667a\u80fd\u4f53\u7ed3\u6784\uff0c\u4e13\u5bb6\u5728\u5206\u533a\u8bc1\u636e\u4e0a\u64cd\u4f5c\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u76ee\u6807\u8fdb\u884c\u534f\u8c03\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u5d4c\u5165\u6295\u5f71\u6765\u8fd1\u4f3c\u8d21\u732e\u611f\u77e5\u7684\u4fe1\u7528\u5206\u914d\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u8fb9\u9645\u6548\u7528\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u80bf\u7624\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u591a\u5b66\u79d1\u80bf\u7624\u59d4\u5458\u4f1a\u6570\u636e\u96c6\uff09\u4e2d\uff0cCoMMa\u6bd4\u6570\u636e\u96c6\u4e2d\u5316\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "CoMMa\u6846\u67b6\u901a\u8fc7\u8d21\u732e\u611f\u77e5\u7684\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u89e3\u91ca\u4e14\u6570\u5b66\u57fa\u7840\u624e\u5b9e\u7684\u51b3\u7b56\u8def\u5f84\uff0c\u5728\u80bf\u7624\u5b66\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.09163", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.09163", "abs": "https://arxiv.org/abs/2602.09163", "authors": ["Xingjian Zhang", "Sophia Moylan", "Ziyang Xiong", "Qiaozhu Mei", "Yichen Luo", "Jiaqi W. Ma"], "title": "FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases", "comment": null, "summary": "Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.", "AI": {"tldr": "FlyBench\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u667a\u80fd\u4f53\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u8fdb\u884c\u7aef\u5230\u7aef\u672c\u4f53\u8bba\u7b56\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u57fa\u4e8e\u57fa\u56e0\u7b26\u53f7\u4ece16,898\u7bc7\u5168\u6587\u8bba\u6587\u4e2d\u641c\u7d22\u548c\u9605\u8bfb\uff0c\u751f\u6210\u7ed3\u6784\u5316\u6ce8\u91ca\uff0c\u5305\u62ec\u57fa\u56e0\u672c\u4f53\u672f\u8bed\u3001\u8868\u8fbe\u6a21\u5f0f\u548c\u5386\u53f2\u540c\u4e49\u8bcd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6216\u5173\u7cfb\u63d0\u53d6\u7b49\u5b64\u7acb\u5b50\u4efb\u52a1\uff0c\u65e0\u6cd5\u6355\u6349\u79d1\u5b66\u77e5\u8bc6\u5e93\u7ef4\u62a4\u6240\u9700\u7684\u7aef\u5230\u7aef\u7b56\u5c55\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9700\u8981\u4e13\u5bb6\u7b56\u5c55\u4eba\u641c\u7d22\u76f8\u5173\u8bba\u6587\u3001\u8de8\u6587\u6863\u534f\u8c03\u8bc1\u636e\u5e76\u751f\u6210\u57fa\u4e8e\u672c\u4f53\u7684\u6ce8\u91ca\u3002", "method": "\u5f00\u53d1\u4e86FlyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b7,397\u4e2a\u4e13\u5bb6\u7b56\u5c55\u7684\u6ce8\u91ca\uff0c\u6db5\u76d6100\u4e2a\u57fa\u56e0\uff0c\u6765\u81ea\u679c\u8747\u77e5\u8bc6\u5e93FlyBase\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u7ebf\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u8bb0\u5fc6\u5316\u3001\u56fa\u5b9a\u6d41\u7a0b\u3001\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u57fa\u4e8e\u57fa\u56e0\u7b26\u53f7\u4ece16,898\u7bc7\u5168\u6587\u8bba\u6587\u4e2d\u641c\u7d22\u548c\u9605\u8bfb\uff0c\u751f\u6210\u7ed3\u6784\u5316\u6ce8\u91ca\u3002", "result": "\u67b6\u6784\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u4f18\u4e8e\u7b80\u5355\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u6269\u5c55\u9aa8\u5e72\u6a21\u578b\u5e26\u6765\u7684\u6536\u76ca\u9012\u51cf\u3002\u6240\u6709\u57fa\u7ebf\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u5206\u6790\u53d1\u73b0\u667a\u80fd\u4f53\u4e3b\u8981\u4f7f\u7528\u68c0\u7d22\u6765\u786e\u8ba4\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u53d1\u73b0\u65b0\u4fe1\u606f\u3002", "conclusion": "FlyBench\u5c06\u63a8\u52a8\u68c0\u7d22\u589e\u5f3a\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u8fd9\u79cd\u80fd\u529b\u5728\u79d1\u5b66\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524dAI\u667a\u80fd\u4f53\u5728\u7aef\u5230\u7aef\u79d1\u5b66\u672c\u4f53\u7b56\u5c55\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2602.09340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09340", "abs": "https://arxiv.org/abs/2602.09340", "authors": ["Yang Ba", "Mohammad Sadeq Abolhasani", "Michelle V Mancenido", "Rong Pan"], "title": "Measuring Dataset Diversity from a Geometric Perspective", "comment": null, "summary": "Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u548c\u6301\u4e45\u6027\u666f\u89c2\uff08PLs\uff09\u7684\u51e0\u4f55\u591a\u6837\u6027\u5ea6\u91cf\u6846\u67b6PLDiv\uff0c\u8d85\u8d8a\u4f20\u7edf\u71b5\u548c\u5206\u5e03\u53d8\u5f02\uff0c\u76f4\u63a5\u91cf\u5316\u6570\u636e\u96c6\u7684\u51e0\u4f55\u7ed3\u6784\u7279\u5f81", "motivation": "\u73b0\u6709\u591a\u6837\u6027\u5ea6\u91cf\u4e3b\u8981\u5173\u6ce8\u7279\u5f81\u7a7a\u95f4\u7684\u5206\u6563\u5ea6\u6216\u5ea6\u91cf\u7a7a\u95f4\u7684\u5e45\u5ea6\uff0c\u4e3b\u8981\u6355\u6349\u5206\u5e03\u53d8\u5f02\u6216\u71b5\uff0c\u4f46\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86\u6570\u636e\u96c6\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u6570\u636e\u4e30\u5bcc\u51e0\u4f55\u548c\u7ed3\u6784\u7279\u6027\u7684\u591a\u6837\u6027\u5ea6\u91cf\u65b9\u6cd5", "method": "\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u6301\u4e45\u6027\u666f\u89c2\u6846\u67b6\uff0c\u4ece\u6570\u636e\u4e2d\u63d0\u53d6\u548c\u91cf\u5316\u51e0\u4f55\u7279\u5f81\uff0c\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u7684\u591a\u6837\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u591a\u6837\u6027\u4e0e\u5176\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u76f4\u63a5\u5173\u8054", "result": "\u901a\u8fc7\u8de8\u591a\u79cd\u6a21\u6001\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u63d0\u51fa\u7684PLs-based\u591a\u6837\u6027\u5ea6\u91cf\uff08PLDiv\uff09\u662f\u5f3a\u5927\u3001\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\uff0c\u80fd\u591f\u76f4\u63a5\u8fde\u63a5\u6570\u636e\u591a\u6837\u6027\u4e0e\u5176\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784", "conclusion": "PLDiv\u4e3a\u6570\u636e\u96c6\u6784\u5efa\u3001\u589e\u5f3a\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u5de5\u5177\uff0c\u901a\u8fc7\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u6570\u636e\u96c6\u51e0\u4f55\u7ed3\u6784\u591a\u6837\u6027\u7684\u91cf\u5316\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u71b5\u57fa\u5ea6\u91cf\u7684\u5c40\u9650\u6027"}}
{"id": "2602.09341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09341", "abs": "https://arxiv.org/abs/2602.09341", "authors": ["Wei Yang", "Shixuan Li", "Heng Ping", "Peiyu Zhang", "Paul Bogdan", "Jesse Thomason"], "title": "Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge", "comment": null, "summary": "Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.", "AI": {"tldr": "AgentAuditor\u6846\u67b6\u53d6\u4ee3\u591a\u6570\u6295\u7968\uff0c\u901a\u8fc7\u63a8\u7406\u6811\u8def\u5f84\u641c\u7d22\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5171\u8bc6\u9519\u8bef\u95ee\u9898\uff0c\u7ed3\u5408ACPO\u8bad\u7ec3\u63d0\u5347\u5c11\u6570\u6b63\u786e\u9009\u62e9\u7684\u8bc6\u522b\u80fd\u529b", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u591a\u91c7\u7528\u591a\u6570\u6295\u7968\u805a\u5408\u667a\u80fd\u4f53\u8f93\u51fa\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e22\u5f03\u4e86\u63a8\u7406\u8f68\u8ff9\u7684\u8bc1\u636e\u7ed3\u6784\uff0c\u5728\u667a\u80fd\u4f53\u5171\u4eab\u76f8\u5173\u504f\u89c1\u5e76\u6536\u655b\u5230\u76f8\u540c\u9519\u8bef\u63a8\u7406\u7684\"\u5e7b\u89c9\u5171\u8bc6\"\u60c5\u51b5\u4e0b\u8868\u73b0\u8106\u5f31", "method": "\u5f15\u5165AgentAuditor\u6846\u67b6\uff0c\u7528\u63a8\u7406\u6811\u8def\u5f84\u641c\u7d22\u66ff\u4ee3\u6295\u7968\uff0c\u663e\u5f0f\u8868\u793a\u667a\u80fd\u4f53\u8f68\u8ff9\u95f4\u7684\u5171\u8bc6\u4e0e\u5206\u6b67\uff1b\u63d0\u51fa\u53cd\u5171\u8bc6\u504f\u597d\u4f18\u5316(ACPO)\uff0c\u5728\u591a\u6570\u5931\u8d25\u6848\u4f8b\u4e0a\u8bad\u7ec3\u88c1\u51b3\u5668\uff0c\u5956\u52b1\u57fa\u4e8e\u8bc1\u636e\u7684\u5c11\u6570\u9009\u62e9\u800c\u975e\u6d41\u884c\u9519\u8bef", "result": "\u57285\u4e2a\u6d41\u884c\u8bbe\u7f6e\u4e2d\uff0cAgentAuditor\u76f8\u6bd4\u591a\u6570\u6295\u7968\u83b7\u5f97\u9ad8\u8fbe5%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u76f8\u6bd4\u4f7f\u7528LLM-as-Judge\u83b7\u5f97\u9ad8\u8fbe3%\u7684\u63d0\u5347", "conclusion": "AgentAuditor\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u63a8\u7406\u5206\u6b67\u548c\u5c40\u90e8\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5171\u8bc6\u9519\u8bef\u95ee\u9898\uff0c\u4e14\u4e0e\u5177\u4f53MAS\u8bbe\u7f6e\u65e0\u5173"}}
{"id": "2602.09347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09347", "abs": "https://arxiv.org/abs/2602.09347", "authors": ["Jana G. Delfino", "Jason L. Granstedt", "Frank W. Samuelson", "Robert Ochs", "Krishna Juluru"], "title": "Image Quality in the Era of Artificial Intelligence", "comment": "16 pages, 3 figures", "summary": "Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.", "AI": {"tldr": "AI\u5728\u653e\u5c04\u5b66\u56fe\u50cf\u91cd\u5efa\u548c\u589e\u5f3a\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u80fd\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5de5\u4f5c\u6548\u7387\uff0c\u4f46\u4e5f\u5e26\u6765\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\u548c\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\u4e0e\u4fe1\u606f\u5185\u5bb9\u8131\u8282\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e86\u89e3\u5176\u5c40\u9650\u6027\u4ee5\u786e\u4fdd\u5b89\u5168\u6709\u6548\u4f7f\u7528\u3002", "motivation": "AI\u5728\u653e\u5c04\u5b66\u4e2d\u5feb\u901f\u90e8\u7f72\uff0c\u867d\u7136\u80fd\u663e\u8457\u6539\u5584\u56fe\u50cf\u8d28\u91cf\u548c\u5de5\u4f5c\u6548\u7387\uff0c\u4f46\u540c\u65f6\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u53ef\u80fd\u52a0\u5267\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\u4e0e\u5b9e\u9645\u4fe1\u606f\u5185\u5bb9\u4e4b\u95f4\u7684\u8131\u8282\u3002\u7406\u89e3AI\u56fe\u50cf\u91cd\u5efa\u548c\u589e\u5f3a\u7684\u5c40\u9650\u6027\u5bf9\u4e8e\u5b89\u5168\u6709\u6548\u4f7f\u7528\u8be5\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u662f\u4e00\u7bc7\u901a\u8baf\u6587\u7ae0\uff0c\u65e8\u5728\u63d0\u9ad8\u4eba\u4eec\u5bf9AI\u5728\u653e\u5c04\u5b66\u56fe\u50cf\u91cd\u5efa\u548c\u589e\u5f3a\u4e2d\u5c40\u9650\u6027\u7684\u8ba4\u8bc6\uff0c\u901a\u8fc7\u5206\u6790AI\u6280\u672f\u7684\u6f5c\u5728\u98ce\u9669\u548c\u5931\u8d25\u6a21\u5f0f\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u7406\u89e3\u5982\u4f55\u5e73\u8861\u6280\u672f\u4f18\u52bf\u4e0e\u98ce\u9669\u3002", "result": "AI\u5728\u653e\u5c04\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u786e\u5b9e\u80fd\u4ea7\u751f\u66f4\u6e05\u6670\u3001\u66f4\u5e73\u6ed1\u3001\u66f4\u8be6\u7ec6\u7684\u56fe\u50cf\uff0c\u52a0\u5feb\u56fe\u50cf\u83b7\u53d6\u901f\u5ea6\uff0c\u5e76\u8ba9\u4e34\u5e8a\u533b\u751f\u80fd\u66f4\u5feb\u5730\u5ba1\u9605\u56fe\u50cf\u3002\u7136\u800c\uff0cAI\u7684\u5f15\u5165\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u53ef\u80fd\u5bfc\u81f4\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\u4e0e\u5b9e\u9645\u4fe1\u606f\u5185\u5bb9\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "conclusion": "\u4e86\u89e3AI\u56fe\u50cf\u91cd\u5efa\u548c\u589e\u5f3a\u7684\u5c40\u9650\u6027\u5bf9\u4e8e\u653e\u5c04\u5b66\u9886\u57df\u5b89\u5168\u6709\u6548\u5730\u4f7f\u7528\u8be5\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002\u7528\u6237\u9700\u8981\u5728\u4eab\u53d7AI\u6280\u672f\u5e26\u6765\u7684\u597d\u5904\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8ba4\u8bc6\u5176\u5c40\u9650\u6027\u6765\u6700\u5c0f\u5316\u76f8\u5173\u98ce\u9669\uff0c\u5b9e\u73b0\u6280\u672f\u4f18\u52bf\u4e0e\u4e34\u5e8a\u5b89\u5168\u7684\u5e73\u8861\u3002"}}
{"id": "2602.09485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09485", "abs": "https://arxiv.org/abs/2602.09485", "authors": ["Yizhi Wang", "Linan Yue", "Min-Ling Zhang"], "title": "Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models", "comment": null, "summary": "Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.", "AI": {"tldr": "XMCC\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u538b\u7f29\u5668\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u538b\u7f29\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6709\u6548\u7f29\u77ed\u63a8\u7406\u8f68\u8ff9\u540c\u65f6\u4fdd\u6301\u5173\u952e\u6b65\u9aa4\u548c\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "motivation": "\u957f\u601d\u7ef4\u94fe\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u901a\u5e38\u8fc7\u4e8e\u5197\u957f\u4e14\u5305\u542b\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u8fd9\u4f1a\u964d\u4f4e\u63a8\u7406\u6548\u7387\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u53ef\u80fd\u635f\u5bb3\u89c6\u89c9-\u6587\u672c\u63a8\u7406\u7684\u5b8c\u6574\u6027\uff0c\u4ee5\u53ca\u538b\u7f29\u8fc7\u7a0b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faXMCC\uff08\u53ef\u89e3\u91ca\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u538b\u7f29\u5668\uff09\uff0c\u5c06\u538b\u7f29\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u7f29\u77ed\u63a8\u7406\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u548c\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5e76\u4e3a\u538b\u7f29\u51b3\u7b56\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cXMCC\u4e0d\u4ec5\u51cf\u5c11\u4e86\u63a8\u7406\u957f\u5ea6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "XMCC\u89e3\u51b3\u4e86\u957f\u601d\u7ef4\u94fe\u538b\u7f29\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4fdd\u6301\u63a8\u7406\u5b8c\u6574\u6027\u5e76\u63d0\u4f9b\u538b\u7f29\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09489", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09489", "abs": "https://arxiv.org/abs/2602.09489", "authors": ["Lars Henry Berge Olsen", "Dennis Christensen"], "title": "Computing Conditional Shapley Values Using Tabular Foundation Models", "comment": null, "summary": "Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.", "AI": {"tldr": "TabPFN\u7b49\u8868\u683c\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u9ad8\u6548\u8ba1\u7b97Shapley\u503c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6027\u80fd\u6700\u4f18\u4e14\u8fd0\u884c\u65f6\u95f4\u5927\u5e45\u51cf\u5c11", "motivation": "Shapley\u503c\u4f5c\u4e3a\u53ef\u89e3\u91caAI\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u5728\u7279\u5f81\u76f8\u5173\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6761\u4ef6\u671f\u671b\u7684\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u56de\u5f52\u65b9\u6cd5\u56e0\u9700\u8981\u91cd\u590d\u8bad\u7ec3\u800c\u6548\u7387\u4f4e\u4e0b", "method": "\u4f7f\u7528TabPFN\u7b49\u591a\u79cd\u8868\u683c\u57fa\u7840\u6a21\u578b\u53d8\u4f53\u8ba1\u7b97Shapley\u503c\uff0c\u5229\u7528\u5176\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8fd1\u4f3c\u6bcf\u4e2a\u6761\u4ef6\u671f\u671b\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83", "result": "\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0cTabPFN\u8868\u73b0\u6700\u4f73\uff1b\u5373\u4f7f\u4e0d\u662f\u6700\u4f18\u65f6\uff0c\u4e5f\u4ec5\u7565\u5dee\u4e8e\u6700\u4f73\u65b9\u6cd5\uff0c\u4f46\u8fd0\u884c\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\uff08\u4ec5\u4e3a\u5176\u4ed6\u65b9\u6cd5\u7684\u4e00\u5c0f\u90e8\u5206\uff09", "conclusion": "\u8868\u683c\u57fa\u7840\u6a21\u578b\u4e3a\u6761\u4ef6Shapley\u503c\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u4f7f\u5176\u66f4\u9002\u5e94\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1"}}
{"id": "2602.09620", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.09620", "abs": "https://arxiv.org/abs/2602.09620", "authors": ["Jorge Fandinno", "Pedro Cabalar", "Philipp Wanko", "Torsten Schaub"], "title": "FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints", "comment": null, "summary": "Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.", "AI": {"tldr": "FLINGO\u8bed\u8a00\u6269\u5c55\u4e86\u7ea6\u675fASP\uff0c\u5c06ASP\u7684\u4e30\u5bcc\u8868\u8fbe\u7279\u6027\uff08\u5982\u9ed8\u8ba4\u503c\u3001\u672a\u5b9a\u4e49\u5c5e\u6027\u3001\u9009\u62e9\u89c4\u5219\u548c\u805a\u5408\u503c\uff09\u878d\u5165\u6570\u503c\u7ea6\u675f\u4e2d\uff0c\u5e76\u901a\u8fc7\u7ffb\u8bd1\u5230\u6807\u51c6CASP\u683c\u5f0f\u5b9e\u73b0\u3002", "motivation": "\u5f53\u524d\u7ea6\u675fASP\uff08CASP\uff09\u4e2d\u6570\u503c\u7ea6\u675f\u7684\u8868\u8fbe\u65b9\u5f0f\u66f4\u63a5\u8fd1\u540e\u7aef\u6570\u503c\u5904\u7406\u5668\u7684\u8bed\u4e49\uff0c\u800c\u975e\u6807\u51c6ASP\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u5bfc\u81f4ASP\u7684\u8bb8\u591a\u6709\u7528\u7279\u6027\uff08\u5982\u9ed8\u8ba4\u503c\u3001\u672a\u5b9a\u4e49\u5c5e\u6027\u3001\u9009\u62e9\u89c4\u5219\u3001\u805a\u5408\u503c\uff09\u5728\u5207\u6362\u5230\u7ea6\u675f\u8868\u793a\u65f6\u4e22\u5931\u3002", "method": "\u63d0\u51faFLINGO\u8bed\u8a00\u548c\u5de5\u5177\uff0c\u5c06ASP\u7684\u8868\u8fbe\u7279\u6027\u878d\u5165\u6570\u503c\u7ea6\u675f\u4e2d\uff0c\u5e76\u8bbe\u8ba1\u4eceFLINGO\u8bed\u6cd5\u5230\u6807\u51c6CASP\u7a0b\u5e8f\uff08\u9075\u5faaCLINGCON\u8f93\u5165\u683c\u5f0f\uff09\u7684\u7ffb\u8bd1\u65b9\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86FLINGO\u8bed\u8a00\uff0c\u80fd\u591f\u4fdd\u6301ASP\u7684\u4e30\u5bcc\u8868\u8fbe\u7279\u6027\uff0c\u540c\u65f6\u652f\u6301\u6570\u503c\u7ea6\u675f\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "conclusion": "FLINGO\u6210\u529f\u5730\u5c06ASP\u7684\u8868\u8fbe\u80fd\u529b\u4e0eCASP\u7684\u6570\u503c\u7ea6\u675f\u5904\u7406\u76f8\u7ed3\u5408\uff0c\u586b\u8865\u4e86\u5f53\u524dCASP\u7cfb\u7edf\u5728\u8868\u8fbe\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.09653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09653", "abs": "https://arxiv.org/abs/2602.09653", "authors": ["Shiwei Lyu", "Xidong Wang", "Lei Liu", "Hao Zhu", "Chaohe Zhang", "Jian Wang", "Jinjie Gu", "Benyou Wang", "Yue Shen"], "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference", "comment": null, "summary": "Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B-A3B model trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.", "AI": {"tldr": "\u63d0\u51faHealthRubrics\u6570\u636e\u96c6\u548cHealthPrinciples\u6846\u67b6\uff0c\u901a\u8fc7\u533b\u751f\u9a8c\u8bc1\u7684\u504f\u597d\u793a\u4f8b\u548c\u53ef\u91cd\u7528\u4e34\u5e8a\u539f\u5219\uff0c\u89e3\u51b3LLM\u5728\u533b\u7597\u9886\u57df\u8f93\u51fa\u4e0e\u4e34\u5e8a\u504f\u597d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u4e13\u5bb6\u7ea7\u533b\u5b66\u77e5\u8bc6\uff0c\u4f46\u5176\u5f00\u653e\u5f0f\u8f93\u51fa\u4e0e\u7ec6\u7c92\u5ea6\u4e34\u5e8a\u504f\u597d\u5bf9\u9f50\u4ecd\u7136\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u76ee\u6807\u6216\u4e0d\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u6307\u5357\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u521b\u5efaHealthRubrics\u6570\u636e\u96c6\uff0c\u5305\u542b7,034\u4e2a\u533b\u751f\u9a8c\u8bc1\u7684\u504f\u597d\u793a\u4f8b\uff0c\u4e34\u5e8a\u533b\u751f\u6539\u8fdbLLM\u8349\u62df\u7684\u8bc4\u5206\u6807\u51c6\u4ee5\u6ee1\u8db3\u4e25\u683c\u533b\u5b66\u6807\u51c6\uff1b2) \u5c06\u8fd9\u4e9b\u8bc4\u5206\u6807\u51c6\u63d0\u70bc\u4e3aHealthPrinciples\uff1a119\u4e2a\u5e7f\u6cdb\u53ef\u91cd\u7528\u3001\u57fa\u4e8e\u4e34\u5e8a\u7684\u539f\u5219\uff0c\u6309\u4e34\u5e8a\u7ef4\u5ea6\u7ec4\u7ec7\uff0c\u5b9e\u73b0\u8d85\u8d8a\u4eba\u5de5\u6807\u6ce8\u7684\u53ef\u6269\u5c55\u76d1\u7763\u3002", "result": "\u4f7f\u7528\u8be5\u6846\u67b6\u8bad\u7ec3\u768430B-A3B\u6a21\u578b\u5728HealthBench-Hard\u4e0a\u8fbe\u523033.4%\uff0c\u8d85\u8d8a\u4e86\u5305\u62ecDeepseek-R1\u548co3\u5728\u5185\u7684\u66f4\u5927\u6a21\u578b\uff0c\u4e3a\u4e34\u5e8a\u5bf9\u9f50\u5efa\u7acb\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684HealthRubrics\u548cHealthPrinciples\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u533b\u7597\u8f93\u51fa\u4e0e\u4e34\u5e8a\u504f\u597d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u901a\u8fc7\u533b\u751f\u9a8c\u8bc1\u7684\u6570\u636e\u548c\u53ef\u91cd\u7528\u539f\u5219\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.09794", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09794", "abs": "https://arxiv.org/abs/2602.09794", "authors": ["Jiaquan Zhang", "Chaoning Zhang", "Shuxu Chen", "Xudong Wang", "Zhenzhen Huang", "Pengcheng Zheng", "Shuai Yuan", "Sheng Zheng", "Qigan Sun", "Jie Zou", "Lik-Hang Lee", "Yang Yang"], "title": "GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis", "comment": "23pages", "summary": "Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.", "AI": {"tldr": "GHS-TDA\u901a\u8fc7\u6784\u5efa\u5168\u5c40\u5047\u8bbe\u56fe\u7ed3\u5408\u62d3\u6251\u6570\u636e\u5206\u6790\uff0c\u89e3\u51b3\u4f20\u7edf\u601d\u7ef4\u94fe\u65b9\u6cd5\u4e2d\u65e9\u671f\u9519\u8bef\u4f20\u64ad\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u5206\u6790\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u6027\u9650\u5236\uff1a1\uff09\u63a8\u7406\u8fc7\u7a0b\u5bf9\u65e9\u671f\u51b3\u7b56\u9ad8\u5ea6\u654f\u611f\uff0c\u4e00\u65e6\u51fa\u73b0\u521d\u59cb\u9519\u8bef\u4f1a\u4f20\u64ad\u653e\u5927\u4e14\u96be\u4ee5\u7ea0\u6b63\uff1b2\uff09\u7f3a\u4e4f\u7ed3\u6784\u5316\u5206\u6790\u6280\u672f\u8fc7\u6ee4\u5197\u4f59\u63a8\u7406\u548c\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u4e0d\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faGHS-TDA\u65b9\u6cd5\uff1a\u9996\u5148\u6784\u5efa\u8bed\u4e49\u4e30\u5bcc\u7684\u5168\u5c40\u5047\u8bbe\u56fe\uff0c\u805a\u5408\u3001\u5bf9\u9f50\u548c\u534f\u8c03\u591a\u4e2a\u5019\u9009\u63a8\u7406\u8def\u5f84\uff0c\u4e3a\u5c40\u90e8\u63a8\u7406\u5931\u8d25\u65f6\u63d0\u4f9b\u5168\u5c40\u4fee\u6b63\u8def\u5f84\uff1b\u7136\u540e\u5e94\u7528\u57fa\u4e8e\u6301\u7eed\u540c\u8c03\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\uff0c\u6355\u6349\u7a33\u5b9a\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u53bb\u9664\u5197\u4f59\u548c\u4e0d\u4e00\u81f4\uff0c\u63d0\u53d6\u66f4\u53ef\u9760\u7684\u63a8\u7406\u9aa8\u67b6\u3002", "result": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u63a8\u7406\u591a\u6837\u6027\u548c\u62d3\u6251\u7a33\u5b9a\u6027\uff0cGHS-TDA\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u6536\u655b\uff0c\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GHS-TDA\u901a\u8fc7\u5168\u5c40\u534f\u8c03\u548c\u62d3\u6251\u5206\u6790\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u601d\u7ef4\u94fe\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.09802", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09802", "abs": "https://arxiv.org/abs/2602.09802", "authors": ["Manon Reusens", "Sofie Goethals", "Toon Calders", "David Martens"], "title": "Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65c5\u884c\u52a9\u624b\u573a\u666f\u4e2d\u7684\u4e3b\u89c2\u51b3\u7b56\u80fd\u529b\uff0c\u901a\u8fc7\u9009\u62e9\u56f0\u5883\u5206\u6790\u6a21\u578b\u9690\u542b\u652f\u4ed8\u610f\u613f\uff0c\u5e76\u4e0e\u4eba\u7c7b\u57fa\u51c6\u503c\u6bd4\u8f83\uff0c\u53d1\u73b0\u5927\u6a21\u578b\u80fd\u4ea7\u751f\u6709\u610f\u4e49\u7684WTP\u503c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "motivation": "\u968f\u7740LLMs\u5728\u65c5\u884c\u52a9\u624b\u7b49\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u591a\uff0c\u6a21\u578b\u9700\u8981\u5728\u6ca1\u6709\u5ba2\u89c2\u6b63\u786e\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u4e3a\u7528\u6237\u505a\u51fa\u4e3b\u89c2\u9009\u62e9\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u8fd9\u79cd\u4e3b\u89c2\u51b3\u7b56\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5176\u9690\u542b\u652f\u4ed8\u610f\u613f\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u5dee\u5f02\u3002", "method": "\u5728\u65c5\u884c\u52a9\u624b\u80cc\u666f\u4e0b\u5411\u6a21\u578b\u5448\u73b0\u9009\u62e9\u56f0\u5883\uff0c\u4f7f\u7528\u591a\u9879logit\u6a21\u578b\u5206\u6790\u54cd\u5e94\u4ee5\u63a8\u5bfc\u9690\u542b\u652f\u4ed8\u610f\u613f\u4f30\u8ba1\u3002\u9664\u4e86\u57fa\u7ebf\u8bbe\u7f6e\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u5728\u66f4\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u884c\u4e3a\u53d8\u5316\uff0c\u5305\u62ec\u63d0\u4f9b\u7528\u6237\u8fc7\u53bb\u9009\u62e9\u4fe1\u606f\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u3002", "result": "\u8f83\u5927\u578bLLMs\u80fd\u591f\u4ea7\u751f\u6709\u610f\u4e49\u7684WTP\u503c\uff0c\u4f46\u5728\u5c5e\u6027\u5c42\u9762\u663e\u793a\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u6a21\u578b\u503e\u5411\u4e8e\u6574\u4f53\u9ad8\u4f30\u4eba\u7c7bWTP\uff0c\u7279\u522b\u662f\u5728\u5f15\u5165\u6602\u8d35\u9009\u9879\u6216\u5546\u52a1\u5bfc\u5411\u89d2\u8272\u65f6\u3002\u5f53\u6a21\u578b\u57fa\u4e8e\u5148\u524d\u5bf9\u66f4\u4fbf\u5b9c\u9009\u9879\u7684\u504f\u597d\u8fdb\u884c\u6761\u4ef6\u8bbe\u7f6e\u65f6\uff0c\u4f30\u503c\u66f4\u63a5\u8fd1\u4eba\u7c7b\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u4e3b\u89c2\u51b3\u7b56\u652f\u6301\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u6b64\u7c7b\u7cfb\u7edf\u65f6\uff0c\u9700\u8981\u4ed4\u7ec6\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u7528\u6237\u8868\u793a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.10085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10085", "abs": "https://arxiv.org/abs/2602.10085", "authors": ["Richard Bornemann", "Pierluigi Vito Amadori", "Antoine Cully"], "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs", "comment": "Preprint", "summary": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos at https://sites.google.com/view/code-sharp/homepage.", "AI": {"tldr": "CODE-SHARP\u6846\u67b6\u5229\u7528\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u53d1\u73b0\u548c\u6f14\u5316\u6280\u80fd\uff0c\u901a\u8fc7\u4ee3\u7801\u5f62\u5f0f\u7684\u5956\u52b1\u51fd\u6570\u6784\u5efa\u6280\u80fd\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u89e3\u51b3\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5f00\u653e\u5f0f\u7684\u6280\u80fd\u53d1\u73b0\uff0c\u56e0\u4e3a\u6709\u610f\u4e49\u6280\u80fd\u7684\u96c6\u5408\u662f\u672a\u77e5\u7684\u3002\u9700\u8981\u81ea\u52a8\u5316\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u5e76\u652f\u6301\u5f00\u653e\u5f0f\u7684\u6280\u80fd\u53d1\u73b0\u548c\u6f14\u5316\u3002", "method": "\u63d0\u51faCODE-SHARP\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5f00\u653e\u5730\u6269\u5c55\u548c\u7cbe\u70bc\u5c42\u6b21\u5316\u6280\u80fd\u6863\u6848\uff0c\u6784\u5efa\u4e3a\u53ef\u6267\u884c\u5956\u52b1\u51fd\u6570\u4ee3\u7801\u7684\u6709\u5411\u56fe\u3002\u901a\u8fc7\u9ad8\u5c42FM\u89c4\u5212\u5668\u7ec4\u5408\u53d1\u73b0\u7684\u6280\u80fd\uff0c\u4f7f\u5355\u4e00\u76ee\u6807\u6761\u4ef6\u667a\u80fd\u4f53\u80fd\u591f\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "result": "\u5728Craftax\u73af\u5883\u4e2d\uff0c\u4ec5\u4f7f\u7528\u53d1\u73b0\u7684SHARP\u6280\u80fd\u751f\u6210\u7684\u5956\u52b1\u8fdb\u884c\u8bad\u7ec3\u7684\u76ee\u6807\u6761\u4ef6\u667a\u80fd\u4f53\u80fd\u591f\u89e3\u51b3\u8d8a\u6765\u8d8a\u957f\u7684\u65f6\u7a0b\u76ee\u6807\u3002\u7ec4\u5408\u6280\u80fd\u540e\uff0c\u8be5\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5e73\u5747\u8d85\u8fc7\u9884\u8bad\u7ec3\u667a\u80fd\u4f53\u548c\u4efb\u52a1\u7279\u5b9a\u4e13\u5bb6\u7b56\u7565134%\u3002", "conclusion": "CODE-SHARP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5f00\u653e\u5f0f\u7684\u6280\u80fd\u53d1\u73b0\u548c\u6f14\u5316\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\u5e76\u6784\u5efa\u6280\u80fd\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u89e3\u51b3\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2602.10090", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10090", "abs": "https://arxiv.org/abs/2602.10090", "authors": ["Zhaoyang Wang", "Canwen Xu", "Boyi Liu", "Yite Wang", "Siwei Han", "Zhewei Yao", "Huaxiu Yao", "Yuxiong He"], "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning", "comment": "41 pages", "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.", "AI": {"tldr": "\u63d0\u51faAgent World Model\uff08AWM\uff09\u2014\u2014\u4e00\u4e2a\u5b8c\u5168\u5408\u6210\u7684\u73af\u5883\u751f\u6210\u7ba1\u9053\uff0c\u53ef\u6269\u5c55\u52301000\u4e2a\u65e5\u5e38\u573a\u666f\u73af\u5883\uff0c\u6bcf\u4e2a\u73af\u5883\u5e73\u5747\u914d\u590735\u4e2a\u5de5\u5177\uff0c\u4e3a\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u8bad\u7ec3\u9762\u4e34\u73af\u5883\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u591a\u6837\u4e14\u53ef\u9760\u7684\u73af\u5883\u9650\u5236\u4e86\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u4e86AWM\u5408\u6210\u73af\u5883\u751f\u6210\u7ba1\u9053\uff0c\u521b\u5efa\u4ee3\u7801\u9a71\u52a8\u3001\u6570\u636e\u5e93\u652f\u6301\u7684\u73af\u5883\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u72b6\u6001\u8f6c\u6362\u548c\u9ad8\u6548\u4ea4\u4e92\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e861000\u4e2a\u8986\u76d6\u65e5\u5e38\u573a\u666f\u7684\u73af\u5883\uff0c\u6bcf\u4e2a\u73af\u5883\u5e73\u5747\u914d\u590735\u4e2a\u5de5\u5177\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u5408\u6210\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AWM\u4e3a\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u5408\u6210\u73af\u5883\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
