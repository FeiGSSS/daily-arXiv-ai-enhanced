<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: LiveMedBench：一个持续更新、无数据污染、基于量规的医学基准，从在线医疗社区每周收集真实临床案例，解决现有医学基准的数据污染和时间错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准存在两个关键局限：1) 数据污染问题，测试集无意中泄露到训练语料中，导致性能估计虚高；2) 时间错位问题，无法捕捉医学知识的快速演变。此外，当前开放式临床推理的评估指标要么依赖浅层词汇重叠（如ROUGE），要么依赖主观的LLM-as-a-Judge评分，都不足以验证临床正确性。

Method: 1) 构建LiveMedBench基准：每周从在线医疗社区收集真实临床案例，确保与模型训练数据的严格时间分离；2) 多智能体临床筛选框架：过滤原始数据噪声，根据循证医学原则验证临床完整性；3) 自动化量规评估框架：将医生回答分解为细粒度、病例特定的标准，实现比LLM-as-a-Judge更强的专家医生对齐。

Result: LiveMedBench目前包含2,756个真实世界案例，涵盖38个医学专业和多种语言，配有16,702个独特的评估标准。对38个LLM的广泛评估显示，即使表现最好的模型也只达到39.2%的准确率，84%的模型在截止日期后的案例上表现下降，证实了普遍存在的数据污染风险。错误分析进一步发现，上下文应用（而非事实知识）是主要瓶颈，35-48%的失败源于无法将医学知识适应患者特定约束。

Conclusion: LiveMedBench提供了一个持续更新、无污染、基于量规的医学基准，有效解决了现有基准的数据污染和时间错位问题。评估结果显示LLM在临床推理中的表现仍然有限，特别是将医学知识适应具体患者情境的能力不足，这为未来医学AI的发展指明了方向。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [2] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL是一个专门为自动驾驶设计的强化学习平台，通过异步批量推理框架将视觉语言模型与RL训练解耦，解决VLM推理延迟问题，使轻量级RL模型能达到接近大参数VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶中存在样本效率低和语义可解释性不足的问题，而基础模型（特别是视觉语言模型）能提供丰富的上下文感知知识，但其高推理延迟阻碍了在高频RL训练循环中的部署。

Method: 1. 异步批量推理框架：将繁重的VLM推理与仿真循环解耦，解决延迟瓶颈；2. 监督机制：值-边际正则化和优势加权动作指导，将VLM专家动作建议蒸馏到RL策略中；3. 高吞吐量CLIP用于密集奖励塑造，通过条件对比动作对齐解决CLIP的动态盲区问题。

Result: Found-RL提供了一个端到端的微调VLM集成管道，轻量级RL模型能达到接近十亿参数VLM的性能，同时保持实时推理（约500 FPS）。

Conclusion: Found-RL成功地将基础模型高效集成到自动驾驶的强化学习中，通过创新的异步推理和监督机制，解决了VLM延迟问题，使轻量级模型能获得接近大参数VLM的性能，实现了实时学习。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [3] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: LLMs作为QNP抽象生成器用于广义规划，通过自动调试修复抽象错误


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否作为QNP抽象生成器为广义规划问题创建抽象，并解决抽象错误的自动修复问题

Method: 提出提示协议：输入GP域和训练任务给LLMs，生成抽象特征并将初始状态、动作集和目标抽象为QNP问题；设计自动调试方法检测抽象错误并指导LLMs修复

Result: 实验表明，在自动调试的适当指导下，某些LLMs能够生成有用的QNP抽象

Conclusion: LLMs可以作为QNP抽象生成器，结合自动调试方法能够有效生成和修复抽象，为广义规划提供支持

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [4] [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)
*Bo Xue,Yunchong Song,Fanghao Shao,Xuekai Zhu,Lin Chen,Luoyi Fu,Xinbing Wang,Zhouhan Lin*

Main category: cs.AI

TL;DR: FoSS提出基于GFlowNets的跨度生成框架，通过动态跨度词汇和DAG状态空间提升文本生成的多样性和质量


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型基于固定词汇的token级生成形成树状状态空间，限制了灵活性和表达能力。现有动态词汇方法虽然引入检索文本跨度，但未显式建模DAG状态空间，导致组合路径探索受限且存在路径偏差。

Method: 提出FoSS框架：1）通过灵活分割检索文本构建动态跨度词汇；2）确保DAG结构状态空间；3）利用GFlowNets探索多样组合路径；4）结合专用奖励模型生成高质量文本。

Result: FoSS在文本生成任务上比Transformer提升MAUVE分数达12.5%，在知识密集型任务上获得3.5%增益，持续优于现有最佳方法。扩展实验显示FoSS受益于更大模型、更多数据和更丰富的检索语料。

Conclusion: FoSS通过将GFlowNets应用于跨度生成，构建DAG状态空间，有效解决了传统树状状态空间的限制，实现了更灵活、多样和高质量的文本生成，在多个任务上展现出显著优势。

Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.

</details>


### [5] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: NSAM：一种自动学习符号模型来约束DRL动作的神经符号框架，显著提高样本效率并减少约束违反


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在训练和执行过程中可能探索不可行动作，现有方法需要手动指定符号映射函数和动作掩码技术，这限制了方法的自动化程度和适应性

Method: 提出神经符号动作掩码（NSAM）框架，在DRL过程中以最小监督方式自动学习与高维状态约束一致的符号模型，基于学习到的状态符号模型生成动作掩码来排除不可行动作，实现符号推理与深度策略优化的端到端集成

Result: 在多个约束域上的实验结果表明，NSAM显著提高了DRL智能体的样本效率，同时大幅减少了约束违反次数

Conclusion: NSAM框架成功实现了符号推理与深度强化学习的端到端集成，其中符号基础学习和策略学习相互促进，为解决DRL中的约束满足问题提供了一种有效的自动化解决方案

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [6] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在心理理论任务上并不比非推理模型表现更好，有时甚至更差，揭示了推理模型在社交认知推理中的局限性


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在数学和编程等正式推理任务上取得了进步，但这些优势是否能转移到心理理论等社交认知技能上仍不清楚。本研究旨在系统评估推理模型在心理理论任务上的表现。

Method: 对9个先进的大型语言模型进行系统研究，比较推理模型与非推理模型在三个代表性心理理论基准测试上的表现。通过细粒度分析揭示问题，并设计了两种干预方法：慢到快自适应推理和思考到匹配捷径预防。

Result: 1. 推理模型并不总是优于非推理模型，有时表现更差；2. 慢思考崩溃：随着回答变长，准确率显著下降，更大的推理预算反而损害性能；3. 适度和自适应推理有益性能；4. 选项匹配捷径：移除多项选择选项后，推理模型表现显著改善，表明其依赖选项匹配而非真正推理。

Conclusion: 大型推理模型在正式推理任务上的进步不能完全转移到心理理论这类社交推理任务上。实现稳健的心理理论需要开发超越现有推理方法的独特能力。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [7] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: HARPO方法通过调节优势函数平衡异构任务和样本的学习，训练出Omnisapiens-7B 2.0社交行为基础模型，在多任务和未见任务上表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立地建模人类行为维度（情感、认知或社会属性），任务特定建模增加了训练成本并限制了跨行为设置的泛化能力。需要一种能够处理异构行为数据的统一方法。

Method: 提出异构感知相对策略优化（HARPO），通过调节优势函数确保在策略优化过程中没有单个任务或样本产生不成比例的影响，平衡跨异构任务和样本的学习。

Result: 使用HARPO开发的Omnisapiens-7B 2.0模型在行为任务上表现最强，多任务设置提升16.85%，未见任务设置提升9.37%，同时产生更明确和鲁棒的推理轨迹。HARPO相比其他RL方法在行为任务上表现最稳定。

Conclusion: HARPO通过平衡异构任务和样本的学习，有效解决了跨异构行为数据的统一建模问题，为开发社交智能AI提供了更强大的基础模型。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [8] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR框架通过价值引导采样和树状优势强化学习，解决生成式推荐中RL训练的概率-奖励不匹配问题，提升探索效率和推荐质量。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐中的强化学习训练存在概率-奖励不匹配问题：传统基于似然的解码（如束搜索）偏向局部高概率前缀，导致探索不足（高奖励但低概率项被过早剪枝）和优势压缩（共享高概率前缀的轨迹奖励高度相关，比较信号弱）。

Method: 提出V-STAR框架，包含两个协同组件：1）价值引导高效解码（VED），识别关键节点并选择性深化高潜力前缀，提高探索效率；2）Sibling-GRPO，利用树状拓扑计算兄弟相对优势，将学习信号集中在关键分支决策上。

Result: 在离线和在线数据集上的广泛实验表明，V-STAR在严格延迟约束下优于现有最优基线，提供更高的准确性和候选集多样性。

Conclusion: V-STAR通过价值引导采样和树状优势强化学习有效解决了生成式推荐中RL训练的概率-奖励不匹配问题，实现了更好的探索效率和推荐性能。

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [9] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: 研究调查了高等教育中生成式AI的感知情况，特别关注信息技术与电气工程领域，通过混合方法分析师生看法，提出了负责任整合的框架和要求。


<details>
  <summary>Details</summary>
Motivation: 高等教育中生成式AI工具已被广泛采用，但利益相关者的看法存在分歧，受到文化、学科和制度背景的影响。同时欧盟AI法案要求大学确保认知系统的监管合规，因此需要了解不同学科对GenAI的感知，以制定负责任的整合策略。

Method: 采用混合方法，对奥卢大学ITEE学院的61名教职员工和37名学生进行了调查，分析他们对生成式AI的看法和关注点。

Result: 研究揭示了共享和学科特定的主题：对GenAI编程支持的强烈兴趣，以及对响应质量、隐私和学术诚信的担忧。研究识别了高层次要求，并提出了负责任GenAI整合的概念框架。

Conclusion: 学科特定要求强调了利益相关者参与的重要性，高层次要求和框架为大学利用GenAI同时解决利益相关者关切和确保监管合规提供了实践指导。

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [10] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC是一个自适应知识图谱补全框架，通过跨模态协同专家机制解决图密度差异导致的"结构分辨率不匹配"问题，显著提升KGC性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法面临"结构分辨率不匹配"的关键问题：无法协调不同图密度下的表示需求，导致在密集簇中产生结构噪声干扰，在稀疏区域出现灾难性表示崩溃。

Method: 提出SynergyKGC自适应框架：1) 将传统邻居聚合提升为基于关系感知交叉注意力和语义意图驱动门控的主动跨模态协同专家；2) 结合密度依赖的身份锚定策略和双塔一致性架构，协调拓扑异质性并确保表示稳定性。

Result: 在两个公共基准测试上的系统评估验证了方法的优越性，显著提升了KGC命中率，为非均匀结构化数据中的弹性信息整合提供了经验证据。

Conclusion: SynergyKGC通过自适应框架有效解决了知识图谱补全中的结构异质性问题，为处理非均匀结构化数据提供了通用的弹性信息整合原则。

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [11] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RLCER：通过自提出和自演进的评分标准来奖励思维链，无需人工标注，在强化学习中优于仅关注结果的RLVR方法


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）在LLM推理中至关重要，但直接奖励CoT面临挑战：训练奖励模型需要大量人工标注，静态奖励模型难以适应CoT分布的演变且容易受到奖励攻击。需要一种无需人工标注且能逐步演进的自主CoT奖励方法。

Method: 提出RLCER（强化学习与通过自演进评分标准的CoT监督），在结果中心的RLVR基础上，通过自提出和自演进的评分标准来奖励CoT。该方法无需人工标注，能自主演进评分标准。

Result: 自提出和自演进的评分标准即使在没有结果奖励的情况下也能提供可靠的CoT监督信号，使RLCER优于结果中心的RLVR。当作为提示中的提示使用时，这些自提出的评分标准还能进一步提高推理时性能。

Conclusion: RLCER提供了一种有效的自主CoT奖励方法，无需人工标注且能适应CoT分布的演变，在强化学习框架中显著提升了性能，并为推理时性能改进提供了新途径。

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [12] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: LLMs在文化内容生成中存在系统性文化偏见，无法像人类一样根据文化距离进行有意义的跨文化适应，特别是在烹饪食谱领域。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在文化内容生成方面表现出色，但存在系统性文化偏见，可能导致刻板印象、同质化和特定文化表达形式的消失。研究LLMs是否能超越主流文化而真正适应多元文化是一个关键挑战。

Method: 使用GlobalFusion数据集，该数据集根据文化距离测量方法配对不同国家的人类食谱。使用相同的国家配对，用多个LLMs生成文化适应食谱，从而直接比较人类和LLM在跨文化内容创作中的行为。

Result: LLMs无法产生具有文化代表性的适应。与人类不同，它们生成的食谱差异与文化距离不相关。研究发现：1）文化信息在模型内部表示中保存较弱；2）模型通过误解创造性和传统等概念来夸大新颖性；3）模型无法将适应与相关国家联系起来，也无法将其建立在文化显著元素（如食材）上。

Conclusion: 当前LLMs在文化导向的生成方面存在根本性限制，这对它们在文化敏感应用中的使用具有重要影响。

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [13] [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136)
*Jiayi Zhou,Yang Sheng,Hantao Lou,Yaodong Yang,Jie Fu*

Main category: cs.AI

TL;DR: 提出Formal-of-Thought框架，使用双向神经符号架构将自然语言需求转化为形式化规范，通过Dafny和Z3提供数学保证而非概率评分，解决LLM监督LLM的困境。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在高风险领域应用增多，行为安全变得至关重要。当前主流的LLM-as-a-Judge监督范式面临根本困境：概率系统如何可靠地监督其他概率系统而不继承其失败模式？形式化验证提供了原则性解决方案，但自然语言需求到形式化规范的转换成为关键瓶颈。

Method: 提出Formal-of-Thought神经符号框架，采用双向形式化思维架构：LLM作为规范编译器，自上而下将高级人类意图分解为原子化、可验证的约束，然后自下而上使用Dafny规范和Z3可满足性模理论求解来证明合规性，产生数学保证而非概率评分。

Result: 在三个基准测试（行为安全、多领域约束遵守、智能体向上欺骗检测）上验证，对7个智能体模型的实验表明，相比LLM-as-a-Judge基线平均提升16.6%，实现了弱到强泛化（7B法官检测72B智能体欺骗准确率超90%），并通过迭代细化提供接近线性的安全改进。

Conclusion: Formal-of-Thought框架成功解决了自然语言需求到形式化规范的转换瓶颈，为LLM智能体行为安全监督提供了数学保证而非概率评分的方法，显著提升了监督效果和泛化能力。

Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

</details>
