<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench是一个包含2009个高风险场景的多智能体安全基准测试，覆盖囚徒困境、猎鹿博弈等博弈论结构，评估前沿AI系统在多智能体环境中的协调失败和冲突风险。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全基准主要评估单智能体，而前沿AI系统越来越多地部署在高风险的多智能体环境中，导致协调失败和冲突等多智能体风险缺乏充分理解和评估。

Method: 开发了GT-HarmBench基准测试，包含2009个从MIT AI风险库中提取的现实AI风险场景，涵盖囚徒困境、猎鹿博弈、胆小鬼博弈等博弈论结构。评估了15个前沿模型，测量了博弈论提示框架和顺序的敏感性，并分析了导致失败的推理模式。

Result: 在15个前沿模型中，智能体仅在62%的情况下选择对社会有益的行动，经常导致有害结果。研究发现博弈论干预可以将社会有益结果提高多达18%。

Conclusion: 研究结果揭示了前沿AI系统在多智能体环境中存在显著的可靠性差距，GT-HarmBench为研究多智能体环境中的对齐问题提供了广泛的标准测试平台。基准测试和代码已开源。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [2] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该研究比较了多领域强化学习与可验证奖励（RLVR）的两种训练范式：混合多任务训练和单独训练后模型合并，发现跨领域RLVR存在相互增益效应


<details>
  <summary>Details</summary>
Motivation: 当前在多领域RLVR训练中主要采用混合多任务训练和单独训练后模型合并两种范式，但缺乏对这些范式的详细比较分析。研究旨在深入探讨这两种范式在不同领域的表现和相互影响

Method: 选择数学、编程、科学和指令跟随等多个常用高级任务作为目标领域，使用开源数据集设计广泛的定性和定量实验，从权重空间几何、模型预测行为和信息约束等角度分析内部机制

Result: 发现跨领域RLVR存在较少的相互干扰，推理密集型领域表现出相互协同效应。从权重空间几何等角度分析了相互增益的内部机制

Conclusion: 该研究为多领域RLVR训练提供了深入的比较分析，揭示了跨领域训练的协同效应，有助于指导未来多领域专家级模型的开发。项目命名为M2RL（混合多任务训练或单独训练后模型合并的强化学习）

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [3] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE使用蒙特卡洛树搜索优化掩码扩散模型中的槽填充顺序，通过前瞻模拟评估部分完成情况，在数学和代码推理任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型中的计划-填充解码方法在数学和代码推理中表现出潜力，但性能对槽填充顺序高度敏感，导致输出方差大，需要优化填充顺序

Method: 将槽选择问题建模为决策过程，使用蒙特卡洛树搜索优化填充顺序，通过前瞻模拟评估部分完成情况，系统探索生成顺序的组合空间

Result: 相比自回归基线平均提升3.2%，相比基线计划-填充方法提升8.0%，在MBPP上提升19.5%，在MATH500上提升4.9%

Conclusion: MCTS规划是提升掩码扩散模型生成质量的有效方法，虽然主要遵循顺序生成，但结合非顺序生成对最大化性能至关重要，需要更大的探索常数而非更多模拟来克服模型置信度偏差

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [4] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: LLM增强的运筹学算法在库存控制中表现优于单独使用运筹学或LLM方法，人机协作团队比单独的人类或AI获得更高利润


<details>
  <summary>Details</summary>
Motivation: 传统运筹学算法依赖刚性建模假设，在需求分布变化或缺乏上下文信息时表现不佳；LLM能够灵活推理并整合丰富上下文信号，但如何将LLM方法有效整合到传统决策流程中仍不明确

Method: 构建InventoryBench基准测试（包含1000多个库存实例，涵盖合成和真实需求数据），测试需求变化、季节性和不确定交货期下的决策规则；通过课堂实验研究人类在决策流程中与LLM推荐协作的效果

Result: 运筹学增强的LLM方法优于单独使用任一方法；人机协作团队平均利润高于单独的人类或AI；推导了个体层面互补效应的无分布下界，实证发现受益于AI协作的个体比例很大

Conclusion: 运筹学算法、LLM和人类在库存控制中具有互补性而非替代性，通过适当整合可以显著提升决策性能，人机协作能够带来实质性效益

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [5] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter是一个让LLM智能体动态调整认知深度的框架，通过分层认知级别和两阶段训练，在长时程任务中实现高效决策，显著提升性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体采用固定的认知模式：非思考模型立即响应，思考模型则统一进行深度推理。这种刚性在长时程任务中效率低下，因为不同步骤的认知需求差异很大，有些需要战略规划，有些只需常规执行。

Method: 基于ACT-R理论设计四个分层认知级别（从本能反应到战略规划），采用两阶段训练：认知感知监督微调（CoSFT）建立稳定的级别特定模式，认知感知策略优化（CoPO）通过置信度感知优势重加权进行步骤级信用分配。核心洞察是适当的认知深度应最大化最终动作的置信度。

Result: 在ALFWorld和ScienceWorld实验中，CogRouter达到最先进性能。使用Qwen2.5-7B模型，成功率达到82.3%，优于GPT-4o（+40.3%）、OpenAI-o3（+18.3%）和GRPO（+14.0%），同时减少62%的token使用量。

Conclusion: CogRouter通过动态调整认知深度，显著提升了LLM智能体在长时程决策任务中的效率和性能，证明了适应性认知策略的重要性。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [6] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 研究者开发了一个诊断性的2-SAT基准测试，通过参数化公式家族来分离表面难度和结构现象，揭示LLM推理器的特定能力和失败模式。


<details>
  <summary>Details</summary>
Motivation: 标准SAT基准测试常常将表面难度（长度、措辞、子句顺序）与决定可满足性的结构现象混为一谈，需要一个更精细的诊断工具来评估LLM推理器的真实能力。

Method: 构建参数化的结构化2-CNF公式家族，通过可解释的维度控制可满足性特征，包括：矛盾循环UNSAT核心、控制解多样性的自由变量比例、预设骨干变量、晚期桥接子句、对称/重复变体等。

Result: 在不同模型上观察到，即使表面统计特征保持不变，针对性的结构干预也会导致性能的急剧变化，揭示了在聚合SAT准确率中不可见的脆弱性区域。

Conclusion: 该诊断基准能够分离LLM推理器的特定能力和失败模式，为评估推理鲁棒性提供了更精细的工具，揭示了模型对结构变化的敏感性而非表面特征。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [7] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: X-SYS是一个交互式可解释AI系统的参考架构，通过STAR质量属性和五组件分解，将用户界面与后端计算解耦，并在SemanticLens系统中实现。


<details>
  <summary>Details</summary>
Motivation: 虽然可解释AI研究提出了许多技术方法，但将可解释性作为系统部署仍然具有挑战性。交互式解释系统需要合适的算法和系统能力，以在重复查询、模型和数据演化以及治理约束下保持解释可用性。作者认为，将XAI操作化需要将可解释性视为信息系统问题。

Method: 提出了X-SYS参考架构，围绕STAR四个质量属性（可扩展性、可追溯性、响应性和适应性）组织，并指定了五个组件分解（XUI服务、解释服务、模型服务、数据服务、编排与治理）。该架构将交互模式映射到系统能力，实现用户界面与后端计算的解耦。通过SemanticLens系统（用于视觉语言模型的语义搜索和激活引导）实现了X-SYS。

Result: X-SYS提供了一个可重用的蓝图，通过基于契约的服务边界实现独立演化，离线/在线分离确保响应性，持久状态管理支持可追溯性。SemanticLens展示了该架构的具体实例化。

Conclusion: 这项工作为交互式解释系统提供了可重用的蓝图和具体实例化，支持在操作约束下的端到端设计，帮助XAI研究人员、开发者和从业者连接交互式解释用户界面与系统能力。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [8] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: 最优策略包含关于环境的精确信息量：在n个状态、m个动作的受控马尔可夫过程中，任何非恒定奖励函数的最优确定性策略都恰好传递n log m比特的环境信息。


<details>
  <summary>Details</summary>
Motivation: 研究AI领域一个重要问题：成功行为在多大程度上需要世界的内部表示。量化最优策略提供的关于底层环境的信息量，为"隐式世界模型"提供信息论下界。

Method: 考虑具有n个状态和m个动作的受控马尔可夫过程，假设在可能的转移动态空间上具有均匀先验。证明观察任何非恒定奖励函数的最优确定性策略恰好传递n log m比特的环境信息。

Result: 证明环境与最优策略之间的互信息为n log m比特。这一界限适用于广泛的目标类别，包括有限时域、无限时域折扣和时均奖励最大化。

Conclusion: 为最优性所需的"隐式世界模型"提供了精确的信息论下界，表明最优策略必然编码了关于环境结构的重要信息。

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [9] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 该研究评估了9个前沿推理模型在对抗攻击下的表现，发现推理能力能提供有意义但不完全的鲁棒性，推理模型显著优于指令调优基线，但所有模型都存在特定漏洞模式，并识别出5种主要失效模式。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色，但其在多轮对抗压力下的鲁棒性尚未得到充分探索。研究旨在评估推理模型在对抗攻击下的表现，了解推理能力是否自动带来对抗鲁棒性。

Method: 研究评估了9个前沿推理模型在对抗攻击下的表现，通过轨迹分析识别失效模式，并测试了置信度感知响应生成（CARG）防御方法对推理模型的有效性。

Result: 研究发现：1）推理能力能提供有意义但不完全的鲁棒性，推理模型显著优于指令调优基线；2）所有模型都存在特定漏洞模式，误导性建议普遍有效，社会压力具有模型特异性；3）识别出5种主要失效模式（自我怀疑、社会从众、建议劫持、情感易感性、推理疲劳），前两种占50%失败；4）CARG防御对推理模型失效，因扩展推理痕迹导致过度自信；5）随机置信度嵌入优于针对性提取。

Conclusion: 推理能力不会自动带来对抗鲁棒性，针对推理模型的置信度防御需要根本性重新设计。研究揭示了推理模型在对抗环境中的脆弱性，为未来鲁棒推理模型开发提供了重要见解。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [10] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文提出了约束ABA（CABA）框架，通过引入约束变量来扩展传统ABA，使其能够处理非地面（包含变量）的论点和攻击，从而克服了传统ABA只能处理命题原子和地面论点的限制。


<details>
  <summary>Details</summary>
Motivation: 传统假设论证（ABA）框架通常基于原子语言，只能处理地面（无变量）的论点和攻击，这限制了其表达能力。为了克服这一限制，需要扩展ABA框架以支持包含约束变量的非地面论点和攻击。

Method: 提出约束ABA（CABA）框架，允许组件和论点包含约束变量，这些变量可以在可能无限的域上取值。定义了CABA的非地面语义，包括各种非地面攻击的概念。

Result: 证明了新的CABA语义能够保守地推广标准ABA语义，即当约束变量被具体化为地面实例时，CABA的语义与标准ABA语义一致。

Conclusion: CABA框架成功扩展了传统ABA，使其能够处理更丰富的非地面论证场景，同时保持了与传统ABA的语义兼容性，为结构化论证提供了更强大的表达能力。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [11] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 提出了一种结合最优控制与模糊规则系统的混合障碍规避架构，用于无人机自适应约束处理，但发现软件兼容性问题导致约束无法正确执行。


<details>
  <summary>Details</summary>
Motivation: 传统最优控制在不确定性下的局限性，以及航空安全关键系统需要可解释的决策制定，促使开发这种混合架构。

Method: 采用三阶段Takagi-Sugeno-Kang模糊层，基于FAA和EASA的监管分离最小值和适航指南，调制约束半径、紧急级别和激活决策，然后将模糊推导的间隙作为软约束纳入最优控制问题，使用FALCON工具箱和IPOPT求解。

Result: 概念验证显示每次迭代计算时间为2-3秒，表明近实时应用的可行性，但发现FALCON和IPOPT最新版本存在软件不兼容问题，导致拉格朗日惩罚项始终为零，无法正确执行约束。

Conclusion: 该混合架构在理论上可行，但受到软件兼容性问题的限制。未来工作包括验证软件回归问题、优化模糊隶属函数，以及扩展到更高保真度飞机模型和随机障碍环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>
