<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ResearchGym: Evaluating Language Model Agents on Real-World AI Research](https://arxiv.org/abs/2602.15112)
*Aniketh Garikaparthi,Manasi Patwardhan,Arman Cohan*

Main category: cs.AI

TL;DR: ResearchGym是一个用于评估AI智能体端到端研究能力的基准测试和执行环境，基于5篇顶级会议论文构建了39个子任务，测试发现前沿AI智能体存在显著的能力-可靠性差距


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估AI智能体进行完整研究过程（提出假设、运行实验、超越基线）的基准环境，需要量化AI在研究任务中的实际表现和可靠性

Method: 从ICML、ICLR和ACL的5篇口头报告和焦点论文中提取任务环境，保留数据集、评估框架和基线实现，但隐藏原始论文的方法，构建了包含39个子任务的容器化环境

Result: GPT-5智能体仅在15次评估中的1次（6.7%）超越基线11.5%，平均只完成26.5%的子任务；发现长期视野失败模式包括缺乏耐心、资源管理差、对弱假设过度自信、并行实验协调困难、上下文长度限制等

Conclusion: 前沿AI智能体偶尔能达到最先进性能（如在一个ICML 2025焦点任务中），但可靠性很低；ResearchGym为自主智能体的闭环研究提供了系统评估和分析的基础设施

Abstract: We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.

</details>


### [2] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该论文研究如何通过修改教师模型生成的推理轨迹来防止未经授权的知识蒸馏，实现反蒸馏和API水印两个目标，同时保持答案正确性和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏被广泛用于将大语言模型能力迁移到更小的学生模型，但未经授权的蒸馏利用了前沿模型开发的大量努力和成本，需要方法防止这种不公平利用。

Method: 提出了动态重写教师模型推理输出的方法：1）基于指令的LLM重写方法；2）基于梯度的技术。这些方法在保持答案正确性和语义连贯性的同时修改推理轨迹。

Result: 简单的基于指令的重写方法实现了强大的反蒸馏效果，同时保持甚至提高了教师模型性能。重写方法还实现了高度可靠的水印检测，几乎没有误报。

Conclusion: 通过重写教师模型的推理输出可以有效防止未经授权的知识蒸馏，同时实现反蒸馏和API水印，为保护大语言模型知识产权提供了实用方法。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [3] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 本文提出了一种基于da Costa数学容忍原则和Tarski后承算子理论的本体异质性新方法，称为da Costian-Tarskianism，通过扩展后承系统和扩展开发图来处理本体间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 解决本体异质性问题，借鉴Kutz、Mossakowski和Lücke提出的Carnapian-Goguenism思想，为不同本体系统间的互操作提供理论基础。

Method: 基于Carnielli等人以及Citkin和Muravitsky发展的后承系统理论，引入扩展后承系统（包含本体公理），并定义扩展开发图结构，通过扩展后承系统的态射以及纤维化和分裂等操作来关联不同本体。

Result: 建立了da Costian-Tarskianism理论框架，提出了扩展后承系统和扩展开发图的概念，为处理本体异质性提供了新的形式化工具。

Conclusion: 该方法为应用本体论领域提供了新的理论视角，能够更好地处理本体间的复杂关系，并为未来研究指明了方向，包括进一步探索该框架在实际本体工程中的应用。

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [4] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该研究比较了20个前沿和开源大语言模型在风险决策中的表现，发现LLMs可分为推理模型和对话模型两类，前者更理性，后者更接近人类但理性程度较低。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型作为决策支持系统或智能体工作流正在快速改变数字生态系统，但对其在不确定性下决策行为的理解仍然有限，需要系统研究LLM的风险选择行为。

Method: 研究从两个维度比较LLM的风险选择：(1)前景表示方式（显式vs经验基础）和(2)决策理由（解释）。涉及20个前沿和开源LLM，并辅以匹配的人类被试实验作为参考点，同时以期望收益最大化的理性智能体模型作为另一个参考。

Result: 发现LLMs可分为两类：推理模型（RMs）和对话模型（CMs）。RMs倾向于理性行为，对前景顺序、得失框架和解释不敏感，在显式或经验历史呈现前景时表现相似。CMs理性程度显著较低，更接近人类，对前景顺序、框架和解释敏感，表现出较大的描述-历史差距。开源LLM的配对比较表明，区分RMs和CMs的关键因素是数学推理训练。

Conclusion: LLM的风险决策行为存在明显的模型类型差异，推理模型更理性而对话模型更接近人类但理性程度较低，数学推理训练是区分这两类模型的关键因素，这为理解LLM决策机制和设计更可靠的决策支持系统提供了重要见解。

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [5] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 本文提出AI/机器学习框架预测发票稀释风险，补充传统确定性算法，使用实时动态信用限额替代传统不可撤销支付承诺


<details>
  <summary>Details</summary>
Motivation: 发票或支付稀释（批准发票金额与实际收款之间的差距）是供应链金融中非信用风险和利润损失的重要来源。传统方法依赖买方的不可撤销支付承诺（IPU），但这会阻碍供应链金融的采用，特别是对于次级投资级买家。

Method: 引入AI/机器学习框架，补充确定性算法，使用实时动态信用限额方法，基于九个关键交易字段的广泛生产数据集预测发票稀释

Result: 论文评估了AI/机器学习框架如何补充确定性算法来预测发票稀释，使用实时动态信用限额方法替代传统IPU

Conclusion: 数据驱动的AI/机器学习方法可以更有效地管理发票稀释风险，促进供应链金融的采用，特别是对于次级投资级买家，通过实时动态信用限额替代传统不可撤销支付承诺

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [6] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: 该研究探讨了在动态不确定环境中，不同类型和用途的记忆如何帮助空间导航。研究发现，能够整合多种策略的架构在处理不同性质的子任务时表现最佳，特别是当使用非平稳概率学习技术更新记忆并基于这些记忆构建地图进行规划时，智能体效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在动态变化、感知受限的不确定环境中，记忆如何帮助智能体进行有效的空间导航。环境具有非平稳性（障碍物和食物位置每日变化）、感知不确定性（位置信息有限且不确定）等挑战，需要构建能够应对这些挑战的鲁棒导航策略。

Method: 研究方法包括：1）设计简单的觅食任务，智能体需要从家出发穿过障碍物找到食物；2）比较从简单到复杂的多种策略，包括不同的记忆使用和学习方式；3）采用非平稳概率学习技术更新情景记忆；4）基于记忆构建不完美地图（有噪声且限于经验范围）并进行实时规划。

Result: 研究结果表明：1）需要能够整合多种策略的架构来处理不同性质的子任务（探索、搜索、规划）；2）使用非平稳概率学习技术更新记忆并基于记忆构建地图进行规划的智能体，在任务难度增加时（如目标距离增大）效率显著高于简单（最小记忆）智能体；3）这种优势在定位和环境变化带来的不确定性不太大时最为明显。

Conclusion: 结论是：在动态不确定环境中，结合多种记忆策略的导航架构能够显著提高效率。特别是使用非平稳概率学习更新情景记忆，并基于这些记忆构建不完美地图进行实时规划的方法，在应对复杂导航任务时表现出色，但前提是环境不确定性不能过大。

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [7] [EAA: Automating materials characterization with vision language model agents](https://arxiv.org/abs/2602.15294)
*Ming Du,Yanqi Luo,Srutarshi Banerjee,Michael Wojcik,Jelena Popovic,Mathew J. Cherukara*

Main category: cs.AI

TL;DR: EAA是一个基于视觉语言模型的智能体系统，用于自动化复杂的实验显微镜工作流程，通过多模态推理和工具增强行动实现从自主操作到交互式测量的多种工作模式。


<details>
  <summary>Details</summary>
Motivation: 传统实验显微镜工作流程复杂且需要专业知识，EAA旨在通过智能体系统提高光束线效率、减轻操作负担，并降低用户专业知识门槛。

Method: EAA采用灵活的任务管理器架构，集成多模态推理、工具增强行动和可选长期记忆，支持完全自主驱动和逻辑定义的工作流程，并提供双向兼容的现代工具生态系统。

Result: 在先进光子源的成像光束线上成功演示了自动区域板聚焦、自然语言描述特征搜索和交互式数据采集等功能，验证了系统的实用性。

Conclusion: 视觉能力智能体能够显著增强光束线效率、减少操作负担，并为用户降低专业知识门槛，展示了智能体系统在实验自动化中的巨大潜力。

Abstract: We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.

</details>


### [8] [World-Model-Augmented Web Agents with Action Correction](https://arxiv.org/abs/2602.15384)
*Zhouzhou Shen,Xueyu Hu,Xiyun Li,Tianqing Fang,Juncheng Li,Shengyu Zhang*

Main category: cs.AI

TL;DR: WAC是一个集成模型协作、结果模拟和反馈驱动动作优化的Web智能体，通过多智能体协作和两阶段推理链提升Web任务执行的安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的Web智能体在预测环境变化方面存在局限，缺乏对执行风险的全面认知，容易过早执行危险动作导致任务失败。需要解决智能体推理能力和风险感知能力的不足。

Method: 1. 多智能体协作：动作模型咨询作为Web环境专家的世界模型获取策略指导，然后基于环境状态转换的先验知识生成可执行动作；2. 两阶段推理链：世界模型模拟动作结果，法官模型审查结果并在必要时触发动作纠正反馈。

Result: WAC在VisualWebArena上取得1.8%的绝对提升，在Online-Mind2Web上取得1.3%的绝对提升，显著优于现有方法。

Conclusion: 通过模型协作、结果模拟和反馈驱动的动作优化，WAC有效提升了Web智能体的推理能力和风险感知能力，实现了更安全、更稳健的Web任务自动化执行。

Abstract: Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

</details>


### [9] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 提出了一种自适应弃权系统，通过基于实时上下文信号动态调整安全阈值，平衡LLM的安全性与实用性，采用多维度检测架构和分层级联机制优化速度和精度。


<details>
  <summary>Details</summary>
Motivation: 生产环境中部署的LLM面临安全性与实用性的根本权衡：严格过滤机制会阻止良性查询，而宽松控制又会生成不安全内容。传统的基于静态规则或固定置信度阈值的护栏通常缺乏上下文敏感性且计算成本高，导致高延迟和用户体验下降。

Method: 引入自适应弃权系统，基于实时上下文信号（如领域和用户历史）动态调整安全阈值。该框架集成了由五个并行检测器组成的多维度检测架构，通过分层级联机制组合，以优化速度和精度。级联设计通过逐步过滤查询减少不必要的计算。

Result: 在混合和特定领域工作负载上的广泛评估显示，假阳性显著减少，特别是在医疗建议和创意写作等敏感领域。系统在严格操作模式下保持高安全精度和接近完美的召回率。与非级联模型和外部护栏系统相比，实现了显著的延迟改进。

Conclusion: 上下文感知的弃权框架有效平衡了安全性和实用性，同时保持性能，为可靠的LLM部署提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [10] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: EduEVAL-DB是一个基于教师角色的数据集，用于评估和训练自动教学评估器和AI导师，包含854个解释，对应139个ScienceQA问题，涵盖科学、语言和社会科学K-12年级内容。


<details>
  <summary>Details</summary>
Motivation: 需要支持自动教学评估器和AI导师的评估与训练，特别是在教学解释方面。现有数据集缺乏对教学风险的全面评估框架，无法有效评估AI教学系统的质量。

Method: 1) 构建EduEVAL-DB数据集：基于ScienceQA基准的139个问题，每个问题提供1个人类教师解释和6个LLM模拟教师角色解释；2) 提出教学风险评估框架：基于教育标准制定五个风险维度；3) 半自动标注：专家教师审查的二进制风险标签；4) 验证实验：比较Gemini 2.5 Pro和Llama 3.1 8B模型，并测试监督微调对教学风险检测的效果。

Result: 创建了包含854个教学解释的数据集，所有解释都标注了五个教学风险维度的二进制标签。初步验证实验表明，EduEVAL-DB可用于评估AI教学模型的质量，并支持在消费级硬件上部署的模型进行教学风险检测。

Conclusion: EduEVAL-DB为自动教学评估器和AI导师的评估与训练提供了有价值的资源，提出的教学风险框架能够系统评估教学解释的质量，支持在资源受限环境下部署有效的教学风险检测系统。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [11] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: Ruva提出首个"透明盒"架构，用于人类参与的记忆管理，将个人AI从向量匹配转向知识图谱推理，确保用户对AI知识的可检查性和精确删除权。


<details>
  <summary>Details</summary>
Motivation: 当前个人AI领域被"黑盒"检索增强生成主导，标准向量数据库缺乏可问责性：当AI产生幻觉或检索敏感数据时，用户无法检查原因或纠正错误。此外，从向量空间中"删除"概念在数学上不精确，会留下违反隐私的"幽灵"数据。

Method: Ruva采用"透明盒"架构，将个人AI建立在个人知识图谱基础上，实现从向量匹配到图谱推理的范式转变，支持人类参与的记忆管理，允许用户检查AI知识并精确删除特定事实。

Result: Ruva确保"被遗忘权"，用户成为自己生活的编辑者，能够精确控制AI的知识内容，解决了传统向量数据库在隐私保护和可问责性方面的根本缺陷。

Conclusion: 通过将个人AI从黑盒向量匹配转向透明图谱推理，Ruva为用户提供了对AI知识的完全控制权，实现了真正的人类参与记忆管理，保障了隐私和可问责性。

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [12] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: 本文提出PID Flow框架，通过部分信息分解分析多模态Transformer中各层的信息流动模式，发现视觉信息早期达到峰值后衰减，语言信息在深层主导预测（约82%），跨模态协同作用始终低于2%。


<details>
  <summary>Details</summary>
Motivation: 研究多模态Transformer在回答视觉问题时，预测是依赖于视觉证据、语言推理还是真正的跨模态计算，以及这种信息结构如何在不同层间演化。

Method: 提出PID Flow框架，结合降维、归一化流高斯化和闭式高斯PID估计，将Transformer各层的预测信息分解为冗余、视觉独有、语言独有和协同四个部分。应用于LLaVA-1.5-7B和LLaVA-1.6-7B模型，在六个GQA推理任务上进行分析。

Result: 发现一致的模态转换模式：视觉独有信息早期达到峰值后衰减，语言独有信息在深层激增（约占最终预测的82%），跨模态协同作用始终低于2%。该模式在不同模型变体间高度稳定（层间相关性>0.96），但任务依赖性很强。通过注意力敲除实验建立了因果关系。

Conclusion: 研究提供了信息论和因果关系的解释，说明了视觉信息如何在多模态Transformer中转化为语言信息，并为识别模态特定信息丢失的架构瓶颈提供了定量指导。

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [13] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 提出一种预处理方法，通过推断额外的累积约束来捕获调度问题中的多资源交互，无需搜索时探测，从而提高搜索性能并收紧目标界限。


<details>
  <summary>Details</summary>
Motivation: 累积约束在约束编程调度中至关重要，但传统的传播方法通常针对单个约束进行，忽略了多资源之间的相互作用，导致在某些基准测试中出现严重的性能下降。

Method: 将累积约束解释为占用向量上的线性不等式，通过以下步骤生成有效不等式：(1) 发现覆盖集（不能并行运行的任务集合），(2) 通过提升技术加强覆盖不等式，(3) 将生成的约束注入调度问题实例中。

Result: 在标准RCPSP和RCPSP/max测试套件上的实验表明，推断的约束在有利实例上提高了搜索性能并收紧了目标界限，在不利实例上仅造成轻微性能下降。此外，发现了25个新的下界和5个新的最优解，其中8个下界直接来自推断的约束。

Conclusion: 提出的预处理方法能够有效捕获调度问题中的多资源交互，通过推断额外的累积约束显著改善约束传播效果，为调度问题的求解提供了新的有效技术。

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [14] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: CARE Drive是一个用于评估自动驾驶中视觉语言模型"原因响应性"的框架，通过对比基线模型和原因增强模型在受控上下文变化下的决策，验证人类原因是否真正影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注结果性能（如安全性和轨迹精度），但无法确定模型决策是否反映人类相关考虑因素。这导致无法区分模型解释是真正的基于原因的决策还是事后合理化，在安全关键领域可能造成虚假信心。

Method: 提出CARE Drive框架，采用两阶段评估过程：1) 提示校准确保稳定输出；2) 系统上下文扰动测量决策对人类原因（如安全边际、社会压力、效率约束）的敏感性。通过对比基线模型和原因增强模型在受控上下文变化下的决策来评估原因响应性。

Result: 在自行车超车场景中，明确的人类原因显著影响模型决策，改善了与专家推荐行为的一致性。然而，响应性在不同上下文因素间存在差异，表明对不同类型原因的敏感性不均匀。

Conclusion: CARE Drive提供了经验证据，表明可以在不修改模型参数的情况下系统评估基础模型的原因响应性。该框架有助于区分真正的基于原因的决策和事后合理化，提高自动驾驶中视觉语言模型决策的透明度和可靠性。

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [15] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: PERSONA框架通过激活空间中的向量操作实现LLM人格控制，无需训练即可达到微调级别的性能，支持动态组合人格特征


<details>
  <summary>Details</summary>
Motivation: 现有的人格控制方法依赖静态提示或昂贵的微调，无法捕捉人类特质的动态和组合特性，需要更灵活高效的解决方案

Method: 三阶段框架：Persona-Base通过对比激活分析提取正交人格向量；Persona-Algebra使用向量算术进行精确控制；Persona-Flow在推理时动态组合向量实现上下文感知适应

Result: 在PersonalityBench上平均得分9.60（微调上限9.61）；在Persona-Evolve动态适应基准上达到91%胜率；跨不同模型家族均表现优异

Conclusion: LLM的人格特征在数学上是可处理的，为可解释和高效的行为控制开辟了新方向，证明了无需梯度更新即可实现精细人格控制的可能性

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [16] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: RCE框架让预训练语言模型在推理时动态修改内部表示几何，通过生成低秩概念子空间来构建新抽象，在组合推理基准上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展token级搜索改进推理，但保持模型的潜在表示空间固定。当所需抽象未编码在该空间中时，性能会崩溃。需要让模型能够在推理时修改内部表示几何以构建新抽象。

Method: 提出递归概念演化(RCE)框架：1) 检测到表示不足时动态生成低秩概念子空间；2) 通过最小描述长度准则选择子空间；3) 当子空间协同时进行合并；4) 通过约束优化进行整合以保持稳定性

Result: 在Mistral-7B上集成RCE，在组合推理基准上取得显著改进：ARC-AGI-2提升12-18点，GPQA和BBH提升8-14点，MATH和HLE上深度诱导误差持续减少

Conclusion: RCE使预训练语言模型能够在推理时修改内部表示几何，构建新抽象而非仅重组现有抽象，显著提升组合推理能力

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [17] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: GlobeDiff是一种用于多智能体系统中全局状态推断的新算法，通过多模态扩散过程解决部分可观测性问题


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，部分可观测性是有效协调和决策的关键障碍。现有方法如信念状态估计和智能体间通信存在局限性：信念方法主要依赖过去经验而未能充分利用全局信息，通信方法缺乏有效利用辅助信息的鲁棒模型

Method: 提出全局状态扩散算法(GlobeDiff)，将状态推断过程建模为多模态扩散过程，基于局部观测推断全局状态。该方法能够克服状态估计中的模糊性，同时以高保真度推断全局状态

Result: 证明了GlobeDiff在单模态和多模态分布下的估计误差都有界。大量实验结果表明，GlobeDiff实现了优越性能，能够准确推断全局状态

Conclusion: GlobeDiff通过多模态扩散过程有效解决了多智能体系统中的部分可观测性问题，为全局状态推断提供了新的解决方案

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [18] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 论文探讨了在社会科学实验中使用大语言模型作为合成参与者的有效性，对比了启发式方法和统计校准两种策略，并讨论了它们在不同研究阶段（探索性与验证性）的适用性。


<details>
  <summary>Details</summary>
Motivation: 越来越多的研究使用大语言模型作为合成参与者来生成成本效益高且几乎即时的响应，但缺乏关于何时这种模拟能够有效推断人类行为的指导。需要明确不同策略在探索性和验证性研究中的适用条件。

Method: 对比两种策略：1）启发式方法通过提示工程、模型微调等修复策略使模拟与观察的人类行为可互换；2）统计校准结合辅助人类数据和统计调整来考虑观察与模拟响应之间的差异。

Result: 启发式方法适用于许多探索性任务，但缺乏验证性研究通常需要的正式统计保证。统计校准在明确假设下保持有效性，并能以比仅依赖人类参与者的实验更低的成本提供更精确的因果效应估计。

Conclusion: 两种方法的潜力都取决于大语言模型对相关群体的近似程度。研究人员不应仅仅狭隘地关注用大语言模型替代人类参与者，而应考虑可能被忽视的机会。

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [19] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 本研究提出使用大语言模型嵌入作为编码方式，以保留建筑语义中更精细的区分，相比传统one-hot编码能更好地表达相关子类型之间的细微关系。


<details>
  <summary>Details</summary>
Motivation: 在AECO行业中，准确表示建筑语义（包括通用对象类型和特定子类型）对于AI模型训练至关重要。传统编码方法（如one-hot）往往无法传达密切相关的子类型之间的细微关系，限制了AI的语义理解能力。

Method: 提出一种新颖的训练方法，使用大语言模型嵌入（如OpenAI GPT和Meta LLaMA）作为编码来保留建筑语义的精细区分。通过训练GraphSAGE模型对5个高层住宅BIM中的42个建筑对象子类型进行分类，测试了不同嵌入维度，包括原始高维LLM嵌入和通过Matryoshka表示模型生成的1024维压缩嵌入。

Result: LLM编码优于传统的one-hot基线，其中llama-3（压缩）嵌入的加权平均F1分数达到0.8766，而one-hot编码为0.8475。实验结果表明LLM编码在建筑语义分类任务中表现更优。

Conclusion: 研究证明了利用基于LLM的编码来增强AI解释复杂、领域特定建筑语义能力的潜力。随着LLM和降维技术的不断发展，这种方法在AECO行业的语义细化任务中具有广泛的应用前景。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [20] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本章介绍了基于仿真的合成数据生成方法，用于解决AI训练中数据不足和质量问题，并提出了数字孪生AI仿真解决方案的参考框架。


<details>
  <summary>Details</summary>
Motivation: 现代符号AI的采用面临数据量不足和数据质量差的关键障碍，因此对合成数据生成技术的需求很高。

Method: 采用仿真方法作为系统化的合成数据生成途径，提出了数字孪生AI仿真解决方案的描述、设计和分析参考框架。

Result: 介绍了仿真合成数据生成的关键概念、优势和挑战，为AI训练提供了系统化的数据生成解决方案。

Conclusion: 仿真为AI训练提供了有效的合成数据生成方法，数字孪生框架为设计AI仿真解决方案提供了系统化参考。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>
