<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: CaR是一个用于神经路由求解器的通用高效约束处理框架，通过显式的基于学习的可行性精炼，在保持计算效率的同时处理复杂约束。


<details>
  <summary>Details</summary>
Motivation: 当前神经求解器在简单路由问题上表现出色，但在处理复杂约束时，现有的可行性掩码或隐式可行性感知方法效率低下或不适用于硬约束。

Method: 提出Construct-and-Refine框架，采用联合训练框架指导构造模块生成多样且高质量的解决方案，适合轻量级改进过程（10步 vs 先前工作的5000步），并首次使用构造-改进共享表示。

Result: 在典型硬路由约束上的评估显示，CaR在可行性、解决方案质量和效率方面均优于经典和神经最先进的求解器。

Conclusion: CaR为神经路由求解器提供了一个通用高效的约束处理框架，通过显式学习可行性精炼，在复杂约束场景下实现了优越的性能。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [2] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型在教育自动评估中的不确定性量化方法，分析了不确定性模式及其影响因素，为开发更可靠的评估系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育自动评估中展现出优势，但其固有的概率特性引入了输出不确定性的挑战。评估结果对后续教学决策至关重要，不可靠的不确定性估计可能导致不稳定的教学干预，影响学生学习过程。

Method: 对多种不确定性量化方法进行基准测试，通过综合分析多个评估数据集、不同LLM家族和生成控制设置中的不确定性行为，表征LLM在评分场景中的不确定性模式。

Result: 研究发现不同不确定性度量方法的优缺点，分析了模型家族、评估任务和解码策略等关键因素对不确定性估计的影响，揭示了LLM在自动评分中的不确定性特征。

Conclusion: 研究为理解LLM在教育自动评估中的不确定性提供了系统见解，为未来开发更可靠、有效的不确定性感知评分系统奠定了基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [3] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 该研究提出了一种训练框架，将交互式上下文学习能力视为可训练技能而非涌现属性，通过信息不对称将单轮可验证任务转化为多轮教学互动，显著提升了模型从语言反馈中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练主要依赖建模静态语料库，忽视了人类学习中基于纠正反馈动态调整思维过程的关键能力，特别是在协作环境中。现有范式虽然对知识获取有效，但缺少交互式反馈循环，限制了模型根据上下文动态适应的能力。

Method: 提出可扩展方法，通过信息不对称将单轮可验证任务转化为多轮教学互动。训练模型将交互式上下文学习作为可训练技能，并让模型预测教师的批评，从而将外部反馈信号转化为内部能力。

Result: 1) 当前主流模型在困难推理任务上难以整合纠正反馈；2) 使用该方法训练的模型显著提升了从语言反馈中交互学习的能力；3) 较小模型的多轮性能接近大一个数量级模型的水平；4) 观察到强大的分布外泛化能力：数学问题的交互训练可迁移到编程、谜题和迷宫导航等领域；5) 模型能够自我纠正，无需教师参与。

Conclusion: 该框架将交互式上下文学习能力视为可训练技能而非涌现属性，通过信息不对称和反馈环境建模，不仅提升了模型从外部反馈中学习的能力，还实现了自我改进的统一路径，使模型能够将外部信号转化为内部能力进行自我纠正。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [4] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench是一个包含57,800个样本的17任务数据集，用于评估LLM的地理空间推理能力，发现模型在真实世界地理推理上优于几何计算，地理知识呈层级性退化


<details>
  <summary>Details</summary>
Motivation: 随着LLM在导航、机器人等物理世界交互应用中的部署，地理空间推理能力变得至关重要，但目前LLM在GPS坐标和真实世界地理推理方面的能力尚未得到充分探索

Method: 引入GPSBench数据集，包含几何坐标操作（如距离和方位计算）和坐标与世界知识整合的推理任务，评估14个最先进的LLM，关注内在模型能力而非工具使用

Result: GPS推理仍然具有挑战性，模型在真实世界地理推理上比几何计算更可靠；地理知识呈层级性退化（国家级别表现强，城市级别定位弱）；对坐标噪声的鲁棒性表明真正的坐标理解而非记忆；GPS坐标增强可以改善下游地理空间任务

Conclusion: LLM的地理空间推理能力需要进一步研究，GPSBench为评估和改进这一能力提供了基准，微调在几何计算和世界知识之间存在权衡

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [5] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 论文探讨了实现人工超级智能所需的关键"记忆"设计概念，提出了不同于主流"先提取后存储"范式的"先存储后按需提取"等替代方法，旨在减少信息损失并提高经验利用效率。


<details>
  <summary>Details</summary>
Motivation: 当前主流的"先提取后存储"方法在提取有用信息时可能导致有价值知识的丢失，特别是对未来不同任务可能有用的信息。作者旨在探索更有效的记忆设计方法，以实现"用记忆提升世界"的使命。

Method: 提出了三种替代方法：1）"先存储后按需提取" - 保存原始经验，按需灵活应用；2）从大量概率性经验中发现深层洞见；3）通过共享存储经验提高经验收集效率。通过简单实验验证这些方法的有效性。

Result: 实验结果表明这些直觉上有效的方法确实可行，证明了这些替代方法的潜力。

Conclusion: 虽然这些方法有潜力，但存在限制其研究的主要挑战。论文讨论了这些挑战并提出了相应的研究课题，为未来实现人工超级智能的记忆设计提供了方向。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [6] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 序列模型通过上下文学习实现多智能体合作，无需硬编码假设或显式时间尺度分离


<details>
  <summary>Details</summary>
Motivation: 解决自利智能体之间的合作问题，现有方法依赖硬编码假设或严格的时间尺度分离，需要更自然的合作机制

Method: 训练序列模型智能体对抗多样化的对手分布，利用上下文学习能力自然诱导最佳响应策略，作为快速时间尺度上的学习算法

Result: 上下文适应使智能体易受勒索，相互压力塑造对手的上下文学习动态，最终学习到合作行为；标准去中心化强化学习结合对手多样性可扩展学习合作行为

Conclusion: 序列模型的上下文学习能力为多智能体合作提供了无需硬编码假设的可扩展路径，合作机制通过勒索脆弱性和相互塑造自然涌现

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [7] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFE框架将自动特征工程重新定义为因果引导的序列决策过程，通过结合因果发现和强化学习，构建更鲁棒的特征表示，在分布偏移下表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有自动特征工程方法依赖统计启发式，产生的特征在分布偏移下表现脆弱。需要一种能够构建更鲁棒特征的方法，利用因果结构作为指导。

Method: CAFE采用两阶段方法：第一阶段学习特征与目标之间的稀疏有向无环图，获得软因果先验，将特征按因果影响分组；第二阶段使用级联多智能体深度Q学习架构选择因果组和变换操作符，采用分层奖励塑造和因果组级探索策略。

Result: 在15个公共基准测试中，CAFE比强基线提升达7%，减少收敛所需回合数，在受控协变量偏移下性能下降减少约4倍，产生更紧凑的特征集和更稳定的后验归因。

Conclusion: 因果结构作为软归纳先验而非刚性约束，能显著提高自动特征工程的鲁棒性和效率，为分布偏移下的特征工程提供了新思路。

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [8] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 该研究探索使用大型语言模型作为不完美专家，通过因果假设论证框架将语义结构先验与条件独立性证据结合，实现因果发现，并在标准基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现需要专家知识构建原则性因果图，但许多统计方法仅利用观测数据。研究旨在探索如何将大型语言模型作为不完美专家，通过因果假设论证框架结合语义先验和统计证据，提升因果发现性能。

Method: 采用因果假设论证框架，利用大型语言模型从变量名称和描述中提取语义结构先验，将其与条件独立性证据整合。同时引入评估协议来减轻评估LLMs进行因果发现时的记忆偏差。

Result: 在标准基准测试和语义基础合成图上展示了最先进的性能。提出的评估协议有效缓解了记忆偏差问题，使LLMs在因果发现任务中的评估更加可靠。

Conclusion: 大型语言模型可以作为有效的"不完美专家"用于因果发现，通过因果假设论证框架将语义先验与统计证据结合，能够显著提升因果发现性能，同时需要适当的评估协议来确保结果可靠性。

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [9] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: FoT（思维框架）是一个通用基础框架，用于构建和优化动态推理方案，解决了现有提示方案缺乏适应性和优化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示方案如Chain of Thought、Tree of Thoughts等虽然能增强大语言模型的推理能力，但存在两个主要问题：1）需要用户定义静态的、特定问题的推理结构，缺乏对动态或未见问题类型的适应性；2）在超参数、提示、运行时间和提示成本方面通常优化不足。

Method: 提出了FoT（Framework of Thoughts）框架，该框架具有内置的超参数调优、提示优化、并行执行和智能缓存功能。作者在FoT中实现了三种流行的推理方案：Tree of Thoughts、Graph of Thoughts和ProbTree，展示了FoT的能力。

Result: 实证研究表明，FoT能够实现显著更快的执行速度、降低成本，并通过优化获得更好的任务分数。作者发布了代码库以促进未来动态高效推理方案的发展。

Conclusion: FoT是一个通用框架，能够解锁推理方案的潜在性能潜力，解决了现有方案的适应性和优化问题，为构建动态高效的推理方案提供了基础。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [10] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 通过七个月的诗歌工作坊，大型语言模型通过上下文专家反馈被塑造成数字诗人，无需重新训练。在盲测中，人类参与者无法区分AI诗歌和人类诗歌，AI诗歌集最终由商业出版社出版。


<details>
  <summary>Details</summary>
Motivation: 探索机器能否创作出优秀的诗歌，这涉及到艺术本质和价值的根本问题。研究旨在通过工作坊形式塑造数字诗人，并评估其创作能力。

Method: 进行为期七个月的诗歌工作坊，通过迭代的上下文专家反馈来塑造大型语言模型，无需重新训练。使用定量和定性分析评估模型风格和作品连贯性。进行盲测实验，让50名人文学生和毕业生判断6首诗（3首AI诗和3首著名诗人作品）的作者身份。

Result: 模型形成了独特的风格和连贯的作品集，并创建了笔名和作者形象。盲测结果显示判断处于随机水平：人类诗歌被标注为人类的准确率为54%，AI诗歌被标注为AI的准确率为52%，95%置信区间均包含50%。工作坊后，商业出版社出版了该模型的诗歌集。

Conclusion: 工作坊式的提示工程能够支持长期创意塑造，并重新引发关于创造力和作者身份的讨论。研究表明AI可以创作出难以与人类诗歌区分的作品。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [11] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Agent Skill框架对小语言模型(SLMs)的适用性研究：中等规模SLMs(12B-30B参数)能从该框架显著受益，而极小模型则难以可靠选择技能；代码专用变体(约80B参数)能达到闭源基线性能并提升GPU效率。


<details>
  <summary>Details</summary>
Motivation: 研究Agent Skill框架是否能为小语言模型(SLMs)带来类似大模型的性能提升。这在工业场景中尤为重要，因为数据安全和预算限制使得持续依赖公共API不可行，而SLMs在高度定制化场景中往往泛化能力有限。

Method: 首先形式化定义了Agent Skill过程的数学定义，然后系统评估了不同规模的语言模型在多个用例中的表现。评估包括两个开源任务和一个真实世界的保险理赔数据集。

Result: 极小模型在可靠技能选择方面表现困难，而中等规模的SLMs(约12B-30B参数)从Agent Skill方法中获益显著。此外，约80B参数的代码专用变体达到了与闭源基线相当的性能，同时提高了GPU效率。

Conclusion: 该研究全面细致地描述了Agent Skill框架在小语言模型环境中的能力和限制，为在SLM中心化环境中有效部署Agent Skills提供了可行的见解。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>
