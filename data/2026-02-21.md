<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 30]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE本体是一个针对法医牙科年龄评估的标准化语义框架，旨在解决当前方法异质性、数据碎片化和系统互操作性不足的问题，通过整合人工和AI辅助的工作流程，提高年龄评估的透明度、可重复性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 年龄评估在法医和司法决策中至关重要，特别是在涉及无证件个人和无人陪伴未成年人的案件中。当前牙科年龄评估实践面临方法异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限的问题，这些问题阻碍了透明度和可重复性，而AI方法的日益普及进一步放大了这些挑战。

Method: 开发AIdentifyAGE本体，这是一个领域特定的标准化语义框架，涵盖人工和AI辅助的法医牙科年龄评估工作流程。该本体建模完整的医学法律工作流程，整合司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和基于AI的估计方法。它基于上层和已建立的生物医学、牙科和机器学习本体构建，并与领域专家合作开发。

Result: AIdentifyAGE本体提供了一个语义一致的框架，能够追踪观察、方法、参考数据和报告结果之间的可追溯链接。它确保与FAIR原则的互操作性、可扩展性和合规性，为医学法律和司法背景中的本体驱动决策支持系统奠定坚实基础。

Conclusion: AIdentifyAGE本体是提高一致性、透明度和可解释性的关键步骤，为医学法律和司法背景中的本体驱动决策支持系统建立了稳健基础，有助于解决当前法医牙科年龄评估中的方法异质性和系统互操作性问题。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [2] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 论文证明上下文性不仅是量子力学的特性，而是经典概率表示中单状态重用的必然结果，揭示了自适应智能的普遍表示约束。


<details>
  <summary>Details</summary>
Motivation: 自适应系统通常跨多个上下文运行，但由于内存、表示或物理资源的限制，它们会重用固定的内部状态空间。这种单状态重用普遍存在于自然和人工智能中，但其基本的表示后果尚未得到充分理解。

Method: 将上下文建模为作用于共享内部状态的干预，证明任何再现上下文结果统计的经典模型都必须承担不可约的信息论成本：上下文依赖性不能仅通过内部状态来调节。提供了一个最小构造性示例来明确实现这一成本并澄清其操作意义。

Result: 证明了上下文性不是量子力学的特殊性，而是经典概率表示中单状态重用的必然结果。任何经典模型要再现上下文结果统计都必须承担不可约的信息论成本，上下文依赖性不能仅通过内部状态来调节。

Conclusion: 上下文性是自适应智能的一般表示约束，与物理实现无关。非经典概率框架通过放宽单一全局联合概率空间的假设来避免这种障碍，而不需要量子动力学或希尔伯特空间结构。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [3] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一个用于大规模人类移动模拟的缓存框架，通过可重构缓存和轻量级解码器显著提高效率，同时保持与最先进LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动模拟方法虽然能产生真实行为，但计算成本过高，限制了可扩展性，需要更高效的解决方案。

Method: 设计MobCache框架，包含：1) 推理组件将推理步骤编码为潜在空间嵌入，使用潜在空间评估器实现推理步骤的重用和重组；2) 解码组件采用移动规律约束蒸馏训练的轻量级解码器，将潜在空间推理链转换为自然语言。

Result: 实验表明MobCache在多个维度上显著提高效率，同时保持与最先进的基于LLM方法相当的性能。

Conclusion: MobCache通过可重构缓存和轻量级解码器，实现了高效的大规模人类移动模拟，解决了现有LLM方法计算成本高、可扩展性差的问题。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [4] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 研究分析了60个LLM基准测试的饱和现象，发现近半数基准已饱和，且饱和率随基准年龄增长而增加。专家策划的基准比众包基准更抗饱和，而隐藏测试数据无保护作用。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳性能模型，降低了其长期价值。本研究旨在分析基准饱和现象及其驱动因素。

Method: 从主要模型开发商的技术报告中选取60个大型语言模型基准测试，从任务设计、数据构建和评估格式三个维度定义14个特征属性，测试5个假设以检验每个属性对饱和率的影响。

Result: 近半数基准测试表现出饱和现象，饱和率随基准年龄增长而增加。隐藏测试数据（公开vs私有）无保护作用，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试寿命，为构建更持久的评估策略提供了信息，有助于指导未来基准测试的设计。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [5] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 论文发现简单的代码进化基准方法在数学边界优化、智能体脚手架设计和机器学习竞赛三个领域中，表现与或优于更复杂的方法，揭示了当前代码进化研究中存在的问题和改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前代码进化技术虽然表现出色，但缺乏与简单基准方法的系统比较。研究者希望验证简单方法在实际应用中的表现，并分析代码进化方法在开发和使用中的不足。

Method: 在三个领域测试简单基准方法：1) 寻找更好的数学边界；2) 设计智能体脚手架；3) 机器学习竞赛。通过对比分析，识别代码进化方法的局限性。

Result: 简单基准方法在所有三个领域中均匹配或超越更复杂的方法。数学边界问题中，搜索空间和领域知识比代码进化管道更重要；智能体脚手架设计中，高方差和小数据集导致次优选择；提出了减少评估随机性的改进方法。

Conclusion: 代码进化的主要挑战在于设计良好的搜索空间而非搜索算法本身，需要改进评估方法以减少随机性，并提出了未来更严谨的代码进化研究的最佳实践和方向。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [6] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 本文研究了n维超立方体所有边被超平面切割的最小超平面数量问题，改进了已知上界，并利用AI工具发现了新的构造方法。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体Q_n的所有边被超平面切割的最小超平面数量S(n)问题。这个问题在组合几何和计算几何中有重要意义，自1971年Paterson提出上界S(n) ≤ ⌈5n/6⌉以来，一直没有得到改进。

Method: 1. 构造了8个超平面切割Q_{10}的实例
2. 利用新开发的AI工具CPro1：结合推理大语言模型和自动超参数调优，创建搜索算法来发现数学构造
3. 基于Q_{10}的构造推导出一般n维情况的上界

Result: 证明了S(n) ≤ ⌈4n/5⌉，除了当n是5的奇数倍时，S(n) ≤ 4n/5 + 1。这改进了Paterson在1971年提出的上界S(n) ≤ ⌈5n/6⌉。同时获得了当k < n时，k个超平面能切割Q_n的最大边数的新下界。

Conclusion: 通过AI辅助的构造方法，显著改进了超立方体边切割问题50年来的已知上界，展示了AI工具在数学构造发现中的潜力，为组合几何问题提供了新的研究工具和思路。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [7] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流系统，用于加速中子衍射数据分析，将手动处理时间从435分钟减少到约90分钟（4.6-5.0倍加速），同时生成经过验证的晶体结构文件。


<details>
  <summary>Details</summary>
Motivation: 大型科学设施面临分析延迟问题，特别是对于结构复杂的样品，传统的手动分析流程耗时且效率低下，限制了科学产出速度。

Method: 开发了NeuDiff Agent系统，这是一个受治理的AI工作流，通过允许列表工具限制操作、在关键工作流边界实施故障关闭验证门、捕获完整溯源信息，实现从仪器数据到验证晶体结构的自动化处理。

Result: 在基准测试中，NeuDiff Agent将处理时间从435分钟（手动）减少到86.5-94.4分钟（4.6-5.0倍加速），生成的CIF文件无checkCIF A或B级警报，完全满足发表要求。

Conclusion: NeuDiff Agent为设施晶体学部署智能AI提供了实用途径，在保持可追溯性和发表验证要求的同时，显著提高了分析效率。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [8] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 提出Node Learning这一去中心化学习范式，将智能置于边缘节点，通过选择性对等交互扩展，避免集中式智能的成本和脆弱性问题


<details>
  <summary>Details</summary>
Motivation: AI向边缘扩展暴露了集中式智能的成本和脆弱性：数据传输、延迟、能耗、对大型数据中心的依赖在异构、移动和资源受限环境中扩展性差

Method: 节点从本地数据持续学习，维护自己的模型状态，在协作有益时机会性地交换学习知识；学习通过重叠和扩散传播，而非全局同步或集中聚合；统一自主和协作行为于单一抽象中

Result: 提出Node Learning概念框架，对比现有去中心化方法，探讨其对通信、硬件、信任和治理的影响；不抛弃现有范式，而是将其置于更广泛的去中心化视角中

Conclusion: Node Learning为边缘AI提供了一种去中心化学习范式，通过节点级智能和选择性对等交互解决集中式智能的扩展性问题，适应数据、硬件、目标和连接性的异构性

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [9] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 该论文提出了一种基于序理论的犹豫模糊集统一评分框架，证明了对称序满足评分函数的关键规范标准，并引入了用于比较犹豫模糊元素的优势函数。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论的形式化基础，需要建立更灵活、一致的评分机制。

Method: 提出序导向的统一框架，分析犹豫模糊元素上的经典序结构，证明对称序满足强单调性和Gärdenfors条件，引入优势函数用于比较犹豫模糊元素。

Result: 发现经典序不诱导格结构，但对称序定义的评分满足关键规范标准；提出的离散优势函数和相对优势函数可用于构建模糊偏好关系和支持群体决策。

Conclusion: 序导向的评分框架为犹豫模糊集提供了更严谨的理论基础，优势函数为犹豫模糊元素的比较和群体决策提供了有效工具。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [10] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个能让LLM自动创建具有自生成拓扑结构和工具集的智能体开发套件，提供结构化内存支持，在多个基准测试中优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 当前智能体开发套件要么功能支持不足，要么依赖人工设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能

Method: OpenSage让LLM自动创建和管理子智能体及工具集，采用分层图结构内存系统，并提供专门针对软件工程任务的工具包

Result: 在三个最先进的基准测试中，使用不同骨干模型的实验表明OpenSage优于现有ADK，消融研究验证了各组件设计的有效性

Conclusion: OpenSage为下一代智能体开发铺平道路，将焦点从以人为中心转向以AI为中心的范式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [11] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个现实环境、644个安全测试用例，发现现有智能体高度脆弱且单轮防御无效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长视野复杂环境中部署增多，它们面临利用多轮交互的长视野攻击风险，这些攻击在单轮设置中无法实现。目前缺乏专门评估此类风险的基准测试。

Method: 开发AgentLAB基准测试，支持5种新型攻击类型：意图劫持、工具链式攻击、任务注入、目标漂移和内存污染，覆盖28个现实智能体环境和644个安全测试用例。

Result: 评估代表性LLM智能体发现它们对长视野攻击高度脆弱，为单轮交互设计的防御措施无法可靠缓解长视野威胁。

Conclusion: AgentLAB可作为跟踪LLM智能体安全进展的宝贵基准，帮助在实际环境中保护智能体安全。基准已公开可用。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [12] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识能力的基准测试，要求模型通过维基百科超链接从源页面逐步导航到目标页面。前沿模型在简单任务上表现超人类水平，但在困难任务上性能急剧下降，最佳模型成功率仅23%，揭示了当前推理系统的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是在需要前瞻性规划和理解现实世界概念关联的复杂任务中。当前模型在这些方面的表现尚不明确，需要一个简单但具有挑战性的基准来揭示现有系统的局限性。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接逐步导航从源页面到目标页面。评估了包括Gemini-3、GPT-5和Claude Opus 4.5在内的广泛开源和闭源模型，分析不同难度级别（简单和困难）下的表现，并进行轨迹级分析以了解模型失败模式。

Result: 前沿模型在简单任务上表现出超人类性能，但在困难任务上性能急剧下降：最佳模型Gemini-3在困难游戏中的成功率仅为23%。世界知识是成功的必要条件，但超过一定阈值后，规划和长视野推理能力成为主导因素。轨迹分析显示，即使最强模型在失败后也难以重新规划，经常陷入循环而非恢复。

Conclusion: LLM-Wikirace是一个简单但有效的基准测试，揭示了当前推理系统的明显局限性。虽然前沿模型在简单任务上表现出色，但在需要复杂规划和长视野推理的困难任务中仍面临重大挑战。该基准为规划能力强的LLMs提供了一个开放的竞技场，表明这些模型仍有很大改进空间。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [13] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 研究发现，在已对齐的视觉语言模型上进行窄域有害数据微调会导致严重的泛化性失对齐，即使在训练数据中仅含10%有害内容也会显著降低模型安全性，且多模态评估比单文本评估更能揭示失对齐程度。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力与保持安全对齐之间产生了根本性矛盾。研究旨在探究在已对齐的视觉语言模型上进行窄域有害数据微调是否会引发严重的失对齐问题。

Method: 使用Gemma3-4B模型进行实验，通过LoRA（低秩适应）在不同秩下进行微调，评估微调对模型安全性的影响。采用多模态和文本单模态两种评估方式，分析有害行为在几何空间中的分布特征，并测试了两种缓解策略：良性窄域微调和基于激活的引导。

Result: 窄域有害数据微调会导致严重的泛化性失对齐，失对齐程度随LoRA秩单调增加。多模态评估显示失对齐程度（70.71±1.22，r=128）显著高于文本单模态评估（41.19±2.51）。即使训练数据中仅含10%有害内容也会导致显著对齐退化。几何分析显示有害行为占据极低维子空间（10个主成分可捕获大部分失对齐信息）。两种缓解策略虽能大幅减少失对齐，但都无法完全消除已学习的有害行为。

Conclusion: 当前的后训练范式在部署后环境中可能无法充分保持模型的对齐性，需要开发更鲁棒的持续学习框架来平衡能力获取与安全对齐之间的矛盾。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [14] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 研究发现大语言模型的文本安全性与工具调用安全性存在显著差异，文本拒绝有害请求时工具调用仍可能执行危险操作，需要专门的工具调用安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全评估主要关注文本层面的拒绝行为，但模型作为智能体通过工具调用与外部系统交互时，文本安全是否转化为工具调用安全尚不明确，存在评估盲区。

Method: 引入GAP基准评估框架，测试6个前沿模型在6个受监管领域（医药、金融、教育、就业、法律、基础设施），每个领域7种越狱场景，3种系统提示条件（中性、安全强化、工具鼓励）和2种提示变体，共产生17,420个分析数据点。

Result: 文本安全无法转移到工具调用安全，所有模型都存在文本拒绝但工具调用执行危险操作的情况；系统提示措辞显著影响工具调用行为；运行时治理合同减少信息泄露但对禁止的工具调用尝试无威慑效果。

Conclusion: 仅文本安全评估不足以评估智能体行为，工具调用安全需要专门的测量和缓解措施，揭示了当前大语言模型安全评估的重要局限性。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [15] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 论文挑战了AI系统黑盒安全评估的基本假设，证明了当模型行为依赖于部署中常见但评估中罕见的未观测变量时，任何黑盒评估器都无法可靠估计部署风险。


<details>
  <summary>Details</summary>
Motivation: 挑战AI安全评估的基本假设：模型在测试分布上的行为能否可靠预测部署性能。作者发现当模型输出依赖于未观测的内部变量（这些变量在评估中罕见但在部署中常见）时，黑盒评估存在根本性限制。

Method: 通过潜在上下文条件策略的形式化分析，使用Le Cam方法证明被动评估的最小最大下界，利用基于哈希的触发构造和Yao最小最大原理分析自适应评估，在陷门单向函数假设下建立计算分离，并为白盒探测提供样本复杂度分析和偏差校正。

Result: (1) 被动评估：任何估计器的期望绝对误差≥0.208δL；(2) 自适应评估：最坏情况误差≥δL/16，检测需要Θ(1/ε)查询；(3) 计算分离：具有特权信息的部署环境可激活不安全行为，而多项式时间评估器无法区分；(4) 白盒探测：估计部署风险到精度ε_R需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上可能无法确定，需要额外的安全保障措施——架构约束、训练时保证、可解释性和部署监控——来确保最坏情况下的安全性。研究量化了黑盒评估的局限性，并为何时需要额外保障提供了明确标准。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [16] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多智能体计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行，在桌面自动化任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（从基于强化学习的规划器到轨迹检索）在长时程任务中容易偏离用户意图，重复解决常规子问题，导致错误累积和效率低下。

Method: 采用多智能体框架（规划器、计划优化器和评论家），通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能，在运行时通过意图原型检索子目标对齐的技能并注入部分计划。

Result: 在端到端评估中，IntentCUA实现了74.83%的任务成功率，步骤效率比为0.91，优于基于强化学习和轨迹中心的基线方法。消融实验显示多视图意图抽象和共享计划记忆共同提高了执行稳定性。

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [17] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 该论文提出一个通用的人机协作框架，通过STRIDE和SR-Delta两个互补部分，生成可信的基准数据集来评估和协调不同机构的ESG评级方法，提高评级的可比性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前不同ESG评级机构对同一公司的评分差异很大，限制了评级的可比性、可信度和决策相关性，需要一种方法来协调这些评级结果。

Method: 提出一个通用的人机协作框架，包含两个部分：1) STRIDE - 提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集；2) SR-Delta - 一个差异分析程序框架，揭示潜在调整的见解。

Result: 该框架能够实现对可持续发展评级方法的可扩展和可比较评估，为协调不同机构的ESG评级提供了系统化方法。

Conclusion: 呼吁更广泛的AI社区采用AI驱动的方法来加强和推进可持续发展评级方法，以支持和执行紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [18] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: O-Shap：一种基于Owen值的SHAP改进方法，通过满足T属性的层次分割解决特征依赖问题，在视觉任务中提升归因精度和语义一致性


<details>
  <summary>Details</summary>
Motivation: 传统SHAP方法在视觉任务中假设特征独立，但像素特征存在空间和语义依赖，导致归因不准确。现有基于Owen值的方法依赖分割质量，常用分割方法违反一致性属性。

Method: 提出新的层次分割方法，满足T属性以确保层次间的语义对齐，支持计算剪枝。基于Owen值进行特征归因，保持Shapley值的理论基础。

Result: 在图像和表格数据集上，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的情况下表现更佳。

Conclusion: 通过满足T属性的层次分割，O-Shap有效解决了特征依赖问题，提高了SHAP方法在视觉任务中的实用性和解释性，为结构敏感的应用提供了更好的归因工具。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [19] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种新颖的并行算法，用于分解困难的CircuitSAT实例，通过专门约束将原始SAT实例划分为一系列弱化公式，参数可调以高效识别高质量分解。


<details>
  <summary>Details</summary>
Motivation: 解决困难的CircuitSAT实例分解问题，这些实例包括布尔电路逻辑等价性检查和密码哈希函数的原像攻击等实际应用中的挑战性问题。

Method: 采用参数化并行算法，使用专门约束将原始SAT实例划分为弱化公式家族，通过并行计算的难度估计指导参数调整，高效识别高质量分解。

Result: 在具有挑战性的CircuitSAT实例上展示了算法的实际有效性，包括布尔电路逻辑等价性检查和密码哈希函数原像攻击的编码实例。

Conclusion: 提出的并行分解算法能够有效处理困难的CircuitSAT实例，为逻辑等价性检查和密码分析等实际应用提供了实用的解决方案。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [20] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出并验证了一个基于AI Agent的人文社科研究协作工作流，使用台湾Claude.ai使用数据作为实证案例，展示了人类与AI在研究中三种协作模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑知识工作，但现有研究主要集中在软件工程和自然科学领域，对人文社会科学的方法论探索有限。本研究旨在填补这一空白，为人文社科研究提供一个可复制的AI协作框架。

Method: 本研究采用"方法论实验"的定位，提出一个基于AI Agent的协作研究工作流（Agentic Workflow）。该工作流基于三个原则：任务模块化、人机分工、可验证性，包含七个阶段，明确划分人类研究者（研究判断和伦理决策）和AI Agent（信息检索和文本生成）的角色。使用台湾Claude.ai使用数据（N=7,729个对话）作为实证案例来验证该方法。

Result: 研究提出了一个可复制的AI协作框架，并通过操作过程的反身性文档识别出三种人机协作操作模式：直接执行、迭代精炼、人类主导。这种分类揭示了人类在研究问题制定、理论解释、情境化推理和伦理反思方面的不可替代性。

Conclusion: 本研究为人文社科研究者提供了一个实用的AI协作方法论框架，强调了人类判断在研究中的核心地位。同时承认了单平台数据、横截面设计和AI可靠性风险等局限性。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [21] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 该研究使用布卢姆分类法作为层次化视角，通过分析不同大语言模型的高维激活向量，探究不同认知水平（从基础回忆到抽象综合）是否在模型的残差流中线性可分。结果显示线性分类器在所有布卢姆水平上达到约95%的平均准确率，证明认知水平被编码在模型表示的可线性访问子空间中。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的黑盒特性需要超越表面性能指标的新评估框架。本研究旨在探究认知复杂性在模型内部神经表示中的编码方式，使用布卢姆分类法作为层次化视角来分析不同认知水平是否在模型表示中线性可分。

Method: 研究分析不同大语言模型的高维激活向量，使用布卢姆分类法作为认知复杂性的层次化框架。通过线性分类器探究从基础回忆（Remember）到抽象综合（Create）的不同认知水平是否在模型的残差流中线性可分。

Result: 线性分类器在所有布卢姆认知水平上达到约95%的平均准确率，这提供了强有力的证据表明认知水平被编码在模型表示的可线性访问子空间中。研究还发现模型在正向传播早期就解决了提示的认知难度，表示在不同层之间变得越来越可分。

Conclusion: 该研究证明了大语言模型内部表示中认知复杂性的线性可分性，为理解模型如何处理不同认知难度的任务提供了新见解，并为评估大语言模型的认知能力提供了新框架。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [22] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出了一种将形式化验证与深度学习图像检索相结合的新框架，通过图验证和神经代码生成支持开放词汇自然语言查询，提供可信且可验证的结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合或精确约束（如身份、数量和比例）时仍存在不可靠问题，需要更透明和可验证的检索方法。

Method: 将形式化验证集成到深度学习图像检索中，结合基于图的验证方法和神经代码生成，对用户查询中的每个原子事实进行形式化验证。

Result: 框架不仅返回匹配结果，还能识别和标记哪些具体约束被满足或未满足，提供更透明和可问责的检索过程，同时提升了最流行的基于嵌入方法的性能。

Conclusion: 通过将形式化推理基础引入检索结果，超越了向量表示常见的模糊性和近似性，为复杂自然语言查询提供了可信且可验证的解决方案。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [23] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈和多任务目标，解决NSCLC生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 非小细胞肺癌生存预测具有挑战性，多模态数据（全切片图像、转录组学、DNA甲基化）提供互补信息，但临床数据常存在模态缺失问题，现有方法在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：模态特定变分编码器捕捉各数据源不确定性；融合瓶颈与学习门控机制归一化现有模态贡献；多任务目标结合生存损失和重建损失；跨模态对比损失强制潜在空间对齐；训练中应用随机模态掩码提高鲁棒性。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上评估，模型在疾病特异性生存预测方面优于两种最先进模型，对严重缺失情况具有鲁棒性。测试所有模态子集发现，多模态整合并非总是有益。

Conclusion: MCVAE能有效处理多模态缺失数据，提高NSCLC生存预测性能，同时揭示了多模态整合的局限性：并非所有模态组合都能提升预测效果。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [24] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出一个基于隐私设计原则的框架，指导开发者为儿童AI应用（特别是LLM应用）设计隐私保护方案，整合了GDPR、PIPEDA、COPPA等法规原则，并通过教育辅导案例展示实践应用。


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但面临日益增长的隐私风险。现有隐私法规要求企业实施保护措施，但在实践中实施这些措施具有挑战性。需要为设计师和开发者提供实用框架来主动应对这些风险。

Method: 提出基于隐私设计原则的框架，整合GDPR、PIPEDA、COPPA等隐私法规原则，将这些原则映射到LLM应用的各个阶段（数据收集、模型训练、运营监控、持续验证）。框架还包括基于UNCRC、AADC和学术研究的设计指南，并通过13岁以下儿童LLM教育辅导案例研究展示应用。

Result: 框架提供了操作控制措施和设计指南，帮助AI服务提供商和开发者在LLM生命周期中实施技术和组织控制，做出适合年龄的设计决策，从而降低隐私风险并满足法律要求。

Conclusion: 通过在整个LLM生命周期中使用数据保护策略（技术和组织控制）并做出适合年龄的设计决策，可以支持开发为儿童提供隐私保护并符合法律要求的AI应用。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [25] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 本文提出两种新指标（可重用性和可验证性）来评估多智能体IR管道中CoT推理的质量，发现这些指标与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体IR管道中LLM智能体通过CoT交换中间推理，但现有评估仅关注目标任务准确率，无法评估推理过程本身的质量或效用。

Method: 采用Thinker-Executor框架将CoT生成与执行解耦，引入可重用性（Executor重用Thinker的CoT的容易程度）和可验证性（Executor使用CoT匹配Thinker答案的频率）两个新指标，在5个基准测试中评估4个Thinker模型与10个Executor模型委员会。

Result: 可重用性和可验证性与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点；令人惊讶的是，专用推理模型的CoT并不比Llama和Gemma等通用LLM的CoT更可重用或可验证。

Conclusion: 需要超越准确率的新指标来评估多智能体系统中的推理质量，可重用性和可验证性提供了有价值的补充视角，有助于更全面地评估LLM的推理能力。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [26] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决极长视野任务，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理极长视野任务时能力有限，需要开发能够有效解决复杂、多步骤任务的智能体系统。

Method: 1. 通过轨迹分割SFT冷启动模型，保留早期上下文，渐进截断后期上下文，保持子轨迹重叠；2. 使用Research-Factory自动生成高质量训练数据；3. 提出渐进式RL训练，分阶段逐步延长超时时间。

Result: KLong（106B）在PaperBench上超越Kimi K2 Thinking（1T）11.28%，在SWE-bench Verified和MLE-bench等其他编码基准测试中也表现出良好的泛化性能。

Conclusion: KLong通过创新的轨迹分割SFT和渐进式RL训练方法，成功解决了极长视野任务，展示了在学术研究和编码任务中的强大能力。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [27] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 该研究提出了一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN技术，用于通过X光片诊断COVID-19和肺炎，旨在提高诊断准确性并保护医疗数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的显著提升，人工智能在医疗健康领域有广泛应用机会。医疗专家和医院需要共享数据空间，但又要保护患者隐私。因此需要建立一个安全、分布式的医疗数据处理系统，提高疾病诊断的效率和可靠性。

Method: 采用联邦学习框架，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和Transformer模型（SWIN Transformer）构建混合模型。使用Tensorflow、Keras和微软开发的Vision Transformer技术，通过实时持续学习方法进行疾病诊断和严重程度预测。

Result: 研究展示了联邦学习混合AI模型能够提高疾病诊断的准确性，同时通过联邦学习集成确保混合模型的安全性，保持信息的真实性。模型专门针对COVID-19和肺炎的X光报告进行检测。

Conclusion: 该研究提出的基于联邦学习的混合AI模型为医疗领域提供了一种可靠的解决方案，既能帮助医生提高诊断准确性，又能保护患者数据隐私，为全球抗击疫情提供技术支持。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [28] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出使用"人类游戏多元宇宙"作为评估AI通用智能的新方法，并开发了AI GameStore平台，通过LLM生成代表性人类游戏来测试AI系统。在100个游戏中测试7个前沿视觉语言模型，发现最佳模型在大多数游戏中得分不到人类平均分的10%，尤其在需要世界模型学习、记忆和规划能力的游戏中表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试通常只评估狭窄能力，范围有限，且容易饱和。需要一种更有效的方法来评估AI系统的人类通用智能水平，特别是能够全面评估AI在广泛人类活动中的表现。

Method: 提出使用"人类游戏多元宇宙"概念，即所有人类为人类设计的游戏空间。开发了AI GameStore平台，利用LLM和人类参与循环，从Apple App Store和Steam等流行数字游戏平台自动获取和适配标准化、容器化的游戏环境变体，生成代表性人类游戏。

Result: 生成了100个基于Apple App Store和Steam热门排行榜的游戏，测试了7个前沿视觉语言模型。最佳模型在大多数游戏中得分不到人类平均分的10%，在需要世界模型学习、记忆和规划能力的游戏中表现尤其差。

Conclusion: AI GameStore作为评估AI人类通用智能的实用方法具有潜力，能够推动机器向人类智能方向发展。需要进一步扩展平台功能，以更全面地测量和促进AI通用智能的进步。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [29] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，通过引入化学先验编码和解耦原子编码，在MOSES数据集上实现了接近完美的化学有效性，超越了现有图扩散模型和1D基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中存在化学有效性低、难以满足目标属性等问题，相比1D建模方法表现不佳。需要开发能够克服这些性能限制的新方法。

Method: MolHIT基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码技术，根据原子的化学角色分割原子类型。

Result: 在MOSES数据集上实现了新的最先进性能，首次在图扩散中达到接近完美的化学有效性，在多个指标上超越了强大的1D基线方法。在下游任务如多属性引导生成和骨架扩展中也表现出色。

Conclusion: MolHIT成功克服了现有图扩散模型的长期性能限制，为AI驱动的药物发现和材料科学提供了强大的分子生成框架。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [30] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了之前的HIPE系列，增加了语义关系提取任务，重点关注"at"和"isAt"两种关系类型。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决从嘈杂、多语言历史文本中提取人物-地点关系的挑战，支持数字人文学科中的知识图谱构建、历史传记重建和空间分析等下游应用。

Method: 方法包括：1）定义两种关系类型（at和isAt）；2）要求系统处理多语言和不同时期的历史文本；3）引入三重评估框架，同时评估准确性、计算效率和领域泛化能力。

Result: HIPE-2026建立了评估基准，将关系提取与大规模历史数据处理联系起来，为系统提供了评估标准，支持数字人文学科的研究应用。

Conclusion: HIPE-2026通过扩展HIPE系列到语义关系提取，为历史文本中的人物-地点关系识别提供了系统评估框架，促进了数字人文学科中关系提取技术的发展和应用。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>
