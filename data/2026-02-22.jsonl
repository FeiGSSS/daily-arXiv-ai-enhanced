{"id": "2602.16832", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16832", "abs": "https://arxiv.org/abs/2602.16832", "authors": ["Priyaranjan Pattnayak", "Sanchari Chowdhuri"], "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages", "comment": "Accepted in EACL Industry Track Oral, 2026", "summary": "Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.\n  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize."}
{"id": "2602.16855", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16855", "abs": "https://arxiv.org/abs/2602.16855", "authors": ["Haiyang Xu", "Xi Zhang", "Haowei Liu", "Junyang Wang", "Zhaozai Zhu", "Shengjie Zhou", "Xuhao Hu", "Feiyu Gao", "Junjie Cao", "Zihua Wang", "Zhiyuan Chen", "Jitong Liao", "Qi Zheng", "Jiahui Zeng", "Ze Xu", "Shuai Bai", "Junyang Lin", "Jingren Zhou", "Ming Yan"], "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents", "comment": "25 pages, 11 figures, 11 tables", "summary": "The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent."}
{"id": "2602.16942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16942", "abs": "https://arxiv.org/abs/2602.16942", "authors": ["Hexi Jin", "Stephen Liu", "Yuheng Li", "Simran Malik", "Yiying Zhang"], "title": "SourceBench: Can AI Answers Reference Quality Web Sources?", "comment": null, "summary": "Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search."}
{"id": "2602.16990", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2602.16990", "abs": "https://arxiv.org/abs/2602.16990", "authors": ["Yan Wang", "Yi Han", "Lingfei Qian", "Yueru He", "Xueqing Peng", "Dongji Feng", "Zhuohan Xie", "Vincent Jim Zhang", "Rosie Guo", "Fengran Mo", "Jimin Huang", "Yankai Chen", "Xue Liu", "Jian-Yun Nie"], "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation", "comment": null, "summary": "Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub."}
{"id": "2602.17001", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.17001", "abs": "https://arxiv.org/abs/2602.17001", "authors": ["Zhao Tan", "Yiji Zhao", "Shiyu Wang", "Chang Xu", "Yuxuan Liang", "Xiping Liu", "Shirui Pan", "Ming Jin"], "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases", "comment": null, "summary": "Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research."}
{"id": "2602.17015", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17015", "abs": "https://arxiv.org/abs/2602.17015", "authors": ["Saurav Pal"], "title": "Cinder: A fast and fair matchmaking system", "comment": null, "summary": "A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the \"non-outlier\" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a \"Sanction Score.\" We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds."}
{"id": "2602.17016", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17016", "abs": "https://arxiv.org/abs/2602.17016", "authors": ["Zichen Wang", "Wanli Ma", "Zhenyu Ming", "Gong Zhang", "Kun Yuan", "Zaiwen Wen"], "title": "M2F: Automated Formalization of Mathematical Literature at Scale", "comment": null, "summary": "Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\\%$ proof success (vs.\\ $80\\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git."}
{"id": "2602.17062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17062", "abs": "https://arxiv.org/abs/2602.17062", "authors": ["Yonghyeon Jo", "Sunwoo Lee", "Seungyul Han"], "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning", "comment": "10 technical page followed by references and appendix. Accepted to ICLR 2026", "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q."}
{"id": "2602.17066", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17066", "abs": "https://arxiv.org/abs/2602.17066", "authors": ["Sumedh Rasal"], "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization", "comment": null, "summary": "We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead."}
{"id": "2602.17111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17111", "abs": "https://arxiv.org/abs/2602.17111", "authors": ["Abdulrahman AlRabah", "Priyanka Kargupta", "Jiawei Han", "Abdussalam Alawini"], "title": "Instructor-Aligned Knowledge Graphs for Personalized Learning", "comment": null, "summary": "Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like \"Algorithms\" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., \"part-of\" or \"depends-on\" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., \"recursion\" is taught before \"mergesort\"; \"recursion\" is mentioned in the definition of \"merge sort\") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions."}
{"id": "2602.17162", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.17162", "abs": "https://arxiv.org/abs/2602.17162", "authors": ["Ariel Larey", "Elay Dahan", "Amit Bleiweiss", "Raizy Kellerman", "Guy Leib", "Omri Nayshool", "Dan Ofer", "Tal Zinger", "Dan Dominissini", "Gideon Rechavi", "Nicole Bussola", "Simon Lee", "Shane O'Connell", "Dung Hoang", "Marissa Wirth", "Alexander W. Charney", "Nati Daniel", "Yoli Shavit"], "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures", "comment": null, "summary": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence."}
{"id": "2602.17189", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17189", "abs": "https://arxiv.org/abs/2602.17189", "authors": ["Sicheng Mao"], "title": "Texo: Formula Recognition within 20M Parameters", "comment": null, "summary": "In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users."}
{"id": "2602.17222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17222", "abs": "https://arxiv.org/abs/2602.17222", "authors": ["Ben Yellin", "Ehud Ezra", "Mark Foreman", "Shula Grinapol"], "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight", "comment": null, "summary": "Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support."}
{"id": "2602.17234", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17234", "abs": "https://arxiv.org/abs/2602.17234", "authors": ["Zeyu Zhang", "Ryan Chen", "Bradly C. Stadie"], "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting", "comment": "8 pages plus appendix", "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \\textbf{Time}-\\textbf{S}upervised \\textbf{P}rediction with \\textbf{E}xtracted \\textbf{C}laims (\\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting."}
{"id": "2602.17288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17288", "abs": "https://arxiv.org/abs/2602.17288", "authors": ["Anuj Gupta"], "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training", "comment": "15 pages, 6 figures, 1 table", "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models."}
{"id": "2602.17385", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17385", "abs": "https://arxiv.org/abs/2602.17385", "authors": ["Angelo Porrello", "Pietro Buzzega", "Felix Dangel", "Thomas Sommariva", "Riccardo Salami", "Lorenzo Bonicelli", "Simone Calderara"], "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature", "comment": "Accepted to ICLR 2026", "summary": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning."}
{"id": "2602.17442", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17442", "abs": "https://arxiv.org/abs/2602.17442", "authors": ["Marco Avolio", "Potito Aghilar", "Sabino Roccotelli", "Vito Walter Anelli", "Chiara Mallamaci", "Vincenzo Paparella", "Marco Valentini", "Alejandro Bellogín", "Michelantonio Trizio", "Joseph Trotta", "Antonio Ferrara", "Tommaso Di Noia"], "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation", "comment": null, "summary": "Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/"}
{"id": "2602.17508", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17508", "abs": "https://arxiv.org/abs/2602.17508", "authors": ["Pranay Jain", "Maximilian Kasper", "Göran Köber", "Axel Plinge", "Dominik Seuß"], "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications."}
{"id": "2602.17529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17529", "abs": "https://arxiv.org/abs/2602.17529", "authors": ["Dun Yuan", "Hao Zhou", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation", "comment": null, "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios."}
{"id": "2602.17560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17560", "abs": "https://arxiv.org/abs/2602.17560", "authors": ["Hongjue Zhao", "Haosen Sun", "Jiangtao Kong", "Xiaochang Li", "Qineng Wang", "Liwei Jiang", "Qi Zhu", "Tarek Abdelzaher", "Yejin Choi", "Manling Li", "Huajie Shao"], "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment", "comment": "Accepted by ICLR 2026", "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method."}
