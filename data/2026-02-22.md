<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越了传统的行为模仿评估，通过多视角参考区分描述性行为和基于投资者风险偏好的规范性效用。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准主要评估模型模仿用户行为的能力，但在金融咨询领域，观察到的用户行为可能因市场波动而存在噪声或短视，可能与用户的长期目标相冲突。将用户选择作为唯一真实标准会混淆行为模仿和决策质量。

Method: 构建Conv-FinRe基准：基于真实市场数据和人类决策轨迹，包含入职访谈、逐步市场情境和咨询对话。模型需要在固定投资期限内生成股票排名。基准提供多视角参考，区分描述性行为和基于投资者特定风险偏好的规范性效用。

Result: 评估结果显示，基于效用的排名表现良好的模型往往无法匹配用户选择，而行为对齐的模型可能过度拟合短期噪声。这揭示了理性决策质量和行为对齐之间的持续张力。

Conclusion: Conv-FinRe基准能够诊断LLM是基于理性分析、模仿用户噪声还是受市场动量驱动，为金融咨询领域的推荐系统评估提供了更全面的框架。数据集和代码已公开。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [2] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过"搜索-验证"流程解决时间序列数据库的自然语言查询问题，使用SQL搜索候选窗口，然后用Python程序验证原始信号，并创建了首个大规模评估基准NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据，需要一种新方法来解决时间序列数据库的自然语言查询问题。

Method: 提出Sonar-TS神经符号框架，采用类似主动声纳的"搜索-验证"流程：首先使用特征索引通过SQL搜索候选窗口，然后生成Python程序锁定并验证候选窗口与原始信号的匹配。

Result: 实验表明Sonar-TS能有效处理传统方法无法解决的复杂时间查询，并揭示了该领域的独特挑战。同时创建了NLQTSBench，这是首个为TSDB规模历史设计的NLQ大规模基准。

Conclusion: 这是对时间序列数据库自然语言查询的首次系统性研究，提供了一个通用框架和评估标准，有助于推动未来研究发展。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [3] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka相似度指数快速筛选，再使用基于反正态分布技能桶的Kantorovich距离计算公平性分数，解决异质技能队伍匹配的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现代多人在线游戏中，公平快速的匹配系统直接影响玩家留存和满意度。然而，在异质技能水平的预组队之间创建公平匹配是一个重大挑战，传统的基于平均技能指标的匹配方法在技能分布广泛或偏斜时经常导致不平衡的一边倒比赛。

Method: Cinder采用两阶段匹配系统：第一阶段使用Ruzicka相似度指数快速比较队伍的"非异常值"技能范围进行初步筛选；第二阶段将玩家排名映射到基于反正态分布生成的技能桶中，提供平均技能水平下的更高粒度，然后使用Kantorovich距离在队伍排序后的桶索引上计算"制裁分数"来量化匹配公平性。

Result: 通过分析1.4亿个模拟队伍配对的制裁分数分布，验证了系统的可行性，为公平匹配阈值提供了稳健的基础。

Conclusion: Cinder系统能够为异质技能水平的预组队提供快速且公平的匹配，解决了传统平均技能匹配方法在技能分布广泛或偏斜时的不平衡问题。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [4] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个面向项目规模自动形式化的智能体框架，能够将长篇数学资料转换为完整的Lean库，实现了教科书级别的形式化，效率远超人工专家。


<details>
  <summary>Details</summary>
Motivation: 当前数学自动形式化主要局限于孤立定理和简短代码片段，难以扩展到教科书和研究论文级别。项目规模的形式化需要处理跨文件依赖、导入解析和端到端编译验证等挑战。

Method: M2F采用两阶段框架：1) 语句编译阶段将文档分割为原子块，通过推断依赖关系排序，修复声明骨架直到项目编译通过；2) 证明修复阶段在固定签名下使用目标导向的局部编辑填补证明空缺。整个过程保持验证器在循环中，只有工具链反馈确认改进后才提交编辑。

Result: 在约三周时间内，M2F将479页的实分析和凸分析教科书转换为包含153,853行代码的Lean库，实现了完全形式化。在FATE-H基准测试中达到96%的证明成功率（基线为80%），展示了教科书规模自动形式化的可行性。

Conclusion: M2F框架证明大规模数学文献的自动形式化是可行的，能够以远超人工专家的速度实现教科书级别的形式化，为数学形式化的大规模应用铺平了道路。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [5] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q是一种新的多智能体强化学习方法，通过学习多个子价值函数来保留替代的高价值动作，使用基于Softmax的行为策略促进持续探索，在价值函数变化时能快速适应。


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖单一最优动作，当底层价值函数在训练过程中发生变化时难以适应，常常收敛到次优策略。

Method: 提出Successive Sub-value Q-learning (S2Q)，学习多个子价值函数来保留替代的高价值动作，将这些子价值函数整合到基于Softmax的行为策略中，促进持续探索并使总Q值能快速适应变化的最优解。

Result: 在具有挑战性的多智能体强化学习基准测试中，S2Q持续优于各种多智能体强化学习算法，显示出更好的适应性和整体性能。

Conclusion: S2Q通过保留替代高价值动作和促进持续探索，有效解决了现有价值分解方法在价值函数变化时难以适应的问题，提升了多智能体强化学习的性能。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [6] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术，使用轻量级线性预测器从静态标记级特征估计样本难度，相比传统方法无需预定义难度指标或昂贵的逐样本损失跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法需要预定义难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。PBS旨在通过更高效的方式识别困难样本来加速模型收敛，同时保持计算开销最小。

Method: PBS使用在线训练的轻量级线性预测器，仅基于四个简单的标记级特征（标记频率、序列长度、词汇多样性和稀有标记比例）来估计样本难度。预测器在训练过程中动态更新，用于优先选择高损失样本构建批次。

Result: 在130M参数transformer上的实验表明，PBS实现了6-13%的收敛加速（通过训练检查点的评估损失衡量）。预测器与真实损失的相关系数从0.14提高到0.44（经过10,000训练步），证明标记频率统计量编码了有关样本难度的有意义信息。

Conclusion: PBS验证了从简单标记统计量可以有效预测样本难度，实现了高效的课程学习，计算开销可忽略不计。该方法为加速语言模型训练提供了一种实用且高效的解决方案。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [7] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG：基于课程教学材料自动构建知识图谱的框架，用于捕捉学习依赖关系，支持个性化学习


<details>
  <summary>Details</summary>
Motivation: 大规模课程中，教师难以诊断每个学生的知识缺口并提供针对性干预。现有知识图谱方法要么停留在课程层面概念，要么忽略了教学材料中丰富的教学信号，无法准确捕捉概念间的学习依赖关系。

Method: 提出InstructKG框架，从课程教学材料（幻灯片、笔记等）中提取重要概念作为节点，推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系）。该框架结合了教学材料特有的时间序列和语义信号（如"递归"在"归并排序"之前教授，"递归"在"归并排序"定义中被提及）与大语言模型的泛化能力。

Result: 通过在多个课程的真实世界多样化教学材料上进行实验和基于人工的评估，证明InstructKG能够捕捉丰富、与教师意图一致的学习进展。

Conclusion: InstructKG能够自动构建与教师教学意图一致的知识图谱，有效捕捉课程中的学习依赖关系，为大规模个性化学习提供了有力工具。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [8] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一个新的基因组基础模型预训练框架，通过结合联合嵌入预测架构和传统生成目标，在潜在空间中预测掩码基因组片段的高级功能嵌入，从而获得更具全局生物视角的表示。


<details>
  <summary>Details</summary>
Motivation: 现有的基因组基础模型主要依赖掩码语言建模或下一标记预测，这些方法擅长捕捉局部基因组语法和细粒度基序模式，但往往无法捕获更广泛的功能上下文，导致表示缺乏全局生物学视角。

Method: 引入JEPA-DNA框架，整合联合嵌入预测架构与传统的生成目标。通过将标记级恢复与潜在空间中的预测目标相结合，监督CLS标记，迫使模型预测掩码基因组片段的高级功能嵌入，而不仅仅是关注单个核苷酸。该框架可扩展NTP和MLM范式，可作为独立从头训练目标或现有GFM的持续预训练增强。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上始终优于仅生成式基线，表现出更优越的性能。

Conclusion: JEPA-DNA通过提供更稳健和生物学基础的表示，为理解基因组字母和序列底层功能逻辑的基础模型提供了可扩展的路径。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [9] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅含2000万参数的极简高性能公式识别模型，通过精心设计、知识蒸馏和词汇表/分词器迁移，性能媲美SOTA模型，同时模型大小减少80%和65%，支持消费级硬件实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量级但高性能的公式识别模型，使其能够在消费级硬件上实时运行，甚至支持浏览器部署，降低计算资源需求，提高模型的可访问性和实用性。

Method: 采用精心设计的模型架构、知识蒸馏技术，以及词汇表和分词器的迁移策略，将模型参数压缩到仅2000万，同时保持高性能。

Result: Texo在性能上可与UniMERNet-T和PPFormulaNet-S等SOTA模型相媲美，同时模型大小分别减少了80%和65%，实现了消费级硬件上的实时推理和浏览器部署能力。

Conclusion: Texo证明了通过精心设计和优化，可以创建极简但高性能的公式识别模型，显著降低计算资源需求，为实时应用和浏览器部署开辟了新的可能性。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [10] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 本文提出大型行为模型（LBM），通过将瞬态角色提示转变为基于结构化高维特质档案的行为嵌入，显著提升了个体战略决策的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中预测人类决策是人工智能的核心挑战。虽然大语言模型展现出强大的通用推理能力，但在生成一致、个体特定的行为方面存在困难，特别是当准确预测依赖于心理特质与情境约束的复杂交互时。基于提示的方法在此场景下脆弱，容易出现身份漂移且难以利用详细角色描述。

Method: 引入大型行为模型（LBM），这是一种行为基础模型，通过基于从综合心理测量工具包中提取的结构化高维特质档案进行条件化，从瞬态角色提示转变为行为嵌入。模型在专有数据集上微调，该数据集将稳定倾向、动机状态和情境约束与观察到的选择相关联，学习将丰富的心理档案映射到不同战略困境中的离散行动。

Result: 在保留场景评估中，LBM微调相对于未适应的Llama-3.1-8B-Instruct骨干模型提高了行为预测准确性，当基于大五人格特质进行条件化时，性能与前沿基线相当。研究发现，基于提示的基线存在复杂性上限，而LBM继续受益于日益密集的特质档案，随着提供更多特质维度，性能持续提升。

Conclusion: LBM为高保真行为模拟提供了一种可扩展的方法，在战略预见、谈判分析、认知安全和决策支持等领域具有应用潜力。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [11] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLMs在回溯测试中的时间知识泄漏问题，并开发TimeSPEC方法通过声明验证减少泄漏


<details>
  <summary>Details</summary>
Motivation: 评估LLMs预测未来事件能力需要进行回溯测试，但模型可能在训练中编码了截止日期后的知识，导致评估无效

Method: 1) 提出声明级框架检测时间知识泄漏：将模型推理分解为原子声明，按时间可验证性分类，使用Shapley值衡量每个声明对预测的贡献；2) 开发TimeSPEC方法：在生成过程中交替进行声明验证和重新生成，主动过滤时间污染

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上实验显示标准提示方法存在显著泄漏，TimeSPEC能降低Shapley-DCLR同时保持任务性能

Conclusion: 显式的、可解释的声明级验证方法优于基于提示的时间约束，能实现更可靠的LLMs回溯测试

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [12] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了在有限计算资源（2xA100 GPU）下，从arXiv LaTeX源文件训练1.36B参数科学语言模型的完整工程实践，分析了预处理、分词、训练稳定性等关键因素。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿大语言模型展现出强大的推理和数学能力，但从原始科学文献训练领域专用语言模型的实际过程缺乏详细文档记录。本研究旨在为计算资源有限的研究者提供训练领域专用模型的工程实践指导。

Method: 构建端到端训练流程：包括元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词，以及在2xA100 GPU上的密集Transformer训练。通过24次实验运行分析训练稳定性、扩展行为、数据损失和基础设施瓶颈。

Result: 研究发现预处理决策显著影响可用token数量，分词策略影响符号稳定性，存储和I/O限制可能与计算能力同等重要。在数据丰富（520亿预训练token）的情况下展现出稳定的训练行为。

Conclusion: 本文提供了训练小型科学语言模型的工程实践记录，而非提出新架构。这些见解有助于计算资源有限的研究者构建领域专用模型，强调了预处理、分词和基础设施优化的重要性。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [13] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无数据的方法来缓解任务向量组合中的表示漂移问题，通过曲率矩阵近似框架实现正则化，在任务添加和否定中达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 任务算术为调整基础模型提供了模块化、可扩展的方式，但组合多个任务向量会导致跨任务干扰，引起表示漂移和性能下降。现有表示漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突

Method: 提出无数据方法，将表示漂移的正则化框架化为曲率矩阵近似问题。采用Kronecker分解近似曲率技术，获得实用的正则化器。该方法具有任务数量上的常数复杂度，并增强了对任务向量缩放的鲁棒性

Result: 在任务添加和否定任务中取得了最先进的结果。方法消除了对保留调优的需求，并促进了对任务向量缩放的鲁棒性

Conclusion: 通过曲率矩阵近似框架提出的无数据正则化方法有效解决了任务向量组合中的表示漂移问题，在保持模块化和数据隐私约束的同时提升了性能

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [14] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 该研究提出了一个针对ARM Cortex处理器（M0+、M4、M7）的AI模型优化基准测试框架，通过自动测试平台系统评估能效、精度和资源利用率，发现浮点运算与推理时间呈近线性关系，并利用帕累托分析平衡能耗与精度权衡。


<details>
  <summary>Details</summary>
Motivation: 在嵌入式系统中优化AI模型时，需要在能效、精度和资源利用之间取得平衡。目前缺乏针对ARM Cortex处理器的系统化基准测试框架，难以指导开发者选择最优的处理器与AI模型组合。

Method: 设计自动化测试平台，系统评估ARM Cortex M0+、M4、M7处理器上AI模型的关键性能指标。通过分析浮点运算与推理时间的相关性，并使用帕累托分析来平衡能耗与模型精度的权衡。

Result: 发现浮点运算与推理时间呈近线性关系，可作为计算需求估算的可靠指标。M7处理器适合短推理周期任务，M4处理器在长推理任务中能效更高，M0+处理器适合简单AI任务。

Conclusion: 该研究为开发者提供了实用的基准测试框架，指导设计高能效AI系统，帮助在嵌入式应用中平衡性能与可持续性要求。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [15] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架结合知识图谱和检索增强生成，提升LLM在电信领域的准确性和可靠性，减少幻觉问题


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域应用困难，因为该领域具有复杂性高、标准不断演进、专业术语多等特点，导致模型输出不准确、幻觉增多、实用性降低

Method: 提出KG-RAG框架，将知识图谱与检索增强生成相结合。知识图谱提供电信标准和文档的结构化领域知识表示，RAG实现动态检索相关事实来支撑模型输出

Result: 在基准数据集上的实验表明，KG-RAG优于纯LLM和标准RAG基线，平均准确率分别比RAG提高14.3%，比纯LLM提高21.6%

Conclusion: KG-RAG能有效在复杂电信场景中生成准确、可靠且可解释的输出，解决了电信领域LLM应用的关键挑战

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [16] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文提出基于常微分方程的统一理论框架ODESteer，用于改进大语言模型激活引导对齐方法，通过屏障函数和多步自适应引导显著提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前激活引导方法存在两个主要问题：缺乏统一的理论框架指导引导方向设计，以及过度依赖单步引导而无法捕捉激活分布的复杂模式。

Method: 提出基于常微分方程的理论框架，将传统激活加法解释为ODE的一阶近似，通过控制理论中的屏障函数定义引导方向，并开发ODESteer方法实现多步自适应引导。

Result: ODESteer在多个LLM对齐基准测试中表现优异，相比最先进的激活引导方法，在TruthfulQA上提升5.7%，UltraFeedback上提升2.5%，RealToxicityPrompts上提升2.4%。

Conclusion: 该工作通过ODE统一了激活引导的理论基础，提出的ODESteer方法在理论和实证上都取得了显著进展，为大语言模型对齐提供了新的原则性视角。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>
