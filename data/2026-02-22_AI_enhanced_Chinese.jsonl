{"id": "2602.16990", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2602.16990", "abs": "https://arxiv.org/abs/2602.16990", "authors": ["Yan Wang", "Yi Han", "Lingfei Qian", "Yueru He", "Xueqing Peng", "Dongji Feng", "Zhuohan Xie", "Vincent Jim Zhang", "Rosie Guo", "Fengran Mo", "Jimin Huang", "Yankai Chen", "Xue Liu", "Jian-Yun Nie"], "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation", "comment": null, "summary": "Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.", "AI": {"tldr": "Conv-FinRe\u662f\u4e00\u4e2a\u7528\u4e8e\u80a1\u7968\u63a8\u8350\u7684\u5bf9\u8bdd\u5f0f\u7eb5\u5411\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u884c\u4e3a\u6a21\u4eff\u8bc4\u4f30\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u53c2\u8003\u533a\u5206\u63cf\u8ff0\u6027\u884c\u4e3a\u548c\u57fa\u4e8e\u6295\u8d44\u8005\u98ce\u9669\u504f\u597d\u7684\u89c4\u8303\u6027\u6548\u7528\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u6a21\u4eff\u7528\u6237\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4f46\u5728\u91d1\u878d\u54a8\u8be2\u9886\u57df\uff0c\u89c2\u5bdf\u5230\u7684\u7528\u6237\u884c\u4e3a\u53ef\u80fd\u56e0\u5e02\u573a\u6ce2\u52a8\u800c\u5b58\u5728\u566a\u58f0\u6216\u77ed\u89c6\uff0c\u53ef\u80fd\u4e0e\u7528\u6237\u7684\u957f\u671f\u76ee\u6807\u76f8\u51b2\u7a81\u3002\u5c06\u7528\u6237\u9009\u62e9\u4f5c\u4e3a\u552f\u4e00\u771f\u5b9e\u6807\u51c6\u4f1a\u6df7\u6dc6\u884c\u4e3a\u6a21\u4eff\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u6784\u5efaConv-FinRe\u57fa\u51c6\uff1a\u57fa\u4e8e\u771f\u5b9e\u5e02\u573a\u6570\u636e\u548c\u4eba\u7c7b\u51b3\u7b56\u8f68\u8ff9\uff0c\u5305\u542b\u5165\u804c\u8bbf\u8c08\u3001\u9010\u6b65\u5e02\u573a\u60c5\u5883\u548c\u54a8\u8be2\u5bf9\u8bdd\u3002\u6a21\u578b\u9700\u8981\u5728\u56fa\u5b9a\u6295\u8d44\u671f\u9650\u5185\u751f\u6210\u80a1\u7968\u6392\u540d\u3002\u57fa\u51c6\u63d0\u4f9b\u591a\u89c6\u89d2\u53c2\u8003\uff0c\u533a\u5206\u63cf\u8ff0\u6027\u884c\u4e3a\u548c\u57fa\u4e8e\u6295\u8d44\u8005\u7279\u5b9a\u98ce\u9669\u504f\u597d\u7684\u89c4\u8303\u6027\u6548\u7528\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u6548\u7528\u7684\u6392\u540d\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u5339\u914d\u7528\u6237\u9009\u62e9\uff0c\u800c\u884c\u4e3a\u5bf9\u9f50\u7684\u6a21\u578b\u53ef\u80fd\u8fc7\u5ea6\u62df\u5408\u77ed\u671f\u566a\u58f0\u3002\u8fd9\u63ed\u793a\u4e86\u7406\u6027\u51b3\u7b56\u8d28\u91cf\u548c\u884c\u4e3a\u5bf9\u9f50\u4e4b\u95f4\u7684\u6301\u7eed\u5f20\u529b\u3002", "conclusion": "Conv-FinRe\u57fa\u51c6\u80fd\u591f\u8bca\u65adLLM\u662f\u57fa\u4e8e\u7406\u6027\u5206\u6790\u3001\u6a21\u4eff\u7528\u6237\u566a\u58f0\u8fd8\u662f\u53d7\u5e02\u573a\u52a8\u91cf\u9a71\u52a8\uff0c\u4e3a\u91d1\u878d\u54a8\u8be2\u9886\u57df\u7684\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.17001", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.17001", "abs": "https://arxiv.org/abs/2602.17001", "authors": ["Zhao Tan", "Yiji Zhao", "Shiyu Wang", "Chang Xu", "Yuxuan Liang", "Xiping Liu", "Shirui Pan", "Ming Jin"], "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases", "comment": null, "summary": "Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.", "AI": {"tldr": "Sonar-TS\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\"\u641c\u7d22-\u9a8c\u8bc1\"\u6d41\u7a0b\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u95ee\u9898\uff0c\u4f7f\u7528SQL\u641c\u7d22\u5019\u9009\u7a97\u53e3\uff0c\u7136\u540e\u7528Python\u7a0b\u5e8f\u9a8c\u8bc1\u539f\u59cb\u4fe1\u53f7\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6NLQTSBench\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fde\u7eed\u5f62\u6001\u610f\u56fe\uff08\u5982\u5f62\u72b6\u6216\u5f02\u5e38\uff09\uff0c\u800c\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u96be\u4ee5\u5904\u7406\u8d85\u957f\u5386\u53f2\u6570\u636e\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u95ee\u9898\u3002", "method": "\u63d0\u51faSonar-TS\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u91c7\u7528\u7c7b\u4f3c\u4e3b\u52a8\u58f0\u7eb3\u7684\"\u641c\u7d22-\u9a8c\u8bc1\"\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u7279\u5f81\u7d22\u5f15\u901a\u8fc7SQL\u641c\u7d22\u5019\u9009\u7a97\u53e3\uff0c\u7136\u540e\u751f\u6210Python\u7a0b\u5e8f\u9501\u5b9a\u5e76\u9a8c\u8bc1\u5019\u9009\u7a97\u53e3\u4e0e\u539f\u59cb\u4fe1\u53f7\u7684\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSonar-TS\u80fd\u6709\u6548\u5904\u7406\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u7684\u590d\u6742\u65f6\u95f4\u67e5\u8be2\uff0c\u5e76\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u72ec\u7279\u6311\u6218\u3002\u540c\u65f6\u521b\u5efa\u4e86NLQTSBench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e3aTSDB\u89c4\u6a21\u5386\u53f2\u8bbe\u8ba1\u7684NLQ\u5927\u89c4\u6a21\u57fa\u51c6\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2602.17015", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17015", "abs": "https://arxiv.org/abs/2602.17015", "authors": ["Saurav Pal"], "title": "Cinder: A fast and fair matchmaking system", "comment": null, "summary": "A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the \"non-outlier\" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a \"Sanction Score.\" We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.", "AI": {"tldr": "Cinder\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5339\u914d\u7cfb\u7edf\uff0c\u901a\u8fc7Ruzicka\u76f8\u4f3c\u5ea6\u6307\u6570\u5feb\u901f\u7b5b\u9009\uff0c\u518d\u4f7f\u7528\u57fa\u4e8e\u53cd\u6b63\u6001\u5206\u5e03\u6280\u80fd\u6876\u7684Kantorovich\u8ddd\u79bb\u8ba1\u7b97\u516c\u5e73\u6027\u5206\u6570\uff0c\u89e3\u51b3\u5f02\u8d28\u6280\u80fd\u961f\u4f0d\u5339\u914d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u591a\u4eba\u5728\u7ebf\u6e38\u620f\u4e2d\uff0c\u516c\u5e73\u5feb\u901f\u7684\u5339\u914d\u7cfb\u7edf\u76f4\u63a5\u5f71\u54cd\u73a9\u5bb6\u7559\u5b58\u548c\u6ee1\u610f\u5ea6\u3002\u7136\u800c\uff0c\u5728\u5f02\u8d28\u6280\u80fd\u6c34\u5e73\u7684\u9884\u7ec4\u961f\u4e4b\u95f4\u521b\u5efa\u516c\u5e73\u5339\u914d\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u5e73\u5747\u6280\u80fd\u6307\u6807\u7684\u5339\u914d\u65b9\u6cd5\u5728\u6280\u80fd\u5206\u5e03\u5e7f\u6cdb\u6216\u504f\u659c\u65f6\u7ecf\u5e38\u5bfc\u81f4\u4e0d\u5e73\u8861\u7684\u4e00\u8fb9\u5012\u6bd4\u8d5b\u3002", "method": "Cinder\u91c7\u7528\u4e24\u9636\u6bb5\u5339\u914d\u7cfb\u7edf\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Ruzicka\u76f8\u4f3c\u5ea6\u6307\u6570\u5feb\u901f\u6bd4\u8f83\u961f\u4f0d\u7684\"\u975e\u5f02\u5e38\u503c\"\u6280\u80fd\u8303\u56f4\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u73a9\u5bb6\u6392\u540d\u6620\u5c04\u5230\u57fa\u4e8e\u53cd\u6b63\u6001\u5206\u5e03\u751f\u6210\u7684\u6280\u80fd\u6876\u4e2d\uff0c\u63d0\u4f9b\u5e73\u5747\u6280\u80fd\u6c34\u5e73\u4e0b\u7684\u66f4\u9ad8\u7c92\u5ea6\uff0c\u7136\u540e\u4f7f\u7528Kantorovich\u8ddd\u79bb\u5728\u961f\u4f0d\u6392\u5e8f\u540e\u7684\u6876\u7d22\u5f15\u4e0a\u8ba1\u7b97\"\u5236\u88c1\u5206\u6570\"\u6765\u91cf\u5316\u5339\u914d\u516c\u5e73\u6027\u3002", "result": "\u901a\u8fc7\u5206\u67901.4\u4ebf\u4e2a\u6a21\u62df\u961f\u4f0d\u914d\u5bf9\u7684\u5236\u88c1\u5206\u6570\u5206\u5e03\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u516c\u5e73\u5339\u914d\u9608\u503c\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u57fa\u7840\u3002", "conclusion": "Cinder\u7cfb\u7edf\u80fd\u591f\u4e3a\u5f02\u8d28\u6280\u80fd\u6c34\u5e73\u7684\u9884\u7ec4\u961f\u63d0\u4f9b\u5feb\u901f\u4e14\u516c\u5e73\u7684\u5339\u914d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5e73\u5747\u6280\u80fd\u5339\u914d\u65b9\u6cd5\u5728\u6280\u80fd\u5206\u5e03\u5e7f\u6cdb\u6216\u504f\u659c\u65f6\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2602.17016", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17016", "abs": "https://arxiv.org/abs/2602.17016", "authors": ["Zichen Wang", "Wanli Ma", "Zhenyu Ming", "Gong Zhang", "Kun Yuan", "Zaiwen Wen"], "title": "M2F: Automated Formalization of Mathematical Literature at Scale", "comment": null, "summary": "Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\\%$ proof success (vs.\\ $80\\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.", "AI": {"tldr": "M2F\u662f\u9996\u4e2a\u9762\u5411\u9879\u76ee\u89c4\u6a21\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u957f\u7bc7\u6570\u5b66\u8d44\u6599\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684Lean\u5e93\uff0c\u5b9e\u73b0\u4e86\u6559\u79d1\u4e66\u7ea7\u522b\u7684\u5f62\u5f0f\u5316\uff0c\u6548\u7387\u8fdc\u8d85\u4eba\u5de5\u4e13\u5bb6\u3002", "motivation": "\u5f53\u524d\u6570\u5b66\u81ea\u52a8\u5f62\u5f0f\u5316\u4e3b\u8981\u5c40\u9650\u4e8e\u5b64\u7acb\u5b9a\u7406\u548c\u7b80\u77ed\u4ee3\u7801\u7247\u6bb5\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u6559\u79d1\u4e66\u548c\u7814\u7a76\u8bba\u6587\u7ea7\u522b\u3002\u9879\u76ee\u89c4\u6a21\u7684\u5f62\u5f0f\u5316\u9700\u8981\u5904\u7406\u8de8\u6587\u4ef6\u4f9d\u8d56\u3001\u5bfc\u5165\u89e3\u6790\u548c\u7aef\u5230\u7aef\u7f16\u8bd1\u9a8c\u8bc1\u7b49\u6311\u6218\u3002", "method": "M2F\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8bed\u53e5\u7f16\u8bd1\u9636\u6bb5\u5c06\u6587\u6863\u5206\u5272\u4e3a\u539f\u5b50\u5757\uff0c\u901a\u8fc7\u63a8\u65ad\u4f9d\u8d56\u5173\u7cfb\u6392\u5e8f\uff0c\u4fee\u590d\u58f0\u660e\u9aa8\u67b6\u76f4\u5230\u9879\u76ee\u7f16\u8bd1\u901a\u8fc7\uff1b2) \u8bc1\u660e\u4fee\u590d\u9636\u6bb5\u5728\u56fa\u5b9a\u7b7e\u540d\u4e0b\u4f7f\u7528\u76ee\u6807\u5bfc\u5411\u7684\u5c40\u90e8\u7f16\u8f91\u586b\u8865\u8bc1\u660e\u7a7a\u7f3a\u3002\u6574\u4e2a\u8fc7\u7a0b\u4fdd\u6301\u9a8c\u8bc1\u5668\u5728\u5faa\u73af\u4e2d\uff0c\u53ea\u6709\u5de5\u5177\u94fe\u53cd\u9988\u786e\u8ba4\u6539\u8fdb\u540e\u624d\u63d0\u4ea4\u7f16\u8f91\u3002", "result": "\u5728\u7ea6\u4e09\u5468\u65f6\u95f4\u5185\uff0cM2F\u5c06479\u9875\u7684\u5b9e\u5206\u6790\u548c\u51f8\u5206\u6790\u6559\u79d1\u4e66\u8f6c\u6362\u4e3a\u5305\u542b153,853\u884c\u4ee3\u7801\u7684Lean\u5e93\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u5f62\u5f0f\u5316\u3002\u5728FATE-H\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523096%\u7684\u8bc1\u660e\u6210\u529f\u7387\uff08\u57fa\u7ebf\u4e3a80%\uff09\uff0c\u5c55\u793a\u4e86\u6559\u79d1\u4e66\u89c4\u6a21\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u53ef\u884c\u6027\u3002", "conclusion": "M2F\u6846\u67b6\u8bc1\u660e\u5927\u89c4\u6a21\u6570\u5b66\u6587\u732e\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u662f\u53ef\u884c\u7684\uff0c\u80fd\u591f\u4ee5\u8fdc\u8d85\u4eba\u5de5\u4e13\u5bb6\u7684\u901f\u5ea6\u5b9e\u73b0\u6559\u79d1\u4e66\u7ea7\u522b\u7684\u5f62\u5f0f\u5316\uff0c\u4e3a\u6570\u5b66\u5f62\u5f0f\u5316\u7684\u5927\u89c4\u6a21\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.17062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17062", "abs": "https://arxiv.org/abs/2602.17062", "authors": ["Yonghyeon Jo", "Sunwoo Lee", "Seungyul Han"], "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning", "comment": "10 technical page followed by references and appendix. Accepted to ICLR 2026", "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.", "AI": {"tldr": "S2Q\u662f\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u591a\u4e2a\u5b50\u4ef7\u503c\u51fd\u6570\u6765\u4fdd\u7559\u66ff\u4ee3\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\uff0c\u4f7f\u7528\u57fa\u4e8eSoftmax\u7684\u884c\u4e3a\u7b56\u7565\u4fc3\u8fdb\u6301\u7eed\u63a2\u7d22\uff0c\u5728\u4ef7\u503c\u51fd\u6570\u53d8\u5316\u65f6\u80fd\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u4ef7\u503c\u5206\u89e3\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\uff0c\u5f53\u5e95\u5c42\u4ef7\u503c\u51fd\u6570\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u751f\u53d8\u5316\u65f6\u96be\u4ee5\u9002\u5e94\uff0c\u5e38\u5e38\u6536\u655b\u5230\u6b21\u4f18\u7b56\u7565\u3002", "method": "\u63d0\u51faSuccessive Sub-value Q-learning (S2Q)\uff0c\u5b66\u4e60\u591a\u4e2a\u5b50\u4ef7\u503c\u51fd\u6570\u6765\u4fdd\u7559\u66ff\u4ee3\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\uff0c\u5c06\u8fd9\u4e9b\u5b50\u4ef7\u503c\u51fd\u6570\u6574\u5408\u5230\u57fa\u4e8eSoftmax\u7684\u884c\u4e3a\u7b56\u7565\u4e2d\uff0c\u4fc3\u8fdb\u6301\u7eed\u63a2\u7d22\u5e76\u4f7f\u603bQ\u503c\u80fd\u5feb\u901f\u9002\u5e94\u53d8\u5316\u7684\u6700\u4f18\u89e3\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cS2Q\u6301\u7eed\u4f18\u4e8e\u5404\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "S2Q\u901a\u8fc7\u4fdd\u7559\u66ff\u4ee3\u9ad8\u4ef7\u503c\u52a8\u4f5c\u548c\u4fc3\u8fdb\u6301\u7eed\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4ef7\u503c\u5206\u89e3\u65b9\u6cd5\u5728\u4ef7\u503c\u51fd\u6570\u53d8\u5316\u65f6\u96be\u4ee5\u9002\u5e94\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2602.17066", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17066", "abs": "https://arxiv.org/abs/2602.17066", "authors": ["Sumedh Rasal"], "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization", "comment": null, "summary": "We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.", "AI": {"tldr": "PBS\u662f\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u4f18\u5148\u5904\u7406\u9ad8\u635f\u5931\u6837\u672c\u6765\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u6536\u655b\u7684\u8bad\u7ec3\u4f18\u5316\u6280\u672f\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u9884\u6d4b\u5668\u4ece\u9759\u6001\u6807\u8bb0\u7ea7\u7279\u5f81\u4f30\u8ba1\u6837\u672c\u96be\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u65e0\u9700\u9884\u5b9a\u4e49\u96be\u5ea6\u6307\u6807\u6216\u6602\u8d35\u7684\u9010\u6837\u672c\u635f\u5931\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u9884\u5b9a\u4e49\u96be\u5ea6\u6307\u6807\uff0c\u800c\u786c\u6837\u672c\u6316\u6398\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u9010\u6837\u672c\u635f\u5931\u8ddf\u8e2a\u3002PBS\u65e8\u5728\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u65b9\u5f0f\u8bc6\u522b\u56f0\u96be\u6837\u672c\u6765\u52a0\u901f\u6a21\u578b\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "method": "PBS\u4f7f\u7528\u5728\u7ebf\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u7ebf\u6027\u9884\u6d4b\u5668\uff0c\u4ec5\u57fa\u4e8e\u56db\u4e2a\u7b80\u5355\u7684\u6807\u8bb0\u7ea7\u7279\u5f81\uff08\u6807\u8bb0\u9891\u7387\u3001\u5e8f\u5217\u957f\u5ea6\u3001\u8bcd\u6c47\u591a\u6837\u6027\u548c\u7a00\u6709\u6807\u8bb0\u6bd4\u4f8b\uff09\u6765\u4f30\u8ba1\u6837\u672c\u96be\u5ea6\u3002\u9884\u6d4b\u5668\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u66f4\u65b0\uff0c\u7528\u4e8e\u4f18\u5148\u9009\u62e9\u9ad8\u635f\u5931\u6837\u672c\u6784\u5efa\u6279\u6b21\u3002", "result": "\u5728130M\u53c2\u6570transformer\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPBS\u5b9e\u73b0\u4e866-13%\u7684\u6536\u655b\u52a0\u901f\uff08\u901a\u8fc7\u8bad\u7ec3\u68c0\u67e5\u70b9\u7684\u8bc4\u4f30\u635f\u5931\u8861\u91cf\uff09\u3002\u9884\u6d4b\u5668\u4e0e\u771f\u5b9e\u635f\u5931\u7684\u76f8\u5173\u7cfb\u6570\u4ece0.14\u63d0\u9ad8\u52300.44\uff08\u7ecf\u8fc710,000\u8bad\u7ec3\u6b65\uff09\uff0c\u8bc1\u660e\u6807\u8bb0\u9891\u7387\u7edf\u8ba1\u91cf\u7f16\u7801\u4e86\u6709\u5173\u6837\u672c\u96be\u5ea6\u7684\u6709\u610f\u4e49\u4fe1\u606f\u3002", "conclusion": "PBS\u9a8c\u8bc1\u4e86\u4ece\u7b80\u5355\u6807\u8bb0\u7edf\u8ba1\u91cf\u53ef\u4ee5\u6709\u6548\u9884\u6d4b\u6837\u672c\u96be\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bfe\u7a0b\u5b66\u4e60\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u8be5\u65b9\u6cd5\u4e3a\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17111", "abs": "https://arxiv.org/abs/2602.17111", "authors": ["Abdulrahman AlRabah", "Priyanka Kargupta", "Jiawei Han", "Abdussalam Alawini"], "title": "Instructor-Aligned Knowledge Graphs for Personalized Learning", "comment": null, "summary": "Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like \"Algorithms\" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., \"part-of\" or \"depends-on\" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., \"recursion\" is taught before \"mergesort\"; \"recursion\" is mentioned in the definition of \"merge sort\") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.", "AI": {"tldr": "InstructKG\uff1a\u57fa\u4e8e\u8bfe\u7a0b\u6559\u5b66\u6750\u6599\u81ea\u52a8\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u5b66\u4e60\u4f9d\u8d56\u5173\u7cfb\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60", "motivation": "\u5927\u89c4\u6a21\u8bfe\u7a0b\u4e2d\uff0c\u6559\u5e08\u96be\u4ee5\u8bca\u65ad\u6bcf\u4e2a\u5b66\u751f\u7684\u77e5\u8bc6\u7f3a\u53e3\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u5e72\u9884\u3002\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u8981\u4e48\u505c\u7559\u5728\u8bfe\u7a0b\u5c42\u9762\u6982\u5ff5\uff0c\u8981\u4e48\u5ffd\u7565\u4e86\u6559\u5b66\u6750\u6599\u4e2d\u4e30\u5bcc\u7684\u6559\u5b66\u4fe1\u53f7\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u6982\u5ff5\u95f4\u7684\u5b66\u4e60\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faInstructKG\u6846\u67b6\uff0c\u4ece\u8bfe\u7a0b\u6559\u5b66\u6750\u6599\uff08\u5e7b\u706f\u7247\u3001\u7b14\u8bb0\u7b49\uff09\u4e2d\u63d0\u53d6\u91cd\u8981\u6982\u5ff5\u4f5c\u4e3a\u8282\u70b9\uff0c\u63a8\u65ad\u5b66\u4e60\u4f9d\u8d56\u5173\u7cfb\u4f5c\u4e3a\u6709\u5411\u8fb9\uff08\u5982\"\u90e8\u5206-\u6574\u4f53\"\u6216\"\u4f9d\u8d56\"\u5173\u7cfb\uff09\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6559\u5b66\u6750\u6599\u7279\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u548c\u8bed\u4e49\u4fe1\u53f7\uff08\u5982\"\u9012\u5f52\"\u5728\"\u5f52\u5e76\u6392\u5e8f\"\u4e4b\u524d\u6559\u6388\uff0c\"\u9012\u5f52\"\u5728\"\u5f52\u5e76\u6392\u5e8f\"\u5b9a\u4e49\u4e2d\u88ab\u63d0\u53ca\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u8bfe\u7a0b\u7684\u771f\u5b9e\u4e16\u754c\u591a\u6837\u5316\u6559\u5b66\u6750\u6599\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u548c\u57fa\u4e8e\u4eba\u5de5\u7684\u8bc4\u4f30\uff0c\u8bc1\u660eInstructKG\u80fd\u591f\u6355\u6349\u4e30\u5bcc\u3001\u4e0e\u6559\u5e08\u610f\u56fe\u4e00\u81f4\u7684\u5b66\u4e60\u8fdb\u5c55\u3002", "conclusion": "InstructKG\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u4e0e\u6559\u5e08\u6559\u5b66\u610f\u56fe\u4e00\u81f4\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u6709\u6548\u6355\u6349\u8bfe\u7a0b\u4e2d\u7684\u5b66\u4e60\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u5927\u89c4\u6a21\u4e2a\u6027\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.17162", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.17162", "abs": "https://arxiv.org/abs/2602.17162", "authors": ["Ariel Larey", "Elay Dahan", "Amit Bleiweiss", "Raizy Kellerman", "Guy Leib", "Omri Nayshool", "Dan Ofer", "Tal Zinger", "Dan Dominissini", "Gideon Rechavi", "Nicole Bussola", "Simon Lee", "Shane O'Connell", "Dung Hoang", "Marissa Wirth", "Alexander W. Charney", "Nati Daniel", "Yoli Shavit"], "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures", "comment": null, "summary": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.", "AI": {"tldr": "JEPA-DNA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u548c\u4f20\u7edf\u751f\u6210\u76ee\u6807\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9884\u6d4b\u63a9\u7801\u57fa\u56e0\u7ec4\u7247\u6bb5\u7684\u9ad8\u7ea7\u529f\u80fd\u5d4c\u5165\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u5177\u5168\u5c40\u751f\u7269\u89c6\u89d2\u7684\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u6216\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u64c5\u957f\u6355\u6349\u5c40\u90e8\u57fa\u56e0\u7ec4\u8bed\u6cd5\u548c\u7ec6\u7c92\u5ea6\u57fa\u5e8f\u6a21\u5f0f\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u6355\u83b7\u66f4\u5e7f\u6cdb\u7684\u529f\u80fd\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u8868\u793a\u7f3a\u4e4f\u5168\u5c40\u751f\u7269\u5b66\u89c6\u89d2\u3002", "method": "\u5f15\u5165JEPA-DNA\u6846\u67b6\uff0c\u6574\u5408\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u4e0e\u4f20\u7edf\u7684\u751f\u6210\u76ee\u6807\u3002\u901a\u8fc7\u5c06\u6807\u8bb0\u7ea7\u6062\u590d\u4e0e\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9884\u6d4b\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u76d1\u7763CLS\u6807\u8bb0\uff0c\u8feb\u4f7f\u6a21\u578b\u9884\u6d4b\u63a9\u7801\u57fa\u56e0\u7ec4\u7247\u6bb5\u7684\u9ad8\u7ea7\u529f\u80fd\u5d4c\u5165\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5173\u6ce8\u5355\u4e2a\u6838\u82f7\u9178\u3002\u8be5\u6846\u67b6\u53ef\u6269\u5c55NTP\u548cMLM\u8303\u5f0f\uff0c\u53ef\u4f5c\u4e3a\u72ec\u7acb\u4ece\u5934\u8bad\u7ec3\u76ee\u6807\u6216\u73b0\u6709GFM\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u589e\u5f3a\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u57fa\u56e0\u7ec4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJEPA-DNA\u5728\u76d1\u7763\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u751f\u6210\u5f0f\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "JEPA-DNA\u901a\u8fc7\u63d0\u4f9b\u66f4\u7a33\u5065\u548c\u751f\u7269\u5b66\u57fa\u7840\u7684\u8868\u793a\uff0c\u4e3a\u7406\u89e3\u57fa\u56e0\u7ec4\u5b57\u6bcd\u548c\u5e8f\u5217\u5e95\u5c42\u529f\u80fd\u903b\u8f91\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2602.17189", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17189", "abs": "https://arxiv.org/abs/2602.17189", "authors": ["Sicheng Mao"], "title": "Texo: Formula Recognition within 20M Parameters", "comment": null, "summary": "In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.", "AI": {"tldr": "Texo\u662f\u4e00\u4e2a\u4ec5\u542b2000\u4e07\u53c2\u6570\u7684\u6781\u7b80\u9ad8\u6027\u80fd\u516c\u5f0f\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u8bcd\u6c47\u8868/\u5206\u8bcd\u5668\u8fc1\u79fb\uff0c\u6027\u80fd\u5ab2\u7f8eSOTA\u6a21\u578b\uff0c\u540c\u65f6\u6a21\u578b\u5927\u5c0f\u51cf\u5c1180%\u548c65%\uff0c\u652f\u6301\u6d88\u8d39\u7ea7\u786c\u4ef6\u5b9e\u65f6\u63a8\u7406\u548c\u6d4f\u89c8\u5668\u90e8\u7f72\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u9ad8\u6027\u80fd\u7684\u516c\u5f0f\u8bc6\u522b\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u751a\u81f3\u652f\u6301\u6d4f\u89c8\u5668\u90e8\u7f72\uff0c\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u578b\u67b6\u6784\u3001\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u4ee5\u53ca\u8bcd\u6c47\u8868\u548c\u5206\u8bcd\u5668\u7684\u8fc1\u79fb\u7b56\u7565\uff0c\u5c06\u6a21\u578b\u53c2\u6570\u538b\u7f29\u5230\u4ec52000\u4e07\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "result": "Texo\u5728\u6027\u80fd\u4e0a\u53ef\u4e0eUniMERNet-T\u548cPPFormulaNet-S\u7b49SOTA\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u6a21\u578b\u5927\u5c0f\u5206\u522b\u51cf\u5c11\u4e8680%\u548c65%\uff0c\u5b9e\u73b0\u4e86\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u5b9e\u65f6\u63a8\u7406\u548c\u6d4f\u89c8\u5668\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "Texo\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u53ef\u4ee5\u521b\u5efa\u6781\u7b80\u4f46\u9ad8\u6027\u80fd\u7684\u516c\u5f0f\u8bc6\u522b\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u548c\u6d4f\u89c8\u5668\u90e8\u7f72\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.17222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17222", "abs": "https://arxiv.org/abs/2602.17222", "authors": ["Ben Yellin", "Ehud Ezra", "Mark Foreman", "Shula Grinapol"], "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight", "comment": null, "summary": "Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5927\u578b\u884c\u4e3a\u6a21\u578b\uff08LBM\uff09\uff0c\u901a\u8fc7\u5c06\u77ac\u6001\u89d2\u8272\u63d0\u793a\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u7ed3\u6784\u5316\u9ad8\u7ef4\u7279\u8d28\u6863\u6848\u7684\u884c\u4e3a\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u4f53\u6218\u7565\u51b3\u7b56\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u9884\u6d4b\u4eba\u7c7b\u51b3\u7b56\u662f\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u6311\u6218\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u751f\u6210\u4e00\u81f4\u3001\u4e2a\u4f53\u7279\u5b9a\u7684\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5f53\u51c6\u786e\u9884\u6d4b\u4f9d\u8d56\u4e8e\u5fc3\u7406\u7279\u8d28\u4e0e\u60c5\u5883\u7ea6\u675f\u7684\u590d\u6742\u4ea4\u4e92\u65f6\u3002\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u8106\u5f31\uff0c\u5bb9\u6613\u51fa\u73b0\u8eab\u4efd\u6f02\u79fb\u4e14\u96be\u4ee5\u5229\u7528\u8be6\u7ec6\u89d2\u8272\u63cf\u8ff0\u3002", "method": "\u5f15\u5165\u5927\u578b\u884c\u4e3a\u6a21\u578b\uff08LBM\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u884c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u4ece\u7efc\u5408\u5fc3\u7406\u6d4b\u91cf\u5de5\u5177\u5305\u4e2d\u63d0\u53d6\u7684\u7ed3\u6784\u5316\u9ad8\u7ef4\u7279\u8d28\u6863\u6848\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4ece\u77ac\u6001\u89d2\u8272\u63d0\u793a\u8f6c\u53d8\u4e3a\u884c\u4e3a\u5d4c\u5165\u3002\u6a21\u578b\u5728\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u8be5\u6570\u636e\u96c6\u5c06\u7a33\u5b9a\u503e\u5411\u3001\u52a8\u673a\u72b6\u6001\u548c\u60c5\u5883\u7ea6\u675f\u4e0e\u89c2\u5bdf\u5230\u7684\u9009\u62e9\u76f8\u5173\u8054\uff0c\u5b66\u4e60\u5c06\u4e30\u5bcc\u7684\u5fc3\u7406\u6863\u6848\u6620\u5c04\u5230\u4e0d\u540c\u6218\u7565\u56f0\u5883\u4e2d\u7684\u79bb\u6563\u884c\u52a8\u3002", "result": "\u5728\u4fdd\u7559\u573a\u666f\u8bc4\u4f30\u4e2d\uff0cLBM\u5fae\u8c03\u76f8\u5bf9\u4e8e\u672a\u9002\u5e94\u7684Llama-3.1-8B-Instruct\u9aa8\u5e72\u6a21\u578b\u63d0\u9ad8\u4e86\u884c\u4e3a\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5f53\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u7279\u8d28\u8fdb\u884c\u6761\u4ef6\u5316\u65f6\uff0c\u6027\u80fd\u4e0e\u524d\u6cbf\u57fa\u7ebf\u76f8\u5f53\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u5b58\u5728\u590d\u6742\u6027\u4e0a\u9650\uff0c\u800cLBM\u7ee7\u7eed\u53d7\u76ca\u4e8e\u65e5\u76ca\u5bc6\u96c6\u7684\u7279\u8d28\u6863\u6848\uff0c\u968f\u7740\u63d0\u4f9b\u66f4\u591a\u7279\u8d28\u7ef4\u5ea6\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "LBM\u4e3a\u9ad8\u4fdd\u771f\u884c\u4e3a\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5728\u6218\u7565\u9884\u89c1\u3001\u8c08\u5224\u5206\u6790\u3001\u8ba4\u77e5\u5b89\u5168\u548c\u51b3\u7b56\u652f\u6301\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.17234", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17234", "abs": "https://arxiv.org/abs/2602.17234", "authors": ["Zeyu Zhang", "Ryan Chen", "Bradly C. Stadie"], "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting", "comment": "8 pages plus appendix", "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \\textbf{Time}-\\textbf{S}upervised \\textbf{P}rediction with \\textbf{E}xtracted \\textbf{C}laims (\\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.", "AI": {"tldr": "\u63d0\u51faShapley-DCLR\u6307\u6807\u91cf\u5316LLMs\u5728\u56de\u6eaf\u6d4b\u8bd5\u4e2d\u7684\u65f6\u95f4\u77e5\u8bc6\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u5f00\u53d1TimeSPEC\u65b9\u6cd5\u901a\u8fc7\u58f0\u660e\u9a8c\u8bc1\u51cf\u5c11\u6cc4\u6f0f", "motivation": "\u8bc4\u4f30LLMs\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u80fd\u529b\u9700\u8981\u8fdb\u884c\u56de\u6eaf\u6d4b\u8bd5\uff0c\u4f46\u6a21\u578b\u53ef\u80fd\u5728\u8bad\u7ec3\u4e2d\u7f16\u7801\u4e86\u622a\u6b62\u65e5\u671f\u540e\u7684\u77e5\u8bc6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u65e0\u6548", "method": "1) \u63d0\u51fa\u58f0\u660e\u7ea7\u6846\u67b6\u68c0\u6d4b\u65f6\u95f4\u77e5\u8bc6\u6cc4\u6f0f\uff1a\u5c06\u6a21\u578b\u63a8\u7406\u5206\u89e3\u4e3a\u539f\u5b50\u58f0\u660e\uff0c\u6309\u65f6\u95f4\u53ef\u9a8c\u8bc1\u6027\u5206\u7c7b\uff0c\u4f7f\u7528Shapley\u503c\u8861\u91cf\u6bcf\u4e2a\u58f0\u660e\u5bf9\u9884\u6d4b\u7684\u8d21\u732e\uff1b2) \u5f00\u53d1TimeSPEC\u65b9\u6cd5\uff1a\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4ea4\u66ff\u8fdb\u884c\u58f0\u660e\u9a8c\u8bc1\u548c\u91cd\u65b0\u751f\u6210\uff0c\u4e3b\u52a8\u8fc7\u6ee4\u65f6\u95f4\u6c61\u67d3", "result": "\u5728350\u4e2a\u5b9e\u4f8b\uff08\u7f8e\u56fd\u6700\u9ad8\u6cd5\u9662\u6848\u4ef6\u9884\u6d4b\u3001NBA\u85aa\u8d44\u4f30\u8ba1\u3001\u80a1\u7968\u56de\u62a5\u6392\u540d\uff09\u4e0a\u5b9e\u9a8c\u663e\u793a\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u6cc4\u6f0f\uff0cTimeSPEC\u80fd\u964d\u4f4eShapley-DCLR\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd", "conclusion": "\u663e\u5f0f\u7684\u3001\u53ef\u89e3\u91ca\u7684\u58f0\u660e\u7ea7\u9a8c\u8bc1\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u65f6\u95f4\u7ea6\u675f\uff0c\u80fd\u5b9e\u73b0\u66f4\u53ef\u9760\u7684LLMs\u56de\u6eaf\u6d4b\u8bd5"}}
{"id": "2602.17288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17288", "abs": "https://arxiv.org/abs/2602.17288", "authors": ["Anuj Gupta"], "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training", "comment": "15 pages, 6 figures, 1 table", "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u8bb0\u5f55\u4e86\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\uff082xA100 GPU\uff09\u4e0b\uff0c\u4ecearXiv LaTeX\u6e90\u6587\u4ef6\u8bad\u7ec31.36B\u53c2\u6570\u79d1\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u5b8c\u6574\u5de5\u7a0b\u5b9e\u8df5\uff0c\u5206\u6790\u4e86\u9884\u5904\u7406\u3001\u5206\u8bcd\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u7b49\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u867d\u7136\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u548c\u6570\u5b66\u80fd\u529b\uff0c\u4f46\u4ece\u539f\u59cb\u79d1\u5b66\u6587\u732e\u8bad\u7ec3\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u8fc7\u7a0b\u7f3a\u4e4f\u8be6\u7ec6\u6587\u6863\u8bb0\u5f55\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u8bad\u7ec3\u9886\u57df\u4e13\u7528\u6a21\u578b\u7684\u5de5\u7a0b\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5305\u62ec\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u5b58\u6863\u9a8c\u8bc1\u3001LaTeX\u63d0\u53d6\u3001\u6587\u672c\u89c4\u8303\u5316\u3001\u9886\u57df\u611f\u77e5\u5206\u8bcd\uff0c\u4ee5\u53ca\u57282xA100 GPU\u4e0a\u7684\u5bc6\u96c6Transformer\u8bad\u7ec3\u3002\u901a\u8fc724\u6b21\u5b9e\u9a8c\u8fd0\u884c\u5206\u6790\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u6269\u5c55\u884c\u4e3a\u3001\u6570\u636e\u635f\u5931\u548c\u57fa\u7840\u8bbe\u65bd\u74f6\u9888\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9884\u5904\u7406\u51b3\u7b56\u663e\u8457\u5f71\u54cd\u53ef\u7528token\u6570\u91cf\uff0c\u5206\u8bcd\u7b56\u7565\u5f71\u54cd\u7b26\u53f7\u7a33\u5b9a\u6027\uff0c\u5b58\u50a8\u548cI/O\u9650\u5236\u53ef\u80fd\u4e0e\u8ba1\u7b97\u80fd\u529b\u540c\u7b49\u91cd\u8981\u3002\u5728\u6570\u636e\u4e30\u5bcc\uff08520\u4ebf\u9884\u8bad\u7ec3token\uff09\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u7a33\u5b9a\u7684\u8bad\u7ec3\u884c\u4e3a\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u8bad\u7ec3\u5c0f\u578b\u79d1\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u7a0b\u5b9e\u8df5\u8bb0\u5f55\uff0c\u800c\u975e\u63d0\u51fa\u65b0\u67b6\u6784\u3002\u8fd9\u4e9b\u89c1\u89e3\u6709\u52a9\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u8005\u6784\u5efa\u9886\u57df\u4e13\u7528\u6a21\u578b\uff0c\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u3001\u5206\u8bcd\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.17385", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17385", "abs": "https://arxiv.org/abs/2602.17385", "authors": ["Angelo Porrello", "Pietro Buzzega", "Felix Dangel", "Thomas Sommariva", "Riccardo Salami", "Lorenzo Bonicelli", "Simone Calderara"], "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature", "comment": "Accepted to ICLR 2026", "summary": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u6570\u636e\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u4e2d\u7684\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u66f2\u7387\u77e9\u9635\u8fd1\u4f3c\u6846\u67b6\u5b9e\u73b0\u6b63\u5219\u5316\uff0c\u5728\u4efb\u52a1\u6dfb\u52a0\u548c\u5426\u5b9a\u4e2d\u8fbe\u5230SOTA\u6548\u679c", "motivation": "\u4efb\u52a1\u7b97\u672f\u4e3a\u8c03\u6574\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\uff0c\u4f46\u7ec4\u5408\u591a\u4e2a\u4efb\u52a1\u5411\u91cf\u4f1a\u5bfc\u81f4\u8de8\u4efb\u52a1\u5e72\u6270\uff0c\u5f15\u8d77\u8868\u793a\u6f02\u79fb\u548c\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u8868\u793a\u6f02\u79fb\u6b63\u5219\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5916\u90e8\u4efb\u52a1\u6570\u636e\uff0c\u8fd9\u4e0e\u6a21\u5757\u5316\u548c\u6570\u636e\u53ef\u7528\u6027\u7ea6\u675f\uff08\u5982\u9690\u79c1\u8981\u6c42\uff09\u76f8\u51b2\u7a81", "method": "\u63d0\u51fa\u65e0\u6570\u636e\u65b9\u6cd5\uff0c\u5c06\u8868\u793a\u6f02\u79fb\u7684\u6b63\u5219\u5316\u6846\u67b6\u5316\u4e3a\u66f2\u7387\u77e9\u9635\u8fd1\u4f3c\u95ee\u9898\u3002\u91c7\u7528Kronecker\u5206\u89e3\u8fd1\u4f3c\u66f2\u7387\u6280\u672f\uff0c\u83b7\u5f97\u5b9e\u7528\u7684\u6b63\u5219\u5316\u5668\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u4efb\u52a1\u6570\u91cf\u4e0a\u7684\u5e38\u6570\u590d\u6742\u5ea6\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u4efb\u52a1\u5411\u91cf\u7f29\u653e\u7684\u9c81\u68d2\u6027", "result": "\u5728\u4efb\u52a1\u6dfb\u52a0\u548c\u5426\u5b9a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u4fdd\u7559\u8c03\u4f18\u7684\u9700\u6c42\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u4efb\u52a1\u5411\u91cf\u7f29\u653e\u7684\u9c81\u68d2\u6027", "conclusion": "\u901a\u8fc7\u66f2\u7387\u77e9\u9635\u8fd1\u4f3c\u6846\u67b6\u63d0\u51fa\u7684\u65e0\u6570\u636e\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u4e2d\u7684\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u5757\u5316\u548c\u6570\u636e\u9690\u79c1\u7ea6\u675f\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd"}}
{"id": "2602.17508", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17508", "abs": "https://arxiv.org/abs/2602.17508", "authors": ["Pranay Jain", "Maximilian Kasper", "G\u00f6ran K\u00f6ber", "Axel Plinge", "Dominik Seu\u00df"], "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9ARM Cortex\u5904\u7406\u5668\uff08M0+\u3001M4\u3001M7\uff09\u7684AI\u6a21\u578b\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u6d4b\u8bd5\u5e73\u53f0\u7cfb\u7edf\u8bc4\u4f30\u80fd\u6548\u3001\u7cbe\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u53d1\u73b0\u6d6e\u70b9\u8fd0\u7b97\u4e0e\u63a8\u7406\u65f6\u95f4\u5448\u8fd1\u7ebf\u6027\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u5e15\u7d2f\u6258\u5206\u6790\u5e73\u8861\u80fd\u8017\u4e0e\u7cbe\u5ea6\u6743\u8861\u3002", "motivation": "\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u4f18\u5316AI\u6a21\u578b\u65f6\uff0c\u9700\u8981\u5728\u80fd\u6548\u3001\u7cbe\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9ARM Cortex\u5904\u7406\u5668\u7684\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u96be\u4ee5\u6307\u5bfc\u5f00\u53d1\u8005\u9009\u62e9\u6700\u4f18\u7684\u5904\u7406\u5668\u4e0eAI\u6a21\u578b\u7ec4\u5408\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7cfb\u7edf\u8bc4\u4f30ARM Cortex M0+\u3001M4\u3001M7\u5904\u7406\u5668\u4e0aAI\u6a21\u578b\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\u3002\u901a\u8fc7\u5206\u6790\u6d6e\u70b9\u8fd0\u7b97\u4e0e\u63a8\u7406\u65f6\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u4f7f\u7528\u5e15\u7d2f\u6258\u5206\u6790\u6765\u5e73\u8861\u80fd\u8017\u4e0e\u6a21\u578b\u7cbe\u5ea6\u7684\u6743\u8861\u3002", "result": "\u53d1\u73b0\u6d6e\u70b9\u8fd0\u7b97\u4e0e\u63a8\u7406\u65f6\u95f4\u5448\u8fd1\u7ebf\u6027\u5173\u7cfb\uff0c\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u9700\u6c42\u4f30\u7b97\u7684\u53ef\u9760\u6307\u6807\u3002M7\u5904\u7406\u5668\u9002\u5408\u77ed\u63a8\u7406\u5468\u671f\u4efb\u52a1\uff0cM4\u5904\u7406\u5668\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e2d\u80fd\u6548\u66f4\u9ad8\uff0cM0+\u5904\u7406\u5668\u9002\u5408\u7b80\u5355AI\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6307\u5bfc\u8bbe\u8ba1\u9ad8\u80fd\u6548AI\u7cfb\u7edf\uff0c\u5e2e\u52a9\u5728\u5d4c\u5165\u5f0f\u5e94\u7528\u4e2d\u5e73\u8861\u6027\u80fd\u4e0e\u53ef\u6301\u7eed\u6027\u8981\u6c42\u3002"}}
{"id": "2602.17529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17529", "abs": "https://arxiv.org/abs/2602.17529", "authors": ["Dun Yuan", "Hao Zhou", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation", "comment": null, "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.", "AI": {"tldr": "KG-RAG\u6846\u67b6\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u63d0\u5347LLM\u5728\u7535\u4fe1\u9886\u57df\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898", "motivation": "\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u4fe1\u9886\u57df\u5e94\u7528\u56f0\u96be\uff0c\u56e0\u4e3a\u8be5\u9886\u57df\u5177\u6709\u590d\u6742\u6027\u9ad8\u3001\u6807\u51c6\u4e0d\u65ad\u6f14\u8fdb\u3001\u4e13\u4e1a\u672f\u8bed\u591a\u7b49\u7279\u70b9\uff0c\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u4e0d\u51c6\u786e\u3001\u5e7b\u89c9\u589e\u591a\u3001\u5b9e\u7528\u6027\u964d\u4f4e", "method": "\u63d0\u51faKG-RAG\u6846\u67b6\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u76f8\u7ed3\u5408\u3002\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u7535\u4fe1\u6807\u51c6\u548c\u6587\u6863\u7684\u7ed3\u6784\u5316\u9886\u57df\u77e5\u8bc6\u8868\u793a\uff0cRAG\u5b9e\u73b0\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u4e8b\u5b9e\u6765\u652f\u6491\u6a21\u578b\u8f93\u51fa", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKG-RAG\u4f18\u4e8e\u7eafLLM\u548c\u6807\u51c6RAG\u57fa\u7ebf\uff0c\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u6bd4RAG\u63d0\u9ad814.3%\uff0c\u6bd4\u7eafLLM\u63d0\u9ad821.6%", "conclusion": "KG-RAG\u80fd\u6709\u6548\u5728\u590d\u6742\u7535\u4fe1\u573a\u666f\u4e2d\u751f\u6210\u51c6\u786e\u3001\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u89e3\u51b3\u4e86\u7535\u4fe1\u9886\u57dfLLM\u5e94\u7528\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2602.17560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17560", "abs": "https://arxiv.org/abs/2602.17560", "authors": ["Hongjue Zhao", "Haosen Sun", "Jiangtao Kong", "Xiaochang Li", "Qineng Wang", "Liwei Jiang", "Qi Zhu", "Tarek Abdelzaher", "Yejin Choi", "Manling Li", "Huajie Shao"], "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment", "comment": "Accepted by ICLR 2026", "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6ODESteer\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c4f\u969c\u51fd\u6570\u548c\u591a\u6b65\u81ea\u9002\u5e94\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6307\u5bfc\u5f15\u5bfc\u65b9\u5411\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u4f9d\u8d56\u5355\u6b65\u5f15\u5bfc\u800c\u65e0\u6cd5\u6355\u6349\u6fc0\u6d3b\u5206\u5e03\u7684\u590d\u6742\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u6fc0\u6d3b\u52a0\u6cd5\u89e3\u91ca\u4e3aODE\u7684\u4e00\u9636\u8fd1\u4f3c\uff0c\u901a\u8fc7\u63a7\u5236\u7406\u8bba\u4e2d\u7684\u5c4f\u969c\u51fd\u6570\u5b9a\u4e49\u5f15\u5bfc\u65b9\u5411\uff0c\u5e76\u5f00\u53d1ODESteer\u65b9\u6cd5\u5b9e\u73b0\u591a\u6b65\u81ea\u9002\u5e94\u5f15\u5bfc\u3002", "result": "ODESteer\u5728\u591a\u4e2aLLM\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728TruthfulQA\u4e0a\u63d0\u53475.7%\uff0cUltraFeedback\u4e0a\u63d0\u53472.5%\uff0cRealToxicityPrompts\u4e0a\u63d0\u53472.4%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7ODE\u7edf\u4e00\u4e86\u6fc0\u6d3b\u5f15\u5bfc\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u7684ODESteer\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u539f\u5219\u6027\u89c6\u89d2\u3002"}}
