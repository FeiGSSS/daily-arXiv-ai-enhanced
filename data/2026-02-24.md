<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 该研究探讨了使用形式化领域本体（特别是OpenMath本体）通过检索增强生成来提升语言模型在数学推理任务中的可靠性，发现本体引导的上下文在检索质量高时能提升性能，但无关上下文会降低性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在幻觉、脆弱性和缺乏形式化基础等根本性限制，这些在需要可验证推理的高风险专业领域（如数学）中尤为成问题。研究旨在探索形式化领域本体是否能通过检索增强生成来提高语言模型的可靠性。

Method: 使用数学作为概念验证，实现了一个神经符号管道，利用OpenMath本体结合混合检索和交叉编码器重排序，将相关定义注入模型提示中。在MATH基准上评估了三个开源模型。

Result: 评估显示，当检索质量高时，本体引导的上下文能提高性能，但无关上下文会主动降低性能，这突显了神经符号方法的潜力和挑战。

Conclusion: 形式化领域本体在提升语言模型可靠性方面具有潜力，但检索质量至关重要。无关上下文会损害性能，这表明需要更精确的检索机制来充分发挥神经符号方法的优势。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [2] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: TTG是一个基于编程谜题对决的评估框架，让大语言模型相互挑战创建谜题，通过Elo评分比较模型能力，无需人工创建测试题。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理能力面临挑战：人工创建高质量测试题成本高昂，且难以区分模型是真正推理还是见过类似训练数据。需要一种无法被设计饱和的评估范式。

Method: 受16世纪数学决斗启发，设计Token Games评估框架：模型通过创建编程谜题相互挑战（给定返回布尔值的Python函数，找到使其返回True的输入），然后进行两两对决计算Elo评分。

Result: 评估了10个前沿模型，TTG的排名结果与Humanity's Last Exam等现有基准高度一致，且无需人工创建谜题。发现创建优质谜题对当前模型仍是高度挑战性的任务，这是先前基准未测量的能力。

Conclusion: TTG提出了一种新的推理评估范式，不会被设计饱和，能够同时测试模型的创造力、任务创建能力和问题解决能力，为模型评估开辟了新方向。

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [3] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: WorkflowPerturb是一个用于评估工作流评估指标的基准测试，通过向黄金工作流应用受控扰动来研究指标性能


<details>
  <summary>Details</summary>
Motivation: LLM生成的结构化工作流评估困难，因为指标分数通常未校准，且分数变化不能直接反映工作流退化的严重程度

Method: 提出WorkflowPerturb基准，包含4,973个黄金工作流和44,757个扰动变体，应用三种扰动类型（缺失步骤、压缩步骤、描述变化），每种在10%、30%、50%严重级别上测试

Result: 基准测试了多个指标家族，分析其敏感性和校准性，使用预期分数轨迹和残差，结果揭示了指标家族间的系统性差异，支持基于严重程度的工作流评估分数解释

Conclusion: WorkflowPerturb为工作流评估指标提供了受控基准，有助于理解指标性能并支持严重程度感知的评估分数解释，数据集将在接受后发布

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [4] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: 该研究将离线强化学习与跨具身学习相结合，通过分析16个机器人平台的运动数据集，发现该方法在包含大量次优轨迹的数据集上优于纯行为克隆，但次优数据比例和机器人类型增加会导致梯度冲突，通过基于形态相似性的分组策略可有效缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器人策略预训练中高质量演示数据收集成本高的问题，通过结合离线强化学习和跨具身学习来利用专家和次优数据，并聚合不同形态机器人的轨迹来获取通用控制先验。

Method: 采用离线强化学习与跨具身学习相结合的方法，构建包含16个不同机器人平台的运动数据集进行评估。针对次优数据比例和机器人类型增加导致的梯度冲突问题，提出基于形态相似性的分组策略，将机器人按形态相似性聚类，并使用组梯度更新模型。

Result: 实验证实该方法在包含丰富次优轨迹的数据集上预训练效果优于纯行为克隆。但随着次优数据比例和机器人类型增加，不同形态间的梯度冲突会阻碍学习。提出的静态分组策略能显著减少机器人间的冲突，并优于现有的冲突解决方法。

Conclusion: 离线强化学习与跨具身学习结合的方法能有效利用次优数据进行机器人策略预训练，但需要解决多机器人类型带来的梯度冲突问题。基于形态相似性的简单静态分组策略是有效的解决方案。

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [5] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: 研究发现无监督表示学习即使排除敏感属性，仍会编码这些属性信息，挑战了"通过无知实现公平"的假设


<details>
  <summary>Details</summary>
Motivation: 挑战"通过无知实现公平"的假设，即无监督表示在训练中排除敏感属性时会被认为是中立的。研究旨在证明即使明确排除敏感属性，这些属性仍会在无监督嵌入中作为主导潜在轴出现。

Method: 使用SOMtime（基于高容量自组织映射的拓扑保持表示方法），在两个大规模真实世界数据集（五个国家的世界价值观调查和人口普查收入数据集）上进行实验，比较PCA、UMAP、t-SNE和自编码器等传统方法。

Result: SOMtime恢复了与排除的敏感属性（如年龄和收入）对齐的单调排序，Spearman相关性高达0.85，而PCA和UMAP通常低于0.23，t-SNE和自编码器最多达到0.34。无监督分割SOMtime嵌入会产生人口统计学偏斜的聚类。

Conclusion: "通过无知实现公平"在表示层面对有序敏感属性失败，公平审计必须扩展到机器学习管道的无监督组件。无监督表示可能无意中编码敏感属性信息，带来下游公平风险。

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [6] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: OMAD：首个基于扩散策略的在线多智能体强化学习框架，通过放松的策略目标最大化联合熵，实现高效探索与协调，在多个任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成和离线设置中表现出强大的表达能力和多模态表示能力，但在在线多智能体强化学习中尚未充分探索。主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1）使用放松的策略目标最大化缩放联合熵，实现有效探索而不依赖可处理似然；2）在CTDE范式下，采用联合分布值函数优化去中心化扩散策略；3）利用可处理的熵增强目标指导扩散策略的同步更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上进行广泛评估，OMAD成为新的最先进方法，样本效率显著提高2.5倍到5倍。

Conclusion: OMAD成功将扩散策略应用于在线多智能体强化学习，通过创新的放松策略目标和联合分布值函数，克服了扩散模型不可处理似然性的障碍，实现了高效的探索和协调。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>
