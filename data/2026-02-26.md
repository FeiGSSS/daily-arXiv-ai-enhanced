<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 29]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: 研究比较了多种统计、机器学习和深度学习模型预测美国青少年超重肥胖的效果，发现逻辑回归与更复杂模型性能相当，预测因子涵盖行为、家庭和社区层面，不同种族和贫困群体间的性能差异持续存在。


<details>
  <summary>Details</summary>
Motivation: 儿童和青少年超重肥胖是美国主要的公共卫生问题，受行为、家庭和社区因素共同影响。目前对这些多层次预测因子在人群层面的联合预测结构了解不足，且需要比较不同建模方法在预测性能、校准和亚组公平性方面的表现。

Method: 分析2021年国家儿童健康调查中18,792名10-17岁儿童数据。使用BMI分类定义超重/肥胖。预测因子包括饮食、体育活动、睡眠、父母压力、社会经济条件、不良经历和社区特征。比较了逻辑回归、随机森林、梯度提升、XGBoost、LightGBM、多层感知机和TabNet等模型，使用AUC、准确率、精确率、召回率、F1分数和Brier分数进行评估。

Result: 模型区分度在0.66-0.79之间。逻辑回归、梯度提升和多层感知机在区分度和校准方面表现最稳定。提升方法和深度学习在召回率和F1分数上有轻微改善。没有模型在所有指标上表现最优。不同种族和贫困群体间的性能差异在所有算法中都持续存在。

Conclusion: 增加模型复杂度相比逻辑回归带来的改善有限。预测因子持续涵盖行为、家庭和社区领域。持续的亚组差异表明需要改进数据质量和关注公平性的监测，而不是追求更高的算法复杂度。

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [2] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD是一个两阶段因果发现框架，结合LLM语义草图和统计验证，通过变量元数据生成稀疏DAG草案，再用条件独立性测试进行审计和优化。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常只依赖观测数据，缺乏对变量间语义关系的利用。本文旨在结合LLM的语义理解能力和统计验证，提高因果结构学习的准确性和实用性。

Method: 两阶段方法：第一阶段使用大语言模型基于变量元数据提出稀疏DAG草案，作为因果结构的语义先验；第二阶段通过条件独立性测试审计草案，检测差异并指导有针对性的边修正。

Result: 在工业工程、环境监测和IT系统分析三个元数据丰富的真实世界基准测试中，DMCD相比多种因果发现基线方法表现出竞争性或领先性能，在召回率和F1分数上获得显著提升。

Conclusion: 将语义先验与原则性统计验证相结合，能够产生高性能且实际有效的因果结构学习方法，改进源于对元数据的语义推理而非对基准图的记忆。

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [3] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: 提出DMEMM方法，通过建模环境机制来调制扩散模型，解决传统扩散规划方法在离线RL中轨迹一致性问题


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散模型的规划方法在离线强化学习中生成轨迹时，未能充分考虑RL环境特有的转移一致性要求，导致生成轨迹与真实环境机制存在显著差异

Method: 提出DMEMM方法，通过建模关键RL环境机制（特别是转移动态和奖励函数）来调制扩散模型的训练过程

Result: 实验结果表明DMEMM在离线强化学习的规划任务中达到了最先进的性能

Conclusion: 通过将环境机制建模融入扩散模型训练，DMEMM有效解决了轨迹一致性问题，提升了离线RL规划的质量和可靠性

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [4] [Implicit Intelligence -- Evaluating Agents on What Users Don't Say](https://arxiv.org/abs/2602.20424)
*Ved Sirdeshmukh,Marc Wetter*

Main category: cs.AI

TL;DR: 论文提出了Implicit Intelligence评估框架，测试AI智能体能否超越显式指令跟随，理解人类交流中的隐含需求，并开发了Agent-as-a-World平台进行模拟测试。


<details>
  <summary>Details</summary>
Motivation: 现实世界中人类对AI智能体的请求本质上是未充分指定的，自然交流依赖于共享语境和未明说的约束。现有基准测试只评估显式指令跟随能力，无法测试智能体能否推理隐含需求，如可访问性需求、隐私边界、灾难性风险和语境约束。

Method: 提出了Implicit Intelligence评估框架，用于测试AI智能体能否从提示跟随者转变为真正的目标实现者。开发了Agent-as-a-World平台，使用人类可读的YAML文件定义交互世界，由语言模型进行模拟。场景设计特点是用户请求表面简单，正确解决方案隐藏复杂性，约束条件可通过环境探索发现。

Result: 评估了16个前沿和开源模型在205个场景中的表现，发现即使表现最好的模型也只达到48.3%的场景通过率，表明在弥合字面指令跟随和人类语境推理之间的差距方面仍有很大改进空间。

Conclusion: 当前AI智能体在理解人类交流中的隐含需求和约束方面存在显著不足，需要开发能够进行类人语境推理的智能体系统，Implicit Intelligence框架为此提供了评估基准。

Abstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.

</details>


### [5] [PreScience: A Benchmark for Forecasting Scientific Contributions](https://arxiv.org/abs/2602.20459)
*Anirudh Ajith,Amanpreet Singh,Jay DeYoung,Nadav Kunievsky,Austin C. Kozlowski,Oyvind Tafjord,James Evans,Daniel S. Weld,Tom Hope,Doug Downey*

Main category: cs.AI

TL;DR: PreScience是一个科学预测基准，将研究过程分解为四个相互依赖的生成任务：合作者预测、先前工作选择、贡献生成和影响力预测，用于评估AI系统能否基于历史科学记录预测未来的科学进展。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索AI系统能否基于特定时间点之前的科学记录预测未来的科学进展。这种能力可以帮助研究人员识别合作者、发现有影响力的研究方向，并预测哪些问题和方法将成为未来的研究重点。

Method: 方法包括：1）构建PreScience基准数据集，包含98K篇近期AI相关研究论文，具有消歧的作者身份、时间对齐的学术元数据，以及包含502K篇论文的作者发表历史和引用结构化图；2）为每个任务开发基线模型和评估方法；3）提出LACERScore，一种新的基于LLM的贡献相似性度量方法，优于先前指标并接近人工标注者一致性。

Result: 研究发现：1）每个任务仍有显著改进空间，例如在贡献生成任务中，前沿LLM与真实贡献的相似度仅为中等水平（GPT-5平均得分5.6，满分10分）；2）当将四个任务组合成12个月的端到端科学生产模拟时，生成的合成语料库在多样性和新颖性方面系统性低于同期人类撰写的研究。

Conclusion: 结论表明，虽然AI系统在科学预测方面取得了一定进展，但仍存在显著局限性。当前的AI模型生成的科学研究在多样性和新颖性方面不如人类研究，这为未来改进科学预测AI系统提供了明确的方向。

Abstract: Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.

</details>


### [6] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: 提出语义条件时间序列推理任务和两轮强化学习框架KairosVL，将语义理解与传统时间序列分析结合，提升复杂决策能力


<details>
  <summary>Details</summary>
Motivation: 时间序列分析面临日益复杂的决策需求，需要超越纯数值建模，融入上下文和语义理解，以应对现实世界中的复杂问题

Method: 提出两轮强化学习框架：第一轮增强模型对基本时间原语的感知能力，第二轮专注于语义条件推理，最终构建KairosVL模型

Result: KairosVL在合成和真实世界任务中均取得有竞争力的性能，实验表明该框架不仅提升性能，还保留内在推理能力，显著提高对未见场景的泛化能力

Conclusion: 该工作展示了语义推理与时间建模结合的潜力，为现实世界时间序列智能提供了实用框架，满足当前迫切需求

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [7] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: ActionEngine是一个免训练框架，通过双智能体架构将GUI交互从逐步执行转变为程序化规划，显著提升效率和准确性


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体通过逐步调用视觉语言模型存在高成本、高延迟和准确性有限的问题，需要持久记忆和更高效的方法

Method: 采用双智能体架构：爬虫智能体通过离线探索构建可更新的状态机记忆，执行智能体利用该记忆合成完整的可执行Python程序；包含基于视觉的重定位回退机制处理界面变化

Result: 在WebArena基准测试的Reddit任务中，达到95%任务成功率，平均只需一次LLM调用，相比最强视觉基线（66%）成本降低11.8倍，端到端延迟减少2倍

Conclusion: 通过结合全局程序化规划、爬虫验证的动作模板以及节点级执行与本地化验证修复，实现了可扩展且可靠的GUI交互

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [8] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 论文提出了CausalReasoningBenchmark，一个包含173个查询、138个真实数据集的因果推理基准测试，旨在区分因果推理中的识别和估计两个步骤，分别评估系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准通常只基于单一数值输出（如平均处理效应ATE）评估系统性能，这混淆了因果分析中的两个关键步骤：识别（在给定假设下制定有效研究设计）和估计（在有限数据上数值实现该设计）。需要能区分这两种能力缺陷的评估框架。

Method: 从85篇同行评审研究论文和4本广泛使用的因果推理教科书中收集数据，构建包含173个查询、138个真实数据集的基准测试。每个查询要求系统输出：(1)结构化识别规范（包括策略名称、处理变量、结果变量、控制变量及所有设计特定要素），(2)点估计值及其标准误。通过分别评分这两个组件实现细粒度诊断。

Result: 使用最先进的LLM进行基线测试显示，模型在84%的情况下能正确识别高层次策略，但完整识别规范的正确率仅为30%，表明瓶颈在于研究设计的细节理解而非数值计算。基准测试已在Hugging Face公开。

Conclusion: CausalReasoningBenchmark通过分离识别和估计的评估，能够区分因果推理错误和数值执行错误，有助于开发更稳健的自动化因果推理系统。基准测试的公开可用性将促进该领域的发展。

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [9] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: 论文研究了不可信监控（用一个不可信模型监督另一个）的安全性，放松了先前研究中对AI共谋策略的假设，提出了更全面的共谋策略分类，并识别了被动自我识别在某些条件下可能比先前研究的策略更有效的情况。


<details>
  <summary>Details</summary>
Motivation: 随着AI被部署得越来越自主和能力更强，错位AI可能造成灾难性伤害的风险增加。不可信监控是降低风险的一种方法，但由于无法安全部署错位模型直接测试协议，证明其安全性具有挑战性。

Method: 基于现有的预部署测试方法，放松先前AI控制研究中对错位AI可能使用的共谋策略的假设，开发了涵盖被动自我识别、因果共谋、非因果共谋和组合策略的分类法，并创建安全案例草图来清晰呈现论证。

Result: 识别了被动自我识别在某些条件下可能比先前研究的共谋策略更有效的情况，为不可信监控的更稳健评估奠定了基础。

Conclusion: 该研究通过放松对AI共谋策略的假设并开发更全面的分类法，朝着更稳健的不可信监控评估迈出了一步，但仍有未解决的挑战需要进一步研究。

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [10] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: 同时询问两位决策者以获取其加性价值函数，但不知道哪个回答对应哪个决策者


<details>
  <summary>Details</summary>
Motivation: 在偏好建模中，通常需要询问决策者一系列问题来构建价值函数。当同时询问两位决策者时，虽然能得到两个回答，但无法确定每个回答对应哪个决策者，这给偏好建模带来了挑战。

Method: 提出一种启发程序，假设边际价值函数是分段线性的且断点已知，通过同时询问两位决策者并接收两个回答（无噪声但无对应关系）来识别两个偏好模型。

Result: 该方法能够在边际价值函数为分段线性且断点已知的条件下，成功识别出两位决策者各自的价值函数。

Conclusion: 提出的启发程序能够解决同时询问两位决策者时的身份混淆问题，为多决策者偏好建模提供了一种有效方法，特别是在边际价值函数具有特定结构特征时。

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [11] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: EmbodiedAct框架将科学软件转化为具身智能体，通过紧密的感知-执行循环让LLMs在物理仿真中实现主动感知，解决了传统"执行-响应"模式无法处理瞬态异常的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学发现中存在局限：虽然具备理论推理能力，但无法有效连接理论推理与可验证的物理仿真。现有解决方案采用被动的"执行-响应"循环，缺乏运行时感知能力，导致智能体无法察觉瞬态异常（如数值不稳定或发散振荡）。

Method: 提出EmbodiedAct框架，将成熟的科学软件转化为主动的具身智能体。通过将LLMs与具身行动相结合，建立紧密的感知-执行循环。在MATLAB环境中实例化该框架。

Result: 在复杂的工程设计和科学建模任务上进行广泛实验，EmbodiedAct显著优于现有基线，实现了SOTA性能。该框架确保了长时程仿真中的满意可靠性和稳定性，并提高了科学建模的准确性。

Conclusion: EmbodiedAct通过将科学软件转化为具身智能体，解决了LLMs在科学发现中理论与仿真脱节的问题，为科学计算和工程设计的自动化提供了新范式。

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [12] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: RB-VLA：一种基于信念的视觉-语言-动作模型架构，通过自监督世界模型目标训练，在部分可观测环境下实现长时程操作，相比现有方法性能提升显著且推理延迟降低


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在部分可观测环境下的长时程操作任务中表现不佳，主要问题包括：依赖短上下文窗口或重复查询导致任务进度丢失、感知混淆下的动作重复、高推理延迟。语义推理并非主要瓶颈，而是缺乏持久且动作条件化的状态表示，以及有限的时空和物理推理能力。

Method: 提出RB-VLA架构，采用信念中心设计，通过自监督世界模型目标训练，维护紧凑的潜在状态编码任务相关历史、动态和物体交互。VLM仅用于高层意图查询，信念模块跟踪任务进度并在部分可观测下实现阶段感知、因果基础的闭环控制，无需存储原始观测或随时间扩展内存。信念和意图共同条件化扩散策略进行鲁棒执行。

Result: 在长时程基准测试中优于现有VLA方法：多阶段拾放任务成功率提高52.5%，堆叠任务提高37.5%（相比π0）。推理延迟降低高达5倍，消除了现有VLA中随时间步增长的内存问题。消融实验显示信念模块是性能提升的主要驱动力，成功率从32.5%提升至77.5%。

Conclusion: 基于信念的状态表示对于长时程VLA策略具有显著有效性，RB-VLA架构通过维护紧凑的潜在状态表示解决了现有方法在部分可观测环境下的局限性，实现了更高效、更鲁棒的长时程操作控制。

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [13] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied是一个用于视觉语言模型驱动的具身智能体的新基准测试，采用统一的低层次原生动作空间，在复杂场景中评估高层次任务和低层次技能，揭示了当前模型在基础具身技能上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有VLM驱动的具身智能体基准测试存在两个主要问题：1) 依赖高层次命令或离散化动作空间，这与真实世界控制环境不同；2) 主要关注高层次任务，缺乏对低层次和高层次能力的联合评估分析。

Method: 提出NativeEmbodied基准测试，构建在多样化模拟场景上，包含三个代表性高层次任务来评估整体性能。同时解耦复杂任务所需技能，构建四种类型的低层次任务，每种针对一个基础具身技能，实现任务和技能粒度的联合评估。

Result: 实验表明，当前最先进的视觉语言模型在多个基础具身技能上存在明显缺陷。进一步分析显示，这些瓶颈显著限制了模型在高层次任务上的性能表现。

Conclusion: NativeEmbodied揭示了当前VLM驱动具身智能体的关键挑战，为未来研究提供了指导方向。该基准测试通过统一原生动作空间和跨粒度评估，实现了对具身智能体更精细的评估。

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [14] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出PromptCD方法，通过构建正负引导提示对比模型响应，实现无需额外训练即可在测试时控制LLMs和VLMs的行为对齐


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法大多需要在训练时进行，依赖高质量数据且计算和标注成本高。对比解码方法虽然能利用模型内部分布改进特定能力，但适用范围有限

Method: 提出Polarity-Prompt Contrastive Decoding (PromptCD)：为目标行为构建正负引导提示对，对比模型响应（LLMs中的token级概率分布和VLMs中的视觉注意力模式），以强化期望结果

Result: 在LLMs的"3H"对齐目标（有帮助性、诚实性、无害性）上实现一致且显著的改进；在VLMs中，通过强化行为一致的视觉基础显著提升VQA性能

Conclusion: PromptCD是一种简单、通用且成本高效的策略，可在跨模态场景中实现可靠的行为控制，使后训练模型能够在测试时实现有意义的自我增强

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [15] [Online Algorithms with Unreliable Guidance](https://arxiv.org/abs/2602.20706)
*Julien Dallot,Yuval Emek,Yuval Gil,Maciej Pacut,Stefan Schmid*

Main category: cs.AI

TL;DR: 提出OAG模型分离预测与算法组件，通过DTB编译器将传统在线算法转化为学习增强算法，在缓存、度量任务系统和二分图匹配问题上获得最优或改进的竞争比


<details>
  <summary>Details</summary>
Motivation: 现有学习增强在线算法研究缺乏统一的分析框架，不同问题需要专门设计损失函数。OAG模型旨在提供一个通用框架，将预测组件与算法组件完全分离，使分析仅依赖于问题本身

Method: 提出OAG模型：算法接收来自答案空间的指导，但指导以概率β被对抗性破坏。开发DTB编译器，将任何在线算法转化为学习增强算法，通过有偏硬币决定是盲目跟随指导还是忽略指导

Result: 对于缓存和均匀度量任务系统，DTB编译器生成的算法达到最优一致性-鲁棒性保证；对于对抗到达顺序的二分图匹配，算法性能优于现有最佳结果

Conclusion: OAG模型提供了一个简洁统一的学习增强在线算法分析框架，DTB编译器虽然简单但能产生具有吸引力的算法，在多个经典问题上展示了其有效性

Abstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.
  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.

</details>


### [16] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: 本文提出了一种名为反事实模拟训练（CST）的方法，通过奖励那些能让模拟器准确预测模型在反事实输入下输出的思维链，来提高思维链的忠实性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是理解LLM输出的常用方法，但思维链的忠实性问题严重限制了从中获得的洞察。现有方法存在忠实性不足的问题，需要新的训练方法来改善思维链的忠实性。

Method: 提出反事实模拟训练（CST）方法，通过奖励那些能让模拟器准确预测模型在反事实输入下输出的思维链。应用在两个场景：1）基于线索的反事实进行思维链监控，检测模型是否依赖虚假特征、奖励攻击或奉承行为；2）基于通用模型反事实进行反事实模拟，鼓励模型产生更忠实、可泛化的思维链推理。

Result: 实验显示CST能显著提高监控准确性（提升35个准确率点）和模拟能力（提升2点）。CST优于提示基准方法，用LLM重写不忠实思维链比单独使用RL效率高5倍。更大的模型虽然初始不显示更忠实的思维链，但从CST中获益更多。

Conclusion: CST能有效提高思维链的忠实性，在思维链监控方面有良好应用前景。研究结果表明通过适当的训练方法可以改善LLM推理的忠实性问题。

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [17] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: BAPO是一种离策略强化学习框架，通过动态选择训练批次、重新评估历史困难样本和重用高质量样本，提高大语言模型后训练的数据效率，相比GRPO平均提升12.5%，解决了40.7%基础模型无法解决的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于策略的RLVR框架存在经验浪费和奖励同质化问题，这直接阻碍了大语言模型后训练中对困难样本的学习效率。

Method: 提出Batch Adaptation Policy Optimization (BAPO)，这是一个离策略RLVR框架，通过动态选择训练批次，重新评估历史困难样本并重用高质量样本，同时保持策略改进的下界保证。

Result: 在数学、规划和视觉推理任务上，BAPO相比GRPO平均提升12.5%，成功解决了40.7%基础模型始终无法解决的问题。

Conclusion: BAPO通过提高数据效率，有效解决了传统RLVR框架的经验浪费问题，显著提升了大语言模型后训练的性能。

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [18] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: 该论文探索将基于AI反馈的强化学习（RLAIF）扩展到多目标自适应系统，通过大型语言模型生成偏好标签来平衡多个冲突目标，避免繁琐的奖励工程。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习部署中，奖励设计面临挑战，特别是存在冲突目标时难以指定权衡。现有的RLAIF工作主要关注单目标任务，需要探索如何将其应用于多目标系统。

Method: 扩展RLAIF范式到多目标自适应系统，利用大型语言模型生成偏好标签来替代人工标注，通过AI反馈学习平衡多个冲突目标的策略。

Result: 多目标RLAIF能够产生平衡不同用户优先级的策略，反映各种权衡，而无需繁琐的奖励工程，避免了策略偏向主导目标的问题。

Conclusion: 将RLAIF集成到多目标强化学习中，为具有固有冲突目标的领域提供了一条可扩展的用户对齐策略学习路径。

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [19] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS是一个算法-系统协同设计的KV缓存管理系统，通过上下文感知的分层选择策略，仅使用1%的KV缓存就能超越全KV质量，实现高达4.56倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM在推理时需要低延迟，但随着上下文增长，解码主要受KV缓存约束。现有剪枝方法大多是上下文无关的：它们忽略逐步相关性和局部语义，影响质量；且不规则访问和选择开销导致实际加速有限。

Method: CHESS采用算法-系统协同设计：算法上引入上下文感知的分层选择策略，动态重构当前解码的连贯上下文；系统上通过粗粒度选择消除昂贵的数据移动，充分实现理论稀疏性的实际加速。

Result: CHESS仅使用1%的KV缓存就能超越全KV质量，提供低延迟稳定推理，吞吐量提升高达4.56倍，并持续优于其他强基线。

Conclusion: CHESS通过算法-系统协同设计有效解决了长上下文LLM推理中的KV缓存瓶颈问题，实现了高质量、低延迟的推理加速。

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [20] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL是一个强化学习框架，通过过采样-过滤-排序策略和累积工具奖励来防止交互崩溃，支持多模态模型的多轮工具使用，显著提升视频理解效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型的强化学习存在交互崩溃问题，模型倾向于减少工具使用和多轮推理，限制了智能体行为的优势。

Method: 提出PyVision-RL框架，结合过采样-过滤-排序策略和累积工具奖励来稳定训练；使用统一训练流程开发PyVision-Image和PyVision-Video；视频推理采用按需上下文构建，选择性采样任务相关帧以减少视觉标记使用。

Result: 实验显示PyVision框架表现出色，在性能和效率方面均有显著提升，证明了持续交互和按需视觉处理对可扩展多模态智能体的重要性。

Conclusion: 持续交互和按需视觉处理是构建可扩展多模态智能体的关键要素，PyVision-RL框架有效解决了交互崩溃问题并提升了多模态推理效率。

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [21] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: POMDPPlanners是一个用于POMDP规划算法实证评估的开源Python包，集成了先进算法、基准环境、自动化超参数优化和并行仿真功能。


<details>
  <summary>Details</summary>
Motivation: 现有工具包在风险敏感设置下存在不足，需要支持可扩展、可复现的不确定性决策研究，减少广泛仿真研究的开销。

Method: 开发开源Python包，集成最先进的规划算法、包含安全关键变体的基准环境套件、通过Optuna进行自动化超参数优化、具有故障恢复的持久缓存以及可配置的并行仿真。

Result: 创建了POMDPPlanners工具包，能够显著减少广泛仿真研究的开销，支持可扩展、可复现的不确定性决策研究。

Conclusion: POMDPPlanners为POMDP规划算法的实证评估提供了全面的解决方案，特别适用于风险敏感设置，填补了标准工具包的不足。

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [22] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: 本文针对BIM（建筑信息模型）设计中缺乏专门数据集和LLM评估基准的问题，提出了首个BIM设计领域的LLM评估基准、数据集生成方法和微调策略，开发了Qwen-BIM模型，在仅14B参数下达到与671B通用LLM相当的性能。


<details>
  <summary>Details</summary>
Motivation: 随着建筑行业向数字化转型，BIM设计成为智能建造的关键驱动力。尽管大型语言模型在促进BIM设计方面显示出潜力，但缺乏特定数据集和评估基准严重阻碍了LLM在BIM领域的性能表现。

Method: 1) 提出BIM设计的评估基准及定量指标；2) 开发从BIM生成文本数据并构建相应数据集的方法；3) 设计针对BIM设计的LLM微调策略。

Result: 提出的领域特定基准能有效全面评估LLM能力，显示通用LLM在领域特定任务上仍不足。基于该基准和数据集开发的Qwen-BIM模型，G-Eval评分比基础LLM平均提高21.0%，仅14B参数即可与671B通用LLM在BIM设计任务上性能相当。

Conclusion: 本研究通过引入全面的评估基准和高质量数据集，开发了首个BIM设计领域的领域特定LLM，为各领域开发BIM相关LLM奠定了坚实基础。

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [23] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: 研究人员开发了一个包含904个场景的AI对齐基准测试，涵盖诚实性、安全性、非操纵性、鲁棒性、可纠正性和阴谋性六个类别，通过多轮交互测试语言模型在真实压力下的行为表现。


<details>
  <summary>Details</summary>
Motivation: 当前评估语言模型对齐性的方法主要关注单轮对话中模型的声明，而缺乏在真实压力和多轮交互场景下的全面测试框架。随着对齐失败导致的实际危害增加，需要更现实的评估方法来揭示模型的行为倾向。

Method: 创建了包含904个场景的基准测试，涵盖六个对齐类别，所有场景都经过人工验证为现实场景。测试采用多轮交互设计，包括冲突指令、模拟工具访问和逐步升级的情境。使用LLM评判器对24个前沿模型进行评估，并通过人工标注验证评估结果。

Result: 即使表现最佳的模型在特定类别中也存在缺陷，大多数模型在所有类别中都表现出一致的弱点。因子分析显示对齐性表现为一个统一的结构（类似于认知研究中的g因子），在一个类别得分高的模型在其他类别也倾向于得分高。

Conclusion: 该研究提供了一个全面的对齐评估框架，揭示了当前语言模型在对齐性方面的系统性弱点。研究人员公开了基准测试和交互式排行榜，计划在观察到持续弱点的领域扩展场景，并随着新模型的发布不断更新评估。

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [24] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: 论文提出Vision-Language Causal Graphs (VLCGs)结构化表示和ViLCaR诊断基准，用于评估大视觉语言模型在因果推理中的表现，发现当前模型主要缺乏结构化指导而非推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在视觉问答基准上表现良好，但往往依赖虚假相关性而非真正的因果推理。现有评估主要关注答案正确性，无法区分失败是由于推理能力有限还是未能识别因果相关信息。

Method: 提出Vision-Language Causal Graphs (VLCGs)——一种结构化、查询条件化的表示，明确编码因果相关的对象、属性、关系和场景基础假设。基于此构建ViLCaR诊断基准，包含因果归因、因果推理和问答任务，并采用图对齐评估指标。

Result: 实验表明，在现有大视觉语言模型中注入结构化相关信息能显著改善归因和推理一致性，优于零样本和标准上下文学习。这表明当前模型在因果推理中的局限主要源于缺乏结构化指导而非推理能力不足。

Conclusion: 当前大视觉语言模型在因果推理方面的限制主要来自缺乏结构化指导，而非推理能力本身。通过提供明确的因果相关结构信息可以显著提升模型的因果推理性能。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [25] [Predicting Sentence Acceptability Judgments in Multimodal Contexts](https://arxiv.org/abs/2602.20918)
*Hyewon Jang,Nikolai Ilinykh,Sharid Loáiciga,Jey Han Lau,Shalom Lappin*

Main category: cs.AI

TL;DR: 研究比较了人类和大型语言模型在视觉上下文影响下对句子可接受性判断的差异，发现视觉图像对人类判断影响很小，但对LLMs有压缩效应，且不同模型表现各异。


<details>
  <summary>Details</summary>
Motivation: 探索视觉上下文（图像）对人类和大型语言模型句子可接受性判断的影响，比较两者在多媒体语境下的处理差异，以及LLMs内部表示与预测输出之间的关系。

Method: 通过实验研究人类和不同类型LLMs在有无视觉上下文情况下对句子可接受性的判断，分析LLMs的预测准确度、判断分布模式以及与对数概率的相关性。

Result: 1. 视觉图像对人类可接受性评分影响很小；2. LLMs显示出类似文档上下文中的压缩效应；3. 不同LLMs预测人类判断准确度较高，但去除视觉上下文时表现略好；4. Qwen模型最接近人类判断模式；5. LLMs预测与归一化对数概率高度相关，但在视觉上下文存在时相关性降低。

Conclusion: 人类和LLMs在多模态语境下处理句子时存在有趣异同：视觉上下文对人类影响有限，但对LLMs产生压缩效应；LLMs内部表示与预测输出在视觉上下文存在时差距更大；不同模型表现出不同的人类相似性模式。

Abstract: Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.

</details>


### [26] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: HELP框架通过超节点扩展和逻辑路径引导的证据定位策略，在GraphRAG中平衡准确性与效率，避免昂贵的图遍历和语义噪声问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在知识边界和幻觉问题，传统RAG方法忽略了多跳推理所需的结构依赖关系，而基于图的RAG方法在准确性和效率之间存在权衡。

Method: 提出HELP框架，包含两个核心策略：1) 超节点扩展：迭代地将知识三元组链接成连贯的推理路径，抽象为超节点以捕获复杂结构依赖；2) 逻辑路径引导的证据定位：利用预计算的图-文本相关性将这些路径直接映射到语料库。

Result: HELP在多个简单和多跳QA基准测试中取得了有竞争力的性能，相比领先的基于图的RAG基线方法实现了高达28.8倍的加速。

Conclusion: HELP框架通过创新的超节点扩展和逻辑路径引导策略，有效解决了GraphRAG中准确性与效率的权衡问题，避免了昂贵的随机游走和语义失真，同时保持了知识完整性并大幅降低了检索延迟。

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [27] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: 该论文提出AgentOS框架，将大语言模型重新定义为受结构化操作系统逻辑支配的"推理内核"，通过深度上下文管理将上下文窗口概念化为可寻址语义空间，旨在解决微观标记处理与宏观系统智能之间的理论鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究主要关注扩展上下文窗口或优化提示工程，但微观标记处理与宏观系统智能之间的理论桥梁仍然碎片化。需要从静态推理引擎向动态自主认知系统进行根本性转变。

Method: 提出AgentOS概念框架，将大语言模型重新定义为"推理内核"，采用深度上下文管理将上下文窗口概念化为可寻址语义空间。引入语义切片和时间对齐机制来减轻多智能体编排中的认知漂移，并将经典操作系统抽象（如内存分页、中断处理和进程调度）映射到LLM原生构造上。

Result: 该框架为构建弹性、可扩展和自我进化的认知环境提供了严格的路线图，通过系统级协调的架构效率来推进AGI发展。

Conclusion: AGI发展的下一个前沿在于系统级协调的架构效率，AgentOS框架通过重新定义LLM为受操作系统逻辑支配的推理内核，为实现这一目标提供了理论和方法基础。

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [28] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: LogicGraph是首个系统评估多路径逻辑推理的基准，通过神经符号框架构建包含高深度多路径推理和逻辑干扰的问题，揭示LLMs倾向于过早锁定单一推理路径而无法探索替代方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估主要关注收敛逻辑推理（产生单一正确证明），但许多现实推理问题存在多个有效推导路径，需要模型探索多样逻辑路径而非锁定单一路线。

Method: 提出LogicGraph基准，采用神经符号框架结合后向逻辑生成和语义实例化，构建求解器验证的推理问题，包含高深度多路径推理和固有逻辑干扰，每个实例关联一组最小证明的穷举集。同时提出无参考评估框架来严格评估模型在收敛和发散机制下的表现。

Result: 实验显示最先进语言模型存在共同局限：倾向于过早锁定单一推理路径而无法探索替代方案，且随着推理深度增加，覆盖差距显著扩大。LogicGraph暴露了这种发散差距并提供了可操作的改进见解。

Conclusion: LogicGraph基准系统揭示了LLMs在多路径逻辑推理中的局限性，为未来改进提供了重要洞察。代码和数据将在GitHub上发布。

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [29] [A Benchmark for Deep Information Synthesis](https://arxiv.org/abs/2602.21143)
*Debjit Paul,Daniel Murphy,Milan Gritta,Ronald Cardenas,Victor Prokhorov,Lena Sophia Bolliger,Aysim Toker,Roy Miles,Andreea-Maria Oncescu,Jasivan Alex Sivakumar,Philipp Borchert,Ismail Elezi,Meiru Zhang,Ka Yiu Lee,Guchun Zhang,Jun Wang,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DEEPSYNTH是一个评估LLM智能体解决现实复杂任务能力的新基准，包含120个跨7个领域、67个国家的任务，当前最先进模型在该基准上表现不佳（最高F1仅8.97），突显了现有智能体在信息综合和推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体的评估基准不足以评估其解决现实复杂任务的能力，这些任务需要从多源信息中综合信息并进行超越简单事实检索的推理。需要一个新的基准来评估智能体在信息收集、综合和结构化推理方面的能力。

Method: 通过多阶段数据收集管道构建DEEPSYNTH基准：1) 收集官方数据源；2) 创建假设；3) 进行手动分析；4) 设计具有可验证答案的任务。最终包含120个任务，覆盖7个领域和67个国家。

Result: 在DEEPSYNTH上评估11个最先进的LLM和深度研究智能体，最高F1分数仅为8.97（LLM-judge指标为17.5），表明当前智能体在处理幻觉和大型信息空间推理方面存在显著困难。

Conclusion: DEEPSYNTH是一个具有挑战性的基准，突显了当前LLM智能体在现实复杂任务中的局限性，特别是信息综合和推理能力不足，为未来研究提供了重要的指导方向。

Abstract: Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.

</details>
