<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Dynamic Survey of Soft Set Theory and Its Extensions](https://arxiv.org/abs/2602.21268)
*Takaaki Fujita,Florentin Smarandache*

Main category: cs.AI

TL;DR: 本书对软集理论及其主要扩展进行了综述性概述，涵盖核心定义、代表性构造和当前发展方向


<details>
  <summary>Details</summary>
Motivation: 软集理论为参数化决策建模提供了直接框架，通过为每个属性分配给定宇宙的子集来结构化表示不确定性。该理论在过去几十年中已扩展到多种变体，并与拓扑学、拟阵理论等多个领域建立了联系。

Method: 采用综述性方法，系统性地概述软集理论及其主要扩展，包括超软集、超超软集、树软集、双极软集和动态软集等变体，突出核心定义和代表性构造。

Result: 提供了软集理论及其扩展的全面概述，展示了该理论的发展脉络、核心概念框架以及与其他数学领域的连接。

Conclusion: 本书通过系统性的综述，为读者提供了软集理论及其扩展的完整图景，指明了当前研究的关键方向和未来发展路径。

Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.

</details>


### [2] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: PANGAEA-GPT：基于分层多智能体框架的自主数据发现与分析系统，解决地球科学数据规模增长带来的可扩展性挑战


<details>
  <summary>Details</summary>
Motivation: 地球科学数据快速积累导致可扩展性挑战，PANGAEA等存储库中大量数据集未被充分利用，限制了数据可重用性。需要一种能够自主发现和分析异构存储库数据的解决方案。

Method: 提出PANGAEA-GPT分层多智能体框架，采用集中式Supervisor-Worker拓扑结构，具有严格的数据类型感知路由、沙盒确定性代码执行和通过执行反馈的自我校正机制，使智能体能够诊断和解决运行时错误。

Result: 通过物理海洋学和生态学的用例场景，展示了系统能够以最少人工干预执行复杂的多步骤工作流程，证明其处理异构存储库数据的能力。

Conclusion: 该框架为通过协调智能体工作流程查询和分析异构存储库数据提供了一种方法论，解决了地球科学数据可扩展性和可重用性挑战。

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [3] [ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning](https://arxiv.org/abs/2602.21534)
*Xiaoxuan Wang,Han Zhang,Haixin Wang,Yidan Shi,Ruoyan Li,Kaiqiao Han,Chenyi Tong,Haoran Deng,Renliang Sun,Alexander Taylor,Yanqiao Zhu,Jason Cong,Yizhou Sun,Wei Wang*

Main category: cs.AI

TL;DR: 本文提出了ARLArena框架和SAMPO方法，用于解决Agentic强化学习（ARL）训练不稳定的问题，通过系统化分析政策梯度的四个核心设计维度，实现了稳定且高性能的ARL训练。


<details>
  <summary>Details</summary>
Motivation: 尽管Agentic强化学习（ARL）在解决复杂多步交互任务方面显示出潜力，但训练过程极不稳定，经常导致训练崩溃。这种不稳定性限制了ARL在更大环境和更长交互时间尺度上的可扩展性，也阻碍了对算法设计选择的系统性探索。

Method: 首先提出ARLArena框架，包括：1）构建干净标准化的测试平台；2）将政策梯度分解为四个核心设计维度并评估每个维度的性能和稳定性。基于此分析，提出了SAMPO（稳定Agentic政策优化方法），旨在缓解ARL中的主要不稳定因素。

Result: SAMPO在各种Agentic任务中实现了持续稳定的训练和强大的性能表现。该研究为ARL提供了统一的政策梯度视角，并为构建稳定可复现的基于LLM的Agent训练流程提供了实用指导。

Conclusion: 本研究通过系统化分析ARL训练稳定性问题，提出了ARLArena框架和SAMPO方法，为Agentic强化学习领域提供了理论基础和实践指导，有助于推动该领域向更大规模和更复杂任务的发展。

Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.

</details>


### [4] [The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems](https://arxiv.org/abs/2602.21745)
*Hyo Jin Kim*

Main category: cs.AI

TL;DR: ASIR勇气模型是一个将真相披露形式化为状态转换而非人格特质的相动力学框架，适用于人类和AI系统在风险下的真相披露行为。


<details>
  <summary>Details</summary>
Motivation: 传统上将真相披露视为人格特质，但作者认为这实际上是一个相动力学过程，需要建立统一框架来解释人类在不对称风险下的沉默和AI系统在策略约束下的输出失真。

Method: 提出ASIR勇气模型，将真相披露建模为从抑制状态(S0)到表达状态(S1)的相变过程，当促进力超过抑制阈值时发生转换：λ(1+γ)+ψ > θ+φ。该框架可扩展到AI系统，其中抑制对应约束输出状态，结构压力来自竞争目标、上下文张力和递归交互动态。

Result: 该模型提供了统一的结构性解释，既能说明人类在压力下的沉默，也能解释AI偏好驱动的失真。反馈扩展模型显示转换结果如何递归地重新校准系统参数，在重复交互中产生路径依赖和发散效应。

Conclusion: ASIR勇气模型通过将勇气和对齐置于共享的动力学结构中，为人类和人工系统在风险下的真相披露提供了形式化视角，将明显的真实性变化解释为约束相空间中相互作用力的几何结果，而非归因于AI系统的意图。

Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.
  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.
  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.

</details>


### [5] [fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation](https://arxiv.org/abs/2602.21746)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: fEDM+扩展了原有的模糊伦理决策框架，增加了可解释性模块和多元语义验证，提升了伦理AI系统的透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 原有的fEDM框架虽然保证了形式正确性和决策一致性，但未能充分解决两个关键挑战：决策的原则性可解释性，以及在伦理多元主义下的鲁棒性。

Method: 1. 引入可解释性和可追溯性模块（ETM），将伦理决策规则与底层道德原则明确关联，为每个推荐行动计算加权原则贡献度。2. 用多元语义验证框架替代单参考验证，针对多个利益相关者参考进行评估，每个参考编码不同的原则优先级和风险容忍度。

Result: 扩展后的fEDM+保留了形式可验证性，同时实现了增强的可解释性和利益相关者感知的验证，使其适合作为伦理敏感AI系统的监督和治理层。

Conclusion: fEDM+通过引入原则性可解释性和多元验证机制，在保持形式严谨性的同时，更好地应对了伦理决策中的透明性和多元价值挑战，为伦理AI系统提供了更全面的治理框架。

Abstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.

</details>


### [6] [Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem](https://arxiv.org/abs/2602.21814)
*Heejin Jo*

Main category: cs.AI

TL;DR: STAR推理框架将汽车清洗问题的准确率从0%提升到85%，结合用户画像和RAG上下文后达到100%准确率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在"汽车清洗问题"这一需要隐式物理约束推理的基准测试中表现不佳，研究旨在探索哪些提示架构层能够实现正确推理

Method: 使用Claude 3.5 Sonnet进行变量隔离研究，控制超参数，测试不同提示架构层：STAR推理框架、用户画像上下文、RAG上下文

Result: STAR框架单独将准确率从0%提升到85%，用户画像增加10个百分点，RAG再增加5个百分点，完整堆栈条件下达到100%准确率

Conclusion: 对于隐式约束推理任务，结构化推理框架（特别是强制目标表达）比上下文注入更为重要

Abstract: Large language models consistently fail the "car wash problem," a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.

</details>


### [7] [Distill and Align Decomposition for Enhanced Claim Verification](https://arxiv.org/abs/2602.21857)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Fernando Acero,Arturo Oncevay,Charese H. Smiley,Xiaomo Liu,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出基于强化学习的联合优化方法，通过GRPO算法同时优化分解质量和验证器对齐，提升复杂声明验证性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将分解质量与验证性能对齐，复杂声明验证需要将句子分解为可验证的子声明，但现有方法在此方面存在不足

Method: 采用强化学习方法，结合结构化顺序推理、教师蒸馏示例的监督微调，以及平衡格式合规性、验证器对齐和分解质量的多目标奖励函数

Result: 在六个评估设置中，训练的8B分解器将下游验证性能提升至71.75% macro-F1，优于提示方法（+1.99，+6.24）和现有RL方法（+5.84），人类评估确认生成子声明的高质量

Conclusion: 该框架使较小语言模型能够通过联合优化验证准确性和分解质量，实现最先进的声明验证性能

Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

</details>


### [8] [ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices](https://arxiv.org/abs/2602.21858)
*Dezhi Kong,Zhengzhao Feng,Qiliang Liang,Hao Wang,Haofei Sun,Changpeng Yang,Yang Li,Peng Zhou,Shuai Nie,Hongzhen Wang,Linfeng Zhou,Hao Jia,Jiaming Xu,Runyu Shi,Ying Huang*

Main category: cs.AI

TL;DR: ProactiveMobile是一个用于评估移动智能体主动智能能力的基准测试，包含3,660个实例和63个API函数，旨在解决现有MLLMs在主动预测用户意图方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在移动智能体开发中主要局限于被动执行用户命令的反应式范式，而主动智能（智能体自主预测需求并启动行动）是移动智能体的下一个前沿领域。然而，该领域的发展受到缺乏能够应对现实世界复杂性并支持客观、可执行评估的基准测试的严重制约。

Method: 提出了ProactiveMobile基准测试，将主动任务形式化为基于设备上下文信号的四个维度推断潜在用户意图，并从包含63个API的综合函数池中生成可执行函数序列。基准包含14个场景的3,660个实例，通过多答案标注拥抱现实世界复杂性。为确保质量，30名专家团队对基准进行最终审核，验证事实准确性、逻辑一致性和行动可行性，并纠正不合规条目。

Result: 实验表明，微调的Qwen2.5-VL-7B-Instruct模型成功率达到19.15%，优于o1（15.71%）和GPT-5（7.39%）。这表明主动智能是当前MLLMs普遍缺乏的关键能力，但它是可学习的，强调了所提出基准对主动智能评估的重要性。

Conclusion: ProactiveMobile基准测试为移动智能体主动智能研究提供了系统化的评估框架，揭示了当前MLLMs在主动智能方面的不足，并证明了通过专门训练可以提升这种能力，为未来主动智能研究奠定了基础。

Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.

</details>


### [9] [Semantic Partial Grounding via LLMs](https://arxiv.org/abs/2602.22067)
*Giuseppe Canonaco,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: SPG-LLM使用大语言模型分析PDDL描述，在规划任务接地前识别潜在无关的对象、动作和谓词，显著减少接地任务规模，实现更快的接地速度


<details>
  <summary>Details</summary>
Motivation: 经典规划中的接地步骤常因任务规模增大导致接地动作和原子指数增长而成为计算瓶颈。现有部分接地方法主要依赖关系特征或学习嵌入，未能充分利用PDDL描述中的文本和结构线索

Method: SPG-LLM利用大语言模型分析领域和问题文件，在接地前启发式识别潜在无关的对象、动作和谓词，从而显著减少接地任务的规模

Result: 在七个难以接地的基准测试中，SPG-LLM实现了更快的接地速度（通常快几个数量级），在某些领域还能提供相当或更好的规划成本

Conclusion: SPG-LLM通过利用大语言模型分析PDDL的文本和结构信息，有效解决了规划中的接地瓶颈问题，显著提高了接地效率

Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.

</details>


### [10] [Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning](https://arxiv.org/abs/2602.22094)
*Nguyen Cong Nhat Le,John G. Rogers,Claire N. Bonial,Neil T. Dantam*

Main category: cs.AI

TL;DR: 提出基于Petri网可达性松弛的方法，用于鲁棒不变式合成、高效目标不可达性检测和提供不可行性解释，支持目标和约束的增量更新。


<details>
  <summary>Details</summary>
Motivation: 现实中的计划经常因情况变化或理解变化而需要调整，有时甚至不存在可行计划。现有规划方法主要关注可行情况下的高效一次性规划，而缺乏对领域更新和不可行性检测的支持。

Method: 采用Petri网可达性松弛技术，实现鲁棒不变式合成；结合增量约束求解器，支持目标和约束的增量更新；提供不可行性检测和解释机制。

Result: 与基线方法相比，系统生成相当数量的不变式，检测到最多2倍的不可行情况，在一次性规划中表现相当，在顺序计划更新中表现更优。

Conclusion: 提出的Petri网可达性松弛方法能够有效支持计划更新、不可行性检测和解释，在动态规划场景中具有实用价值。

Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.

</details>
