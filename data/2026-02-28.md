<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: 本文提出GYWI系统，结合作者知识图谱与检索增强生成，为LLMs提供可控学术背景和可追溯灵感路径，以生成高质量科学创意。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在科学创意生成中缺乏可控的学术背景和可追溯的灵感路径，导致生成结果质量有限。

Method: 1) 提出作者中心的知识图谱构建方法和灵感源采样算法；2) 结合RAG和GraphRAG的混合检索机制；3) 融入强化学习原理的Prompt优化策略。

Result: 在arXiv数据集上实验表明，GYWI在GPT-4o、DeepSeek-V3等多个LLMs上，在创新性、可靠性、相关性等指标上显著优于主流LLMs。

Conclusion: GYWI系统通过结合知识图谱和混合检索机制，有效提升了LLMs生成科学创意的质量和可控性。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [2] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个综合金融基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题，并发布了基准问题和评估代码。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能全面评估LLMs在金融领域的理论知识和实际应用能力，需要建立一个系统性的评估框架来衡量LLMs在复杂金融任务中的表现。

Method: 1) 理论评估：从广泛认可的金融资格考试中收集多样化考题；2) 实践评估：提出系统性评估矩阵，分类复杂金融领域并覆盖关键子域和业务活动；3) 收集3000个金融场景问题，包括封闭式决策题和开放式问题；4) 在FIRE基准上评估最先进的LLMs，包括轩辕4.0作为金融领域基线模型。

Result: 对最先进的LLMs进行了全面评估，包括轩辕4.0金融领域模型，结果能够系统分析当前LLMs在金融应用中的能力边界。

Conclusion: FIRE基准为评估LLMs在金融领域的理论知识和实践能力提供了全面框架，公开发布基准问题和评估代码将促进未来研究，帮助理解LLMs在金融应用中的能力边界。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [3] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 该论文提出了因果嵌入框架，将多个详细模型映射到粗粒度因果模型的子系统中，作为抽象化的推广，并展示了其在统计边际问题和因果边际问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的因果抽象方法主要关注两个模型之间的关系，但实际应用中需要将多个详细模型整合到一个统一的粗粒度因果模型中。为了处理来自不同表示模型的多个数据集合并问题，需要更通用的框架。

Method: 提出因果嵌入作为抽象化的推广，定义广义一致性概念，建立多分辨率边际问题框架，将多个详细模型映射到粗粒度因果模型的子系统中。

Result: 因果嵌入框架能够统一处理统计边际问题和因果边际问题，展示了在合并来自不同表示模型的数据集时的实际应用价值。

Conclusion: 因果嵌入提供了一个强大的框架，用于整合多个详细因果模型到一个统一的粗粒度模型中，解决了多分辨率建模和数据合并的实际问题。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [4] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究人员开发了CogARC数据集来研究人类抽象推理策略，发现人类在视觉推理任务中表现出高准确性但策略多样性，错误答案也往往高度收敛。


<details>
  <summary>Details</summary>
Motivation: 研究人类在抽象推理中的认知策略灵活性，特别是如何从稀疏示例中快速学习和应用规则，为理解人类抽象推理能力提供实证基础。

Method: 使用CogARC数据集（从ARC基准中选取的人类适应子集），在两个实验中让260名参与者解决75个抽象视觉推理问题，记录高时间分辨率的示例查看、编辑序列和多尝试提交等行为数据。

Result: 参与者整体表现良好（实验1准确率约90%，实验2约80%），但问题难度和参与者表现差异大。困难问题引发更长思考时间和更多策略分歧。任务过程中响应速度加快但准确率略有下降。即使错误答案也往往高度收敛，但解题轨迹在长度和平滑度上存在差异。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，揭示了人们在不确定性下如何泛化、错误泛化和调整策略，为理解人类抽象推理的认知机制提供了重要见解。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [5] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 研究提出一个概率框架，让异质智能体通过学习评估自身可靠性并选择性弃权投票，将孔多塞陪审团定理推广到置信度门控的序列设置中。


<details>
  <summary>Details</summary>
Motivation: 经典认知投票理论（如孔多塞陪审团定理）假设固定参与，但现实世界聚合往往受益于允许智能体说"我不知道"。需要研究智能体如何学习评估自身可靠性并选择性弃权对集体准确性的影响。

Method: 提出概率框架：智能体经历校准阶段，更新对自身固定能力的信念，然后面对最终置信度门控决定是否投票或弃权。推导群体成功概率的非渐近下界，证明选择性参与将CJT渐近保证推广到序列置信度门控设置。

Result: 证明了选择性参与将孔多塞陪审团定理的渐近保证推广到序列置信度门控设置，并通过蒙特卡洛模拟验证了这些边界。框架可应用于AI安全，缓解集体LLM决策中的幻觉问题。

Conclusion: 允许智能体学习自身可靠性并选择性弃权可以改善集体决策准确性，为经典投票理论提供了现实扩展，在AI安全领域有潜在应用价值。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [6] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行综合分析，发现普遍存在捷径行为，潜在表示虽能编码多种可能性但未实现结构化搜索，监督强度存在权衡：强监督减少捷径但限制多样性，弱监督增加多样性但加剧捷径。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为一种新兴推理范式，通过在潜在空间而非文本空间进行多步推理，超越了离散语言标记的限制。尽管已有大量研究致力于提升潜在推理性能，但其内部机制尚未得到充分探究。本文旨在深入分析潜在推理方法，以更好地理解潜在表示在推理过程中的作用和表现。

Method: 对具有不同监督水平的潜在推理方法进行综合分析，识别关键问题：1）观察普遍的捷径行为模式；2）检验潜在推理是否支持潜在空间中的广度优先搜索式探索；3）分析监督强度对潜在推理能力的影响。

Result: 发现两个关键问题：1）普遍存在捷径行为，模型能在不依赖潜在推理的情况下获得高准确率；2）潜在表示虽能编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩。监督强度存在权衡：强监督能缓解捷径行为但限制潜在表示保持多样假设的能力，弱监督允许更丰富的潜在表示但以增加捷径行为为代价。

Conclusion: 潜在推理方法存在系统性局限，需要更深入理解其内部机制。监督强度在缓解捷径行为和保持潜在表示多样性之间存在根本性权衡，这为未来改进潜在推理方法提供了重要指导方向。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [7] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 提出对比世界模型(CWM)，通过对比学习训练动作可行性评分器，在ScienceWorld基准上优于传统监督微调方法


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的动作评分器将每个候选动作独立处理，未能显式区分物理上正确与微妙错误的动作，需要更可靠的可行性评估方法

Method: 使用InfoNCE对比目标对大型语言模型进行微调，通过挖掘困难负例（语义相似但物理不兼容的动作）在评分空间中分离有效与无效动作

Result: 在605个困难负例测试对上，CWM比SFT在最小编辑负例上的Precision@1提高6.76个百分点，AUC-ROC达0.929；在任务执行中，CWM的安全边际(-2.39)显著优于SFT(-3.96)

Conclusion: 对比训练比单纯监督微调能更准确地捕捉物理可行性，为具身智能体提供了更可靠的动作评分器

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [8] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: 论文提出ConstraintBench基准，评估大语言模型在无求解器情况下直接解决完全指定的约束优化问题的能力，发现可行性而非最优性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLMs能否将优化问题表述为求解器代码，但缺乏对LLMs能否直接产生正确解决方案的评估。需要填补这一空白，评估LLMs在无求解器访问情况下的约束优化能力。

Method: 引入ConstraintBench基准，涵盖10个运筹学领域，包含200个任务。每个任务提供自然语言场景描述，模型需返回结构化解决方案，由确定性验证器检查所有约束和求解器证明的最优解。

Result: 评估6个前沿模型发现：可行性是主要瓶颈，最佳模型仅达到65.0%约束满足率；可行解平均达到Gurobi最优目标的89-96%；无模型能在0.1%误差内同时满足可行性和最优性超过30.5%；不同领域难度差异大，可行性从83.3%到0.8%不等。

Conclusion: LLMs在直接约束优化中面临显著挑战，可行性是主要限制因素。ConstraintBench为评估LLMs的约束推理能力提供了标准化基准，揭示了系统性的失败模式，有助于未来改进。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [9] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估编码智能体优化性能的框架，包含版本控制、奖励机制和观察系统，旨在系统研究智能体通过编辑-执行-评估循环进行迭代改进的能力。


<details>
  <summary>Details</summary>
Motivation: 编码智能体的一个重要应用是智能体优化：通过编辑-执行-评估循环迭代改进目标智能体。然而，社区缺乏对此任务性能的系统理解。智能体优化与传统软件工程有根本区别：目标智能体将确定性代码与随机LLM完成交织在一起，需要结构化捕获中间推理和下游执行结果。

Method: 引入VERO（版本控制、奖励和观察）框架，提供：(1) 可复现的评估工具链，包含版本化智能体快照、预算控制评估和结构化执行轨迹；(2) 包含目标智能体和任务的基准套件，以及参考评估程序。

Result: 使用VERO进行了实证研究，比较不同任务上的优化器配置，分析哪些修改能可靠提升目标智能体性能。研究发现了一些能有效改进智能体性能的优化策略。

Conclusion: VERO框架支持编码智能体优化作为核心能力的研究，为系统评估和改进智能体性能提供了标准化工具和方法。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [10] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 该研究利用大语言模型对AI与生命周期评估交叉领域的研究进行系统性综述，揭示了AI技术在LCA中应用的快速增长趋势，特别是LLM驱动方法的兴起，并提出了一个结合传统文献综述与LLM文本挖掘的动态分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用近年来快速发展，但缺乏对该领域研究的全面综合与系统分析。本研究旨在填补这一空白，通过系统综述AI-LCA交叉领域的研究现状，识别当前趋势和未来方向。

Method: 采用大语言模型驱动的文本挖掘方法与传统文献综述技术相结合，构建动态有效的分析框架，既能捕捉高层次研究趋势，又能识别细微的概念模式（主题）。

Result: 分析显示：随着LCA研究持续扩展，AI技术应用急剧增长；LLM驱动方法显著增加；机器学习应用持续增长；AI方法与对应LCA阶段存在统计学显著相关性。

Conclusion: LLM辅助方法具有支持大规模、可重复跨领域综述的潜力，同时为在AI技术快速发展背景下实现计算高效的LCA评估了路径。这项工作有助于LCA从业者将先进工具和及时见解融入环境评估，提升可持续性决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [11] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 论文提出"双可预测性"（bipredictability）作为衡量系统有效利用信息的新指标，证明其在量子、经典和具能动性系统中的不同上限，并基于此区分了能动性与智能，指出当前AI系统只具备前者。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然能处理大量信息并做出复杂预测，但预测成功可能掩盖系统与环境互动质量的下降。需要一种原则性的方法来衡量系统部署的总信息中，有多少真正在观察、行动和结果之间共享。

Method: 提出双可预测性P作为衡量指标，从第一性原理推导其数学定义，证明其在量子系统、经典系统和引入能动性系统中的不同理论上限。通过物理系统（双摆）、强化学习智能体和多轮LLM对话验证这些界限。

Result: 双可预测性P在量子系统中可达1，在经典系统中≤0.5，引入能动性后更低。实验验证了这些理论界限。基于此区分能动性（基于预测行动的能力）与智能（还需从互动中学习、自我监控学习效果并调整观察/行动/结果范围）。

Conclusion: 当前AI系统只具备能动性而非智能。受生物丘脑皮质调节启发，提出实时监控P的反馈架构，为构建自适应、有韧性的AI系统奠定基础。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [12] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，通过定义智能体模板来指导多个LLM的协同工作。


<details>
  <summary>Details</summary>
Motivation: 当前单个大型语言模型仍难以解决许多复杂问题，需要探索如何将多个LLM组合成更强大的整体系统，但缺乏系统化的设计方法。

Method: 提出智能体模板的概念，用于定义单个LLM的角色和功能组合方式；系统梳理现有文献中的语言智能体，识别其背后源自认知模型或AI算法的模板设计。

Result: 展示了多种现有语言智能体如何基于认知科学和AI算法的模板构建，验证了这种设计方法的可行性和有效性。

Conclusion: 认知科学和AI算法启发的智能体模板是开发高效、可解释语言智能体的有力工具，值得进一步研究和应用。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [13] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出基于智能体的AI框架，用于无蜂窝O-RAN中的意图翻译和优化，通过多个LLM智能体协作处理复杂意图，实现节能和资源管理


<details>
  <summary>Details</summary>
Motivation: 现有O-RAN中的智能体大多处理简单意图且独立工作，缺乏处理需要多智能体协调的复杂意图的框架

Method: 提出多智能体框架：监督智能体翻译运营商意图，用户权重智能体确定用户优先级，O-RU管理智能体使用DRL算法管理激活的O-RU，监控智能体保障最小速率要求，采用PEFT方法提升可扩展性

Result: 在节能模式下，相比三种基准方案，提出的框架将活跃O-RU数量减少41.93%；使用PEFT方法相比部署独立LLM智能体减少92%内存使用

Conclusion: 提出的智能体AI框架能够有效处理O-RAN中的复杂意图，实现显著的节能效果和内存优化，为自主RAN提供了可行的多智能体协作解决方案

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [14] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习策略将人类专家作为交互式推理工具，在专业领域任务中显著提升LLM智能体的成功率


<details>
  <summary>Details</summary>
Motivation: LLM智能体在通用推理方面表现出色，但在需要长尾知识的专业领域中表现不佳。人类专家虽然能提供这些知识，但他们的指导往往非结构化且不可靠，难以直接整合到智能体的规划中。

Method: 提出AHCE（主动人类增强挑战参与）框架，核心是人类反馈模块（HFM），该模块通过学习策略将人类专家视为交互式推理工具，实现按需的人机协作。

Result: 在Minecraft中的大量实验表明，该框架显著提升了任务成功率：普通难度任务提高32%，高难度任务提高近70%，且仅需最小化的人类干预。

Conclusion: 成功增强智能体需要学习如何请求专家推理，而不仅仅是简单的求助。主动学习如何利用人类专业知识比被动接收指导更有效。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [15] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard是一个基于检索增强的多智能体框架，将安全评估重新构想为证据辩论，通过基于外部政策文档的对抗性辩论实现最先进的安全性能，无需微调即可适应新治理规则。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全机制严重依赖静态的微调分类器，存在适应僵化问题，无法在不进行昂贵重新训练的情况下强制执行新的治理规则。

Method: 引入CourtGuard框架，采用检索增强的多智能体架构，将安全评估重新构想为证据辩论，通过基于外部政策文档的对抗性辩论来评估模型安全性。

Result: 在7个安全基准测试中达到最先进性能，优于专用政策遵循基线；实现零样本适应性（在维基百科破坏检测任务中达到90%准确率）；能够自动数据整理和审计，创建了9个新颖的对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了一条稳健、可解释且适应性强的路径，能够满足当前和未来的监管要求。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [16] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 该研究发现示例引导在数学推理中的效果不稳定源于策略使用与策略可执行性之间的差距，提出选择性策略检索框架来提升推理性能


<details>
  <summary>Details</summary>
Motivation: 示例引导在数学推理中广泛使用，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用与策略可执行性之间的未充分探索的差距。

Method: 通过分析配对的人类编写和模型生成的解决方案，识别使用与可执行性之间的系统性差异。提出选择性策略检索（SSR）框架，通过经验性、多路径、源感知信号选择性地检索和组合策略，显式建模可执行性。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导，提供了可靠且一致的改进。在AIME25上准确率提升高达+13分，在Apex上提升+5分，对紧凑推理模型效果显著。

Conclusion: 策略使用与可执行性之间的差距是示例引导效果不稳定的关键原因。SSR框架通过显式建模可执行性，能够有效提升数学推理性能，为推理引导提供了更可靠的方法。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [17] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 该论文提出将心理测量学评分者模型整合到AI评估流程中，通过多面Rasch模型分离真实输出质量与评分者行为偏差，提高人类评估的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型训练和评估中至关重要，但这些数据很少被视为存在系统误差的测量。当前AI评估中的人类评分数据存在评分者效应（严重性和中心性偏差），导致观察评分失真，影响结论的可靠性。

Method: 采用心理测量学评分者模型，特别是多面Rasch模型，将真实输出质量与评分者行为分离。使用OpenAI摘要数据集作为实证案例，展示如何通过调整评分者严重性偏差来校正摘要质量估计，并提供评分者表现的诊断洞察。

Result: 通过调整评分者严重性偏差，获得了校正后的摘要质量估计，并提供了评分者表现的诊断信息。研究表明心理测量学建模能够产生更准确、可靠的评估结果。

Conclusion: 将心理测量学建模整合到人机交互评估中，能够更原则性、透明地使用人类数据，使开发者能够基于调整后的分数而非原始、易错的评分做出决策。这为AI开发和评估提供了更稳健、可解释且与构念对齐的实践路径。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [18] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：一种利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务来减少多步推理任务中的内存使用，在智能体任务中可将峰值令牌使用量降低65%


<details>
  <summary>Details</summary>
Motivation: 长期运行的智能体任务（如深度研究）需要在多个网页和文档之间进行多跳推理，导致LLM上下文被外部检索的令牌主导，内存使用快速增长并限制解码性能。现有的KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: 提出SideQuest方法，利用大型推理模型自身通过推理其上下文中有用令牌来执行KV缓存压缩。为防止管理过程相关的令牌污染模型内存，将KV缓存压缩作为与主要推理任务并行执行的辅助任务。仅用215个样本训练模型即可实现。

Result: 评估显示，SideQuest在智能体任务中可将峰值令牌使用量降低高达65%，同时准确率下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让大型推理模型自身管理其上下文，有效解决了多步推理任务中的KV缓存压缩问题，显著减少了内存使用，同时保持了推理质量。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [19] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于在线广告多渠道自动出价的分层框架，结合生成式规划和实时控制，相比现有基线提升13.57%的总体回报。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂多变，多渠道场景下需要有效分配预算和约束。现有优化方法缺乏动态适应性，强化学习方法难以捕捉历史依赖和观测模式。

Method: 提出AHBid框架：1）基于扩散模型的高层生成规划器动态分配预算和约束；2）约束执行机制确保符合约束；3）轨迹精炼机制利用历史数据增强适应性；4）控制式出价算法结合历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线实现了13.57%的总体回报提升。

Conclusion: AHBid通过整合生成式规划和实时控制，有效解决了多渠道自动出价中的动态适应性问题，显著提升了广告投资回报。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [20] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算的同步RLHF训练框架，通过动态资源适配、共享前缀预计算和成本感知的actor扩展策略，实现1.35倍加速和44.8%成本降低


<details>
  <summary>Details</summary>
Motivation: RLHF在大型语言模型对齐中应用广泛，但现有RLHF框架基于服务器基础设施，难以应对RL训练中动态变化的资源需求，导致组件间空闲时间和资源浪费问题

Method: 1) 基于无服务器计算环境构建同步RLHF训练框架；2) 动态适应RLHF流水线中的资源需求变化；3) 预计算共享前缀避免重复计算；4) 采用考虑响应长度变化的成本感知actor扩展策略；5) 高效分配工作负载减少函数内不平衡和空闲时间

Result: 在物理测试平台和大规模模拟集群上的实验表明，RLHFless相比最先进的基线方法实现了1.35倍加速和44.8%的成本降低

Conclusion: RLHFless通过无服务器计算环境有效解决了同步RLHF训练中的资源浪费问题，在提升训练效率的同时显著降低了成本，为大规模RLHF训练提供了可行的解决方案

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [21] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决了传统模型中心方法难以捕捉跨域非结构化依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能依赖于训练数据的质量、数量和相关性。传统方法使用多辅助域数据来丰富目标域信息，但存在域间差异导致负迁移和性能下降的问题。现有的模型中心范式依赖复杂定制架构，难以捕捉跨域的微妙非结构化序列依赖，导致泛化能力差且计算资源需求高。

Method: 提出Taesar（目标对齐序列再生）框架，采用数据中心的对比解码机制，自适应地将跨域上下文编码到目标域序列中。该方法使标准模型能够学习复杂依赖关系，无需复杂的融合架构。

Result: 实验表明Taesar优于模型中心解决方案，并能泛化到各种序列模型。通过生成丰富的数据集，Taesar有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生方法，解决了跨域推荐中的负迁移问题，使标准模型能够有效学习跨域依赖，在性能和泛化能力上优于传统模型中心方法。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [22] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究分析了HealthBench医学AI评估数据集中医生意见分歧的来源，发现81.8%的分歧来自病例层面的残差，无法通过现有元数据、专业领域或语言特征解释，表明医学AI评估中的一致性问题主要是结构性的。


<details>
  <summary>Details</summary>
Motivation: 理解医学AI评估中医生意见分歧的来源，识别可解释分歧的观察特征，为改进医学AI评估设计提供依据。

Method: 对HealthBench数据集中的医生分歧进行分解分析，使用方差分析、回归模型、特征重要性分析等方法，考察评分标准、医生身份、元数据标签、医学专业、语言特征、嵌入表示等因素对分歧的解释能力。

Result: 评分标准身份仅解释3.6-6.9%的分歧方差，医生身份仅解释2.4%。81.8%的病例层面残差无法被现有特征解释。分歧与完成质量呈倒U型关系（AUC=0.689）。可减少的不确定性（缺失上下文、模糊措辞）使分歧几率增加2.55倍，而不可减少的不确定性（真正的医学模糊性）无显著影响。

Conclusion: 医学AI评估中的一致性问题主要是结构性的，但可减少/不可减少不确定性的分离表明，在评估场景中填补信息空白可以降低分歧（当不存在固有的临床模糊性时），这为可操作的评估设计改进指明了方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [23] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench是一个评估LLM智能体长时记忆能力的基准测试，针对真实应用场景中的连续机器生成交互流，提出了包含真实轨迹和合成轨迹的评估框架，并开发了AMA-Agent记忆系统来提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体记忆评估主要关注对话式人机交互，而实际应用中智能体记忆是连续的机器生成交互流，现有评估标准与实际应用存在显著差距，需要更贴近真实场景的评估基准。

Method: 提出AMA-Bench基准测试，包含两个关键组件：1) 真实世界智能体轨迹与专家标注QA；2) 可扩展到任意时长的合成智能体轨迹与基于规则的QA。同时开发AMA-Agent记忆系统，采用因果图结构和工具增强检索。

Result: 现有记忆系统在AMA-Bench上表现不佳，主要因为缺乏因果性和目标信息，以及基于相似性检索的损失性限制。AMA-Agent在AMA-Bench上达到57.22%的平均准确率，比最强基线记忆系统提升11.16%。

Conclusion: AMA-Bench填补了智能体记忆评估的空白，揭示了现有记忆系统的局限性，提出的AMA-Agent通过因果图和工具增强检索有效提升了长时记忆性能，为智能体记忆系统的发展提供了重要基准和改进方向。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [24] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 该研究开发了ClinDet-Bench基准测试，用于评估大语言模型在临床信息不完整场景下的可确定性识别能力，发现现有模型存在过早判断和过度回避问题。


<details>
  <summary>Details</summary>
Motivation: 临床决策常在信息不完整情况下进行，专家需要判断可用信息是否足够做出判断。过早结论和不必要的回避都会危及患者安全。需要评估大语言模型在这方面的能力。

Method: 基于临床评分系统开发ClinDet-Bench基准测试，将信息不完整场景分解为可确定和不可确定条件。识别可确定性需要考虑所有关于缺失信息的假设（包括不太可能的假设），并验证结论是否在所有情况下都成立。

Result: 研究发现近期的大语言模型无法在信息不完整情况下识别可确定性，既会产生过早判断，也会过度回避，尽管它们能正确解释基础评分知识并在完整信息下表现良好。

Conclusion: 现有基准测试不足以评估大语言模型在临床环境中的安全性。ClinDet-Bench为评估可确定性识别能力提供了框架，有助于实现适当的回避，具有在医学和其他高风险领域的应用潜力。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [25] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能开源智能体框架，通过智能体图、深度推理模式和鲁棒工作流解决现有LLM智能体在复杂任务中的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型取得了显著进展，但独立LLM在处理需要与外部工具和动态环境交互的复杂现实任务时能力开始达到瓶颈。现有智能体框架存在工作流简单、性能不稳定、基准支持有限、过度依赖昂贵商业API等问题。

Method: 提出了MiroFlow框架，包含三个核心组件：1）智能体图实现灵活编排；2）可选的深度推理模式提升性能；3）鲁棒的工作流执行确保稳定和可复现的性能。

Result: 在多个智能体基准测试中（包括GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch和FutureX）都取得了最先进的性能表现。

Conclusion: MiroFlow作为一个易于访问、可复现和可比较的基准框架，有望为深度研究社区提供有价值的工具，推动智能体技术的发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [26] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS：一个用于质谱预测的灵活基准框架，支持构建和评估多种深度学习模型架构，提供性能影响因素分析和实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 质谱技术在药物发现和材料科学中提供重要的分子碎片信息，但实验光谱数据缺乏阻碍了分子识别，需要建立计算预测方法。深度学习模型在预测分子结构光谱方面有潜力，但方法异质性和缺乏明确定义的基准使得整体评估具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种不同的模型架构组合，在预处理公共数据集上使用不同指标评估性能。分析影响性能的因素，包括数据集结构多样性、学习率和数据稀疏性等超参数、预训练效果、元数据消融设置和跨域迁移学习分析。

Result: FlexMS框架提供了选择合适模型的实用指导，并通过检索基准模拟实际识别场景，基于预测光谱对潜在匹配进行评分。

Conclusion: FlexMS基准框架解决了质谱预测领域缺乏统一评估标准的问题，为研究人员提供了灵活的工具来构建和评估不同模型架构，并深入分析了影响模型性能的关键因素，推动了该领域的发展。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [27] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: 通过人机协作研究Hermite求积规则误差表示与界，AI在代数运算、系统证明探索、文献综合和LaTeX准备方面表现出色，但需要严格的人类验证和数学直觉指导


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能是否真正有助于创造性数学研究，还是仅仅自动化常规计算并引入错误风险，通过人机协作案例研究提供实证证据

Method: 使用多个AI助手进行系统的人机协作，扩展手动工作成果，通过AI辅助制定和证明多个定理，完整记录研究流程并保持透明度

Result: 发现了Hermite求积规则的新误差表示和界，AI在代数操作、系统证明探索、文献综合和LaTeX准备方面表现出卓越能力，但每一步都需要严格的人类验证

Conclusion: 在适当的怀疑态度和验证协议下，AI工具可以显著加速数学发现，但需要仔细的人类监督和深厚的领域专业知识，人机协作模式对数学研究有重要意义

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [28] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 该论文提出了首个通用智能体评估框架，通过统一协议和Exgentic框架评估通用智能体在不同环境中的表现，建立了首个开放通用智能体排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前通用智能体（能在陌生环境中执行任务而无需领域特定工程）的承诺尚未实现。现有智能体大多是专业化的，缺乏对通用智能体性能的系统性评估。当前的智能体基准测试假设领域特定集成，无法公平评估通用智能体。

Method: 提出了通用智能体评估的概念原则，开发了统一协议实现智能体与基准测试的集成，创建了Exgentic实用框架用于通用智能体评估。在六个环境中对五个主要智能体实现进行了基准测试，建立了首个开放通用智能体排行榜。

Result: 实验表明通用智能体能够跨不同环境泛化，在没有任何环境特定调优的情况下，实现了与领域特定智能体相当的性能。

Conclusion: 该研究将通用智能体评估确立为一流研究目标，通过发布评估协议、框架和排行榜，为通用智能体的系统性研究奠定了基础。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [29] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench是一个针对扫描探针显微镜（SPM）的博士级多模态基准测试，通过自动化数据合成管道生成高质量图像-文本对，使用Anchor-Gated Sieve技术从arXiv和期刊论文中提取数据，并引入SIP-F1评分来评估LLM性能并量化模型"个性"。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在专业科学领域存在数据污染、复杂度不足和人工成本高昂的问题，需要创建专门针对扫描探针显微镜（SPM）的高质量、低成本的自动化评估基准。

Method: 1. 开发全自动数据合成管道，使用Anchor-Gated Sieve技术从2023-2025年的arXiv和期刊论文中提取高质量图像-文本对；2. 采用混合云-本地架构，VLM仅返回空间坐标进行本地高保真裁剪，极大节省token；3. 引入严格不完美惩罚F1（SIP-F1）评分来评估模型性能并量化模型个性类型（保守型、激进型、赌徒型、智慧型）。

Result: SPM-Bench成功建立了扫描探针显微镜领域的专业基准，通过自动化管道生成了高质量数据集，SIP-F1评分不仅建立了严格的能力层次结构，还首次量化了模型的"个性"特征，揭示了当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够准确评估LLM在专业科学领域的性能，并深入理解模型在复杂物理推理中的能力和局限性。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [30] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 本文提出了一种诊断对齐框架，用于分析临床AI系统中AI生成报告与医生验证结果之间的结构化转换信号，在皮肤科病例评估中实现了100%的综合一致性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人机协同验证至关重要，但AI初始推理与专家修正之间的转换很少被作为结构化信号进行分析。现有二元词汇评估方法严重低估了临床意义上的对齐程度。

Method: 引入诊断对齐框架：将AI生成的图像报告保存为不可变的推理状态，并与医生验证结果系统比较。推理流程整合视觉大语言模型、基于BERT的医学实体提取和序列语言模型推理步骤，在专家评审前强制执行领域一致的细化。采用四级一致性评估框架：精确主要匹配率、语义相似性调整率、跨类别对齐和综合一致性率。

Result: 在21个皮肤科病例评估中，精确一致性达到71.4%，语义相似性调整后保持不变。结构化跨类别和鉴别诊断重叠分析显示100%综合一致性，无病例显示完全诊断分歧。

Conclusion: 二元词汇评估严重低估临床意义上的对齐程度。将专家验证建模为结构化转换，能够实现信号感知的修正动态量化，支持可追溯、人机对齐的图像临床决策支持系统评估。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [31] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: RepSPD是一种基于黎曼流形的几何深度学习模型，通过交叉注意力机制和全局双向对齐策略，显著提升了EEG脑电信号解码的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的EEG分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，这限制了脑电信号解码的准确性和解释性。

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真。

Result: 大量实验表明，RepSPD框架显著优于现有的EEG表示方法，展现出卓越的鲁棒性和泛化能力。

Conclusion: RepSPD通过几何深度学习有效整合了功能连接特征和黎曼流形几何，为EEG脑活动解码提供了更准确、更稳健的解决方案。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [32] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 本文提出CC-BOS框架，利用古典中文的简洁性和模糊性绕过LLM安全约束，通过多维度果蝇优化算法自动生成对抗性提示，在黑盒设置下实现高效越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，其安全风险日益受到关注。现有研究表明LLM容易受到越狱攻击，且攻击效果在不同语言环境中存在差异。作者发现古典中文因其简洁性和模糊性能够部分绕过现有安全约束，暴露出LLM的显著漏洞。

Method: 提出CC-BOS框架，将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式和上下文），通过多维度果蝇优化算法（包括嗅觉搜索、视觉搜索和柯西变异）迭代优化，自动生成古典中文对抗性提示。还设计了古典中文到英文的翻译模块以提高可读性和评估准确性。

Result: 大量实验证明CC-BOS框架的有效性，在越狱攻击任务中持续优于最先进的攻击方法。

Conclusion: 古典中文在越狱攻击中具有独特优势，CC-BOS框架能够高效自动化地生成对抗性提示，揭示了LLM在古典中文环境下的安全漏洞，为LLM安全研究提供了新视角。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [33] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体F1比赛策略优化方法，通过交互模块和自博弈训练生成竞争性策略，能够根据对手行为调整进站时机、轮胎选择和能量分配。


<details>
  <summary>Details</summary>
Motivation: F1比赛中，车队需要根据不断变化的比赛条件和竞争对手的行动来调整比赛策略。现有的单智能体策略无法充分考虑到竞争对手的互动影响，因此需要开发能够适应多智能体竞争环境的策略优化方法。

Method: 基于预训练的单智能体策略，引入交互模块来考虑竞争对手的行为。采用交互模块与自博弈训练方案相结合的方法，生成竞争性策略。智能体根据相对性能进行排名，学习平衡能量管理、轮胎磨损、空气动力学互动和进站决策。

Result: 智能体能够根据对手行为自适应调整进站时机、轮胎选择和能量分配，实现稳健且一致的比赛表现。该框架仅依赖真实比赛中可用的信息，因此可以在赛前和赛中支持比赛策略师的决策。

Conclusion: 提出的强化学习方法能够有效优化多智能体F1比赛策略，通过考虑竞争对手互动实现了适应性策略调整，为比赛策略师提供了实用的决策支持工具。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [34] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本文提出AILS-AHD方法，利用大语言模型自动设计启发式规则，解决带容量约束的车辆路径问题，在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 带容量约束的车辆路径问题（CVRP）是组合优化中的基本挑战，其NP难特性在大规模实例中带来显著计算困难。现有方法在处理大规模问题时仍有改进空间，需要更智能的启发式设计方法。

Method: 提出AILS-AHD方法，将进化搜索框架与大语言模型结合，动态生成和优化AILS方法中的破坏启发式。同时引入基于LLM的加速机制提升计算效率。

Result: 与AILS-II和HGS等先进求解器相比，AILS-AHD在中等和大规模实例上均表现出优越性能。在CVRPLib大规模基准测试的10个实例中，为8个实例建立了新的最佳已知解。

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有显著潜力，AILS-AHD方法展示了将大语言模型与进化搜索结合的有效性，为CVRP求解提供了创新解决方案。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [35] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个基于多智能体LLM的情感去毒系统，通过四个智能体分析、调整、监控和指导信息消费，显著降低新闻文章的情感刺激强度（最高19.3%），同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容使消费者暴露于过度的情感刺激，阻碍冷静决策。需要一种系统来支持消费者冷静接收信息，同时不限制对原始文本的访问。

Method: 提出MALLET多智能体情感去毒系统，包含四个智能体：情感分析智能体（使用6情感BERT分类器量化刺激强度）、情感调整智能体（使用LLM将文本重写为BALANCED和COOL两种呈现模式）、平衡监控智能体（聚合每周信息消费模式并生成个性化建议）、个人指导智能体（根据消费者敏感度推荐呈现模式）。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情感平衡改善，同时保持语义保存；刺激降低与语义保存之间的相关性接近零，表明两者可独立控制；类别分析显示体育、商业、科技类刺激大幅降低（17.8-33.8%），而世界类别效果有限，因为事实本身具有高刺激性。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了一个框架，无需限制对原始文本的访问，实现了情感刺激降低与语义保存的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [36] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 论文提出了构建通用世界模型的三位一体一致性理论框架（模态、空间、时间一致性），并引入CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前虽然视频生成模型（如Sora）和数据驱动的缩放定律在近似物理动态方面取得进展，统一多模态模型（UMM）也为整合感知、语言和推理提供了有前景的架构范式，但领域仍缺乏定义通用世界模型必备属性的原则性理论框架。

Method: 1. 提出三位一体一致性理论框架：模态一致性（语义接口）、空间一致性（几何基础）、时间一致性（因果引擎）；2. 系统回顾多模态学习演进轨迹；3. 引入CoW-Bench基准，专注于多帧推理和生成场景，在统一评估协议下评估视频生成模型和UMMs。

Result: 建立了一个原则性的通用世界模型发展路径，阐明了当前系统的局限性和未来进展的架构要求，为领域提供了系统的理论框架和评估基准。

Conclusion: 通过三位一体一致性框架和CoW-Bench基准，为构建通用世界模型提供了理论指导和评估工具，明确了从松散耦合的专业模块向统一架构演进的发展方向，这些架构能够实现内部世界模拟器的协同涌现。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [37] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列推理方法存在两个局限：1) 将时间序列简单视为文本或图像，无法捕捉趋势和季节性等关键模式；2) 混合训练简单和复杂任务时，简单目标主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型，包含：1) 模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2) 任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 大量实验表明，PATRA在多样化时间序列问答任务中优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理机制，有效解决了现有时间序列推理方法的局限性，在复杂时间序列问答任务中取得了显著性能提升。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [38] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个针对单细胞基础模型的自然语言评估框架，通过虚拟细胞抽象统一评估目标，包含五个自然语言任务，并引入知识增强评估来克服传统指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中LLM评估实践不足：现有基准测试碎片化、采用与真实使用场景不符的多选分类格式、依赖缺乏可解释性和生物学基础的指标。

Method: 提出SC-ARENA框架，包括：1）虚拟细胞抽象统一评估目标；2）五个自然语言任务（细胞类型注释、描述、生成、扰动预测、科学问答）；3）知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1）在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是需要机制或因果理解的任务；2）知识增强评估框架确保生物学正确性，提供可解释的证据基础推理，具有高判别能力。

Conclusion: SC-ARENA为单细胞生物学中的LLM评估提供了统一且可解释的框架，指向开发生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [39] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: AIQI是首个被证明在通用强化学习中具有渐近ε最优性的无模型智能体，通过分布动作值函数的通用归纳实现，突破了传统基于模型方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有通用强化学习中的最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。本文旨在探索无模型方法是否也能实现通用最优性，以扩展已知通用智能体的多样性。

Method: 提出Universal AI with Q-Induction (AIQI)，通过分布动作值函数的通用归纳（而不是像先前工作那样对策略或环境进行归纳）来实现无模型学习。在"grain of truth"条件下，证明了AIQI的渐近最优性。

Result: 证明了AIQI是强渐近ε最优和渐近ε贝叶斯最优的，这是首个在通用强化学习中被证明具有渐近最优性的无模型智能体，显著扩展了已知通用智能体的范围。

Conclusion: AIQI展示了无模型方法也能实现通用强化学习中的渐近最优性，为通用智能体设计提供了新的范式，突破了传统基于模型方法的限制。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [40] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出一种解耦证明者-验证者游戏框架，通过训练"翻译器"模型将固定求解器的输出转换为可验证形式，避免传统方法中的可读性税问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要确保其输出能够被能力较弱的系统轻松验证。传统证明者-验证者游戏虽然能提高输出的可验证性，但会降低准确性（可读性税问题）

Method: 将正确性与可验证性条件解耦，训练一个"翻译器"模型，将固定求解器模型的解决方案转换为可验证形式。首先训练求解器最大化正确性，然后训练翻译器在保持求解器答案的同时使其可验证

Result: 提出解耦的证明者-验证者游戏框架，其均衡对应忠实且可验证的翻译器，能够避免可读性税问题

Conclusion: 通过解耦正确性与可验证性训练，可以同时实现模型输出的高正确性和易验证性，解决了传统方法中的可读性税问题

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [41] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个测试时修正或拒绝的剪枝框架，用于动态优化多智能体系统的信息流，无需重新训练，通过检索增强的修正器和失败驱动指示器池来纠正错误，不可修复的输出被剪枝以防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息的级联影响问题。当前解决方案通常采用僵化的结构工程或昂贵的微调，限制了系统的可部署性和适应性。

Method: 提出AgentDropoutV2框架，作为一个主动防火墙，拦截智能体输出并使用检索增强的修正器基于失败驱动的指示器池迭代纠正错误。该机制利用蒸馏的失败模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能够基于任务难度动态调整修正努力，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时框架，能够动态优化多智能体系统的信息流，无需重新训练即可显著提升性能，同时保持系统的完整性和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [42] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent是一个整合大语言模型与临床诊断工具的智能体，通过图像衍生的诊断和视觉证据进行基于证据的胸部X光诊断推理，解决了传统大视觉语言模型在临床诊断中证据不充分、难以验证的问题。


<details>
  <summary>Details</summary>
Motivation: 胸部X光在胸部诊断中至关重要，其解读需要多步骤、基于证据的推理。然而，现有的大视觉语言模型(LVLMs)生成的回答往往缺乏诊断证据的忠实基础，提供的视觉证据有限，且需要昂贵的重新训练来支持新的诊断任务，限制了其在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断智能体，将大语言模型(LLM)与临床基础诊断工具集成，利用图像衍生的诊断和视觉证据进行基于证据的诊断推理。同时构建了CXReasonDial多轮对话基准，包含12个诊断任务的1,946个对话用于评估。

Result: CXReasonAgent能够产生忠实基于证据的响应，相比LVLMs实现了更可靠和可验证的诊断推理。在安全关键的临床环境中，整合临床基础诊断工具的重要性得到凸显。

Conclusion: 通过整合临床基础诊断工具，CXReasonAgent解决了LVLMs在临床诊断中的局限性，提供了更可靠、可验证且适应性强的诊断推理能力，特别适用于安全关键的临床环境。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [43] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN是一个基于神经ODE的脑电动态预测框架，通过将时空频特征整合到谱图节点中，然后使用神经ODE建模连续潜在动态，显著提升了EEG动态预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统潜在变量方法通过循环架构离散化时间建模连续脑动态，会导致累积预测误差和无法捕捉EEG的瞬时非线性特征，需要新的建模方法来解决这些问题。

Method: 提出ODEBRAIN框架：1) 将时空频特征整合到谱图节点中；2) 使用神经ODE建模连续潜在动态；3) 确保潜在表示能捕捉任意时间点的复杂脑状态随机变化。

Result: 大量实验验证表明，ODEBRAIN在EEG动态预测方面显著优于现有方法，具有增强的鲁棒性和泛化能力。

Conclusion: ODEBRAIN通过神经ODE框架有效建模了连续脑动态，克服了传统方法的局限性，为神经科学研究提供了更准确、鲁棒的脑电动态预测工具。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [44] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该论文将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究KM信念更新与AGM信念修正之间的逻辑关系，通过模态逻辑框架形式化比较这两种信念变化理论，探索它们之间的包含关系。

Method: 为KM信念更新的每个公理提供对应的模态逻辑公理，使用包含三个模态算子（B、>、□）的模态逻辑系统，然后将结果逻辑与从AGM公理转换而来的类似逻辑进行比较。

Result: 证明KM逻辑（L_KM）的每个公理都是AGM逻辑（L_AGM）的定理，表明AGM信念修正是KM信念更新的特例。对于强版本KM更新，差异可归结为处理非意外信息的单个公理。

Conclusion: AGM信念修正是KM信念更新的一个特例，两种理论在模态逻辑框架下具有明确的包含关系，强版本KM更新与AGM修正的主要差异在于处理非意外信息的公理。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [45] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出基于重采样的推理方法，通过对输入进行不变变换生成多个样本，聚合推理结果以提高准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型也会因偶然性和认知不确定性产生推理错误，观察到基于输入不变变换的多个样本推理时，推理错误会因认知不确定性而呈现部分独立性

Method: 提出"重采样"推理方法：对训练好的AI模型应用输入的多个变换版本，然后聚合推理输出以获得更准确的结果

Result: 该方法有潜力提高推理准确性，并提供平衡模型大小和性能的策略

Conclusion: 利用认知不确定性导致的推理错误部分独立性，通过重采样和聚合的方法可以改善AI模型的推理性能

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [46] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: GRAVE算法在内存受限环境下不实用，本文提出GRAVE2、GRAVER和GRAVER2三种改进算法，通过两层搜索和节点回收技术大幅减少存储节点数量，同时保持原有游戏强度。


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现优异，但需要存储每个节点的胜率/访问统计信息，在内存受限环境中不实用，限制了其实际应用。

Method: 提出三种改进算法：GRAVE2采用两层搜索，GRAVER使用节点回收技术，GRAVER2结合两种技术，旨在减少存储节点数量。

Result: 这些增强技术能够大幅减少存储节点数量，同时匹配GRAVE算法的游戏强度。

Conclusion: 通过两层搜索和节点回收技术，可以在保持GRAVE算法游戏强度的同时，显著降低内存需求，提高算法在内存受限环境中的实用性。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [47] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLM访问使生物安全相关任务的新手准确率提升4.16倍，甚至在某些任务上超越专家，但用户未能充分发挥LLM潜力，且安全防护措施效果有限


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估LLM是否真正提升新手用户在生物任务上的表现，这对于理解科学加速和双重用途风险至关重要。当前LLM在生物基准测试上表现良好，但尚不清楚是否能让新手用户超越仅使用互联网资源的表现。

Method: 进行了多模型、多基准的人类提升研究，比较了有LLM访问权限的新手与仅能使用互联网的新手在八个生物安全相关任务集上的表现。参与者有充足时间（最复杂的任务长达13小时）处理复杂问题。

Result: LLM访问提供了实质性提升：有LLM的新手准确率比对照组高4.16倍（95% CI [2.63, 6.87]）。在四个有专家基准的任务中，有LLM的新手在三个任务上超越了专家。独立LLM的表现通常超过LLM辅助的新手，表明用户未能充分发挥LLM潜力。89.6%的参与者报告获取双重用途相关信息几乎没有困难。

Conclusion: LLM显著提升了新手在原本需要专业训练的生物任务上的表现，强调了需要持续、交互式的提升评估与传统基准测试并行，同时也凸显了双重用途风险管理的挑战。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [48] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 本文提出了一种细粒度任务分解的多智能体LLM交易框架，相比传统粗粒度指令方法，显著提升了风险调整后收益，并通过投资分析与下游决策偏好的对齐优化系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于多智能体系统的自主金融交易方法通常依赖抽象指令，忽视了真实工作流程的复杂性，导致推理性能下降和决策透明度不足。因此需要更精细的任务分解方法来提升系统性能。

Method: 提出多智能体LLM交易框架，将投资分析明确分解为细粒度任务而非粗粒度指令。使用日本股票数据（价格、财务报表、新闻、宏观信息）在泄漏控制的回测环境中评估。通过标准投资组合优化，利用与股票指数的低相关性和各系统输出的方差。

Result: 实验结果显示，细粒度任务分解相比传统粗粒度设计显著提高了风险调整后收益。进一步分析表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素。投资组合优化方法实现了优越的性能表现。

Conclusion: 细粒度任务分解和多智能体对齐是提升LLM交易系统性能的关键。这些发现为在实际交易系统中应用LLM智能体的智能体结构和任务配置设计提供了重要参考。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>
